{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include libraries which may use in implementation\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn.datasets as ds\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Neural_Network class\n",
    "class Neural_Network(object):        \n",
    "    def __init__(self,inputSize = 2,hiddenlayer = 3, outputSize = 1):        \n",
    "        # size of layers\n",
    "        self.inputSize = inputSize\n",
    "        self.outputSize = outputSize \n",
    "        self.hiddenLayer = hiddenlayer\n",
    "\n",
    "        self.W1 = np.random.rand(inputSize+1, hiddenlayer) \n",
    "        self.W2 = np.random.rand(hiddenlayer, outputSize)    \n",
    "        \n",
    "    def feedforward(self, X):\n",
    "        self.z = np.dot(X, self.W1)\n",
    "        self.activated_z = self.relu(self.z)\n",
    "        self.z1 = np.dot(self.activated_z, self.W2)\n",
    "        output = self.sigmoid(self.z1)\n",
    "        return output\n",
    "    \n",
    "    def sigmoid(self, s):\n",
    "        activated_output = 1 / (1 + (np.exp(-s)))\n",
    "        return activated_output\n",
    " \n",
    "    def sigmoid_derivative(self, s):\n",
    "        der_sigmoid = s * (1 - s)\n",
    "        return der_sigmoid \n",
    "\n",
    "    def tanh(self, s):\n",
    "        positive = np.exp(+s)\n",
    "        negative = np.exp(-s)\n",
    "        out_array = (positive - negative)/(positive + negative)\n",
    "        return out_array\n",
    "\n",
    "    def tanh_derivative(self, s):\n",
    "        derivate_tan = 1 - (s**2)\n",
    "        return derivate_tan\n",
    "    \n",
    "    def relu(self, s):\n",
    "        relu_activation =  np.maximum(0,s)\n",
    "        return relu_activation\n",
    "\n",
    "    def relu_derivative(self, s):\n",
    "        #derivative of relu\n",
    "        s[s<=0] = 0\n",
    "        s[s>0] = 1\n",
    "        return s\n",
    "\n",
    "    def backwardpropagate(self,X, Y, y_pred, lr):  \n",
    "        delta2 = y_pred - Y\n",
    "        d_W2 = (np.dot(self.activated_z.T,delta2))\n",
    "    \n",
    "        delta1 = np.multiply(delta2.dot(self.W2.T),self.relu_derivative(self.activated_z))\n",
    "        d_W1 = (np.dot(X.T, delta1))\n",
    "        \n",
    "        self.W1 = self.W1 - (lr*d_W1)\n",
    "        self.W2 = self.W2 - (lr*d_W2)\n",
    "    \n",
    "    def crossentropy(self, Y, Y_pred):\n",
    "        error_ce =  (-np.sum((Y * np.log2(Y_pred)) + ((1-Y) * np.log2(1- Y_pred))))/len(Y)\n",
    "        return error_ce \n",
    "\n",
    "    def train(self, trainX, trainY, epochs, learningRate, validationX = 'Null', validationY = 'Null'):\n",
    "        train_error = []\n",
    "        valid_error = []\n",
    "        train_accu = []\n",
    "        valid_accu = []\n",
    "        for i in range(epochs):\n",
    "            \n",
    "            pred = self.feedforward(trainX)\n",
    "            self.backwardpropagate(trainX, trainY, pred, learningRate)\n",
    "            train_error.append( self.crossentropy(trainY, pred))\n",
    "            if validationX != 'Null' and validationY != 'Null':\n",
    "                val_pred = self.predict(validationX)\n",
    "                valid_error.append(self.crossentropy(validationY, val_pred))\n",
    "                valid_accu.append(self.accuracy(validationX, validationY))\n",
    "            train_accu.append(self.accuracy(trainX, trainY))\n",
    "            \n",
    "            print (\"epoch:\",i, \"training Loss:\" ,train_error[-1], \"validation Loss:\" ,valid_error[-1],\\\n",
    "                   \" valid acc:\", valid_accu[-1], \" train Acc:\", train_accu[-1])\n",
    "                \n",
    "        self.curve_plot(epochs, train_error, valid_error, train_accu, valid_accu)\n",
    "        \n",
    "    def curve_plot(self, epochs, train_error, valid_error, train_accu, valid_accu):\n",
    "        \n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"loss curve\")\n",
    "        plt.plot(range(epochs), train_error)\n",
    "        plt.plot(range(epochs), valid_error)\n",
    "        plt.show()     \n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"accuracy Curve\")\n",
    "        plt.plot(range(epochs), train_accu)\n",
    "        plt.plot(range(epochs), valid_accu)\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, testX):\n",
    "        # predict the value of testX\n",
    "        test_pred = self.feedforward(testX)\n",
    "        return test_pred\n",
    "    \n",
    "    def accuracy(self, testX, testY):\n",
    "        # predict the value of trainX\n",
    "        test_pred = self.predict(testX)\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        test_pred[test_pred>=0.5] = 1\n",
    "        test_pred[test_pred<0.5] = 0\n",
    "        x= np.mean(testY == test_pred)\n",
    "        return x\n",
    "    \n",
    "    def saveModel(self,name):\n",
    "        np.savez(name, self.W1, self.W2)\n",
    "\n",
    "    def loadModel(self,name):\n",
    "        self.file = np.load(name, allow_pickle = True)\n",
    "        self.W1 = self.file['arr_0']\n",
    "        self.W2 = self.file['arr_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data, label = ds.make_circles(n_samples=1000, factor=.4, noise=0.05)\n",
    "\n",
    "#Lets visualize the dataset\n",
    "reds = label == 0\n",
    "blues = label == 1\n",
    "plt.scatter(data[reds, 0], data[reds, 1], c=\"red\", s=20, edgecolor='k')\n",
    "plt.scatter(data[blues, 0], data[blues, 1], c=\"blue\", s=20, edgecolor='k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: shuffle this dataset before dividing it into three parts\n",
    "from sklearn.utils import shuffle\n",
    "X, y = shuffle(data, label, random_state=0)\n",
    "# Distribute this data into three parts i.e. training, validation and testing\n",
    "\n",
    "trainX, validX, testX = X[:700], X[700:850], X[850:]\n",
    "trainY, validY, testY = y[:700], y[700:850], y[850:]\n",
    "\n",
    "trainY = np.expand_dims(trainY, axis=1)\n",
    "testY = np.expand_dims(testY, axis=1)\n",
    "validY = np.expand_dims(validY, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m , n, o = len(trainX), len(validX), len(testX)\n",
    "trainX = np.hstack((trainX, np.atleast_2d(np.ones(m)).T))\n",
    "testX = np.hstack((testX, np.atleast_2d(np.ones(n)).T))\n",
    "validX = np.hstack((validX, np.atleast_2d(np.ones(o)).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mudasser Afzal\\Anaconda3\\envs\\testing\\lib\\site-packages\\ipykernel_launcher.py:71: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 training Loss: 1.1560757172013347 validation Loss: 1.2416498740334405  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 1 training Loss: 1.15055859126186 validation Loss: 1.2342882689570736  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 2 training Loss: 1.145314690605531 validation Loss: 1.2272450031929836  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 3 training Loss: 1.1403273379514864 validation Loss: 1.2205150617358118  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 4 training Loss: 1.1355750678710663 validation Loss: 1.2140833089679561  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 5 training Loss: 1.1310467607501382 validation Loss: 1.2079335020023432  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 6 training Loss: 1.1267397268499333 validation Loss: 1.202043719591081  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 7 training Loss: 1.1226377238511356 validation Loss: 1.1964114766295315  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 8 training Loss: 1.1187296785918754 validation Loss: 1.1910165531703114  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 9 training Loss: 1.115001310630947 validation Loss: 1.1858489767527605  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 10 training Loss: 1.1114410307217681 validation Loss: 1.1808867209687246  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 11 training Loss: 1.108029764461392 validation Loss: 1.1761193767097662  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 12 training Loss: 1.1047660035573745 validation Loss: 1.1715403723100641  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 13 training Loss: 1.1016378716895916 validation Loss: 1.1671344843385616  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 14 training Loss: 1.0986368187843778 validation Loss: 1.1628957791146937  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 15 training Loss: 1.095758193131391 validation Loss: 1.1588153543975297  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 16 training Loss: 1.0929950511502067 validation Loss: 1.1548912661488844  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 17 training Loss: 1.0903370665789158 validation Loss: 1.151109712231084  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 18 training Loss: 1.0877751649138616 validation Loss: 1.1474503053372218  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 19 training Loss: 1.085302652458443 validation Loss: 1.1439186578137572  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 20 training Loss: 1.082914311380211 validation Loss: 1.140506693477264  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 21 training Loss: 1.0806122276600545 validation Loss: 1.137211260005101  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 22 training Loss: 1.0783901502532944 validation Loss: 1.13400669949089  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 23 training Loss: 1.0762362998195327 validation Loss: 1.130904298633953  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 24 training Loss: 1.0741573452404558 validation Loss: 1.1278973979888365  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 25 training Loss: 1.0721499732084836 validation Loss: 1.1249828102399715  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 26 training Loss: 1.0702068632563582 validation Loss: 1.1221564563960496  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 27 training Loss: 1.0683249251627653 validation Loss: 1.119416935274172  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 28 training Loss: 1.066506325914929 validation Loss: 1.1167625966380819  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 29 training Loss: 1.0647460560275497 validation Loss: 1.1141963198385485  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 30 training Loss: 1.0630414453314272 validation Loss: 1.1117075536336547  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 31 training Loss: 1.0613911140427081 validation Loss: 1.1092907710399433  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 32 training Loss: 1.059793395992691 validation Loss: 1.106947521513842  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 33 training Loss: 1.0582480524606257 validation Loss: 1.1046699227260972  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 34 training Loss: 1.0567511383075787 validation Loss: 1.1024576951375813  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 35 training Loss: 1.0552998848918629 validation Loss: 1.1003019663437983  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 36 training Loss: 1.05388921017123 validation Loss: 1.0981995772360635  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 37 training Loss: 1.052516942313477 validation Loss: 1.0961547957198725  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 38 training Loss: 1.0511861608402155 validation Loss: 1.0941674963650765  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 39 training Loss: 1.0498957524082777 validation Loss: 1.0922312118737705  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 40 training Loss: 1.048639580244003 validation Loss: 1.090342225357933  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 41 training Loss: 1.0474151143915804 validation Loss: 1.0885022685342176  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 42 training Loss: 1.0462259443516864 validation Loss: 1.0867114225264913  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 43 training Loss: 1.0450708623370009 validation Loss: 1.0849656638497893  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 44 training Loss: 1.043947103622968 validation Loss: 1.0832644894828414  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 45 training Loss: 1.0428521837144948 validation Loss: 1.0816008876295236  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 46 training Loss: 1.041782823059654 validation Loss: 1.0799754031262288  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 47 training Loss: 1.040738879702522 validation Loss: 1.078387298063536  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 48 training Loss: 1.0397202808528438 validation Loss: 1.0768378606619176  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 49 training Loss: 1.038725278554442 validation Loss: 1.0753252124396202  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 50 training Loss: 1.0377540219548704 validation Loss: 1.0738474378820824  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 51 training Loss: 1.0368079817601061 validation Loss: 1.0724051213544536  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 52 training Loss: 1.03588684424609 validation Loss: 1.070995084464633  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 53 training Loss: 1.0349875744839625 validation Loss: 1.0696135435189154  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 54 training Loss: 1.0341063667470087 validation Loss: 1.0682622394501418  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 55 training Loss: 1.0332460308408353 validation Loss: 1.0669394418063916  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 56 training Loss: 1.0324047597607064 validation Loss: 1.0656448796632423  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 57 training Loss: 1.0315822222852808 validation Loss: 1.0643763517325346  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 58 training Loss: 1.0307773482872138 validation Loss: 1.0631348140219434  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 59 training Loss: 1.0299908285604247 validation Loss: 1.0619177975906984  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 60 training Loss: 1.0292204015395887 validation Loss: 1.0607254262653592  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 61 training Loss: 1.0284676572324811 validation Loss: 1.059556688312764  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 62 training Loss: 1.0277309210175736 validation Loss: 1.0584060887850584  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 63 training Loss: 1.0270064482296994 validation Loss: 1.057278126168324  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 64 training Loss: 1.0262974294824894 validation Loss: 1.0561704178746865  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 65 training Loss: 1.0256042780276884 validation Loss: 1.0550837084526865  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 66 training Loss: 1.0249254180545244 validation Loss: 1.054018613944088  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 67 training Loss: 1.0242612532357205 validation Loss: 1.0529733671053019  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 68 training Loss: 1.023609979509751 validation Loss: 1.051947428558002  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 69 training Loss: 1.0229716471086983 validation Loss: 1.0509401336326938  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 70 training Loss: 1.0223455071141312 validation Loss: 1.0499499095635227  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 71 training Loss: 1.0217294232656131 validation Loss: 1.0489763162673709  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 72 training Loss: 1.0211232484658412 validation Loss: 1.048017140489859  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 73 training Loss: 1.0205259143486167 validation Loss: 1.047074779907124  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 74 training Loss: 1.0199396750113847 validation Loss: 1.046148741395505  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 75 training Loss: 1.0193641789876502 validation Loss: 1.0452385501783226  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 76 training Loss: 1.0187990883159677 validation Loss: 1.0443437489923963  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 77 training Loss: 1.0182445541980394 validation Loss: 1.0434650407321384  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 78 training Loss: 1.017700948499191 validation Loss: 1.0426006781397659  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 79 training Loss: 1.0171667310832562 validation Loss: 1.0417483239779393  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 80 training Loss: 1.0166416151125905 validation Loss: 1.040909693649022  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 81 training Loss: 1.0161253242718744 validation Loss: 1.0400844073949718  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 82 training Loss: 1.015617099289155 validation Loss: 1.0392712757205702  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 83 training Loss: 1.0151166453253038 validation Loss: 1.038470816614356  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 84 training Loss: 1.0146241110070735 validation Loss: 1.0376833725791843  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 85 training Loss: 1.0141382825289242 validation Loss: 1.0369076591973843  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 86 training Loss: 1.013657807794422 validation Loss: 1.0361436985605632  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 87 training Loss: 1.0131849194216098 validation Loss: 1.0353911766833654  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 88 training Loss: 1.01271910757188 validation Loss: 1.0346490991689958  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 89 training Loss: 1.0122597221043115 validation Loss: 1.0339166121978243  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 90 training Loss: 1.0118086204323988 validation Loss: 1.0331959390051868  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 91 training Loss: 1.0113655789179943 validation Loss: 1.0324863929390484  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 92 training Loss: 1.0109288350193923 validation Loss: 1.0317862406376708  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 93 training Loss: 1.0104985791130996 validation Loss: 1.031095842022739  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 94 training Loss: 1.0100755616890391 validation Loss: 1.0304157592024692  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 95 training Loss: 1.0096593216171437 validation Loss: 1.0297448393572415  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 96 training Loss: 1.0092487325844288 validation Loss: 1.0290821434231856  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 97 training Loss: 1.0088436359871247 validation Loss: 1.0284274726353049  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 98 training Loss: 1.0084438782252516 validation Loss: 1.027781345315876  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 99 training Loss: 1.0080489706360924 validation Loss: 1.0271429258754972  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 100 training Loss: 1.0076587327859194 validation Loss: 1.0265137805635831  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 101 training Loss: 1.0072734346626504 validation Loss: 1.025893729608368  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 102 training Loss: 1.006892968593285 validation Loss: 1.025281935213219  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 103 training Loss: 1.0065176321619047 validation Loss: 1.0246776300385327  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 104 training Loss: 1.006146761750307 validation Loss: 1.0240801346138289  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 105 training Loss: 1.005779834883159 validation Loss: 1.0234904504352735  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 106 training Loss: 1.005418070437939 validation Loss: 1.0229077246763194  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 107 training Loss: 1.0050608643940215 validation Loss: 1.0223322821870218  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 108 training Loss: 1.0047083504813472 validation Loss: 1.0217634529169326  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 109 training Loss: 1.0043597408826956 validation Loss: 1.0212010842708572  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 110 training Loss: 1.0040146744696987 validation Loss: 1.0206445565615268  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 111 training Loss: 1.0036730241519898 validation Loss: 1.0200942235897665  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 112 training Loss: 1.00333499865673 validation Loss: 1.0195499440466416  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 113 training Loss: 1.0030005010409393 validation Loss: 1.0190115804397828  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 114 training Loss: 1.0026693995804103 validation Loss: 1.018478510441765  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 115 training Loss: 1.0023414056429227 validation Loss: 1.0179511227285467  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 116 training Loss: 1.0020166969520032 validation Loss: 1.0174292898168569  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 117 training Loss: 1.001695490390739 validation Loss: 1.0169133937743053  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 118 training Loss: 1.0013775725291725 validation Loss: 1.0164023942996123  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 119 training Loss: 1.0010624318876402 validation Loss: 1.0158965801627793  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 120 training Loss: 1.0007502323335284 validation Loss: 1.015395836848345  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 121 training Loss: 1.0004409121779578 validation Loss: 1.0149000042493412  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 122 training Loss: 1.00013463282505 validation Loss: 1.014409031424143  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 123 training Loss: 0.9998310230586279 validation Loss: 1.0139223898972924  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 124 training Loss: 0.9995299763550237 validation Loss: 1.0134395256698063  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 125 training Loss: 0.9992315500882241 validation Loss: 1.0129612007594186  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 126 training Loss: 0.9989356751220534 validation Loss: 1.012487385974651  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 127 training Loss: 0.9986421651226887 validation Loss: 1.012017166001207  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 128 training Loss: 0.9983508032810842 validation Loss: 1.011550979590416  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 129 training Loss: 0.9980618138730616 validation Loss: 1.0110890630436744  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 130 training Loss: 0.9977752395886113 validation Loss: 1.0106312810669806  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 131 training Loss: 0.997491125975135 validation Loss: 1.010177997508548  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 132 training Loss: 0.9972090677757108 validation Loss: 1.0097285280518435  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 133 training Loss: 0.9969291124806059 validation Loss: 1.0092833478192802  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 134 training Loss: 0.9966513508185054 validation Loss: 1.0088416876047182  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 135 training Loss: 0.9963752418973747 validation Loss: 1.0084035116945036  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 136 training Loss: 0.9961009429658143 validation Loss: 1.0079690198460576  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 137 training Loss: 0.9958288983494579 validation Loss: 1.007538352600016  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 138 training Loss: 0.9955586316695505 validation Loss: 1.007110971661306  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 139 training Loss: 0.9952899728323895 validation Loss: 1.0066871200082974  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 140 training Loss: 0.995023134654733 validation Loss: 1.0062667239864485  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 141 training Loss: 0.9947582435204996 validation Loss: 1.0058498503055362  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 142 training Loss: 0.994495304461854 validation Loss: 1.0054363352778861  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 143 training Loss: 0.9942340573608535 validation Loss: 1.0050261257446  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 144 training Loss: 0.9939743713530296 validation Loss: 1.0046188487711507  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 145 training Loss: 0.9937160641244526 validation Loss: 1.0042147596952133  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 146 training Loss: 0.9934593264201577 validation Loss: 1.0038143084356295  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 147 training Loss: 0.9932042430583523 validation Loss: 1.0034167967922714  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 148 training Loss: 0.9929509523706442 validation Loss: 1.003021486673215  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 149 training Loss: 0.9926991448985999 validation Loss: 1.0026284045418175  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 150 training Loss: 0.9924491308051913 validation Loss: 1.0022385271170196  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 151 training Loss: 0.9922009089655868 validation Loss: 1.0018513652141825  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 152 training Loss: 0.9919542281985576 validation Loss: 1.0014669487743746  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 153 training Loss: 0.9917088616503611 validation Loss: 1.0010852220622461  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 154 training Loss: 0.9914646832110612 validation Loss: 1.0007058972921758  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 155 training Loss: 0.9912216500351672 validation Loss: 1.0003291666024092  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 156 training Loss: 0.9909799642945274 validation Loss: 0.9999552033309081  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 157 training Loss: 0.9907399817370801 validation Loss: 0.9995837253734529  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 158 training Loss: 0.9905010721770364 validation Loss: 0.9992147883883042  valid acc: 0.4533333333333333  train Acc: 0.5228571428571429\n",
      "epoch: 159 training Loss: 0.9902630571261776 validation Loss: 0.9988481615903161  valid acc: 0.4533333333333333  train Acc: 0.5271428571428571\n",
      "epoch: 160 training Loss: 0.990026107476345 validation Loss: 0.9984835584755695  valid acc: 0.46  train Acc: 0.5385714285714286\n",
      "epoch: 161 training Loss: 0.9897901911655651 validation Loss: 0.9981212456616397  valid acc: 0.48  train Acc: 0.55\n",
      "epoch: 162 training Loss: 0.9895552767644638 validation Loss: 0.9977611763369544  valid acc: 0.49333333333333335  train Acc: 0.5728571428571428\n",
      "epoch: 163 training Loss: 0.98932133346 validation Loss: 0.9974033045805851  valid acc: 0.5066666666666667  train Acc: 0.5785714285714286\n",
      "epoch: 164 training Loss: 0.9890883310397328 validation Loss: 0.9970475853394724  valid acc: 0.52  train Acc: 0.5814285714285714\n",
      "epoch: 165 training Loss: 0.9888562398766018 validation Loss: 0.9966939744063614  valid acc: 0.52  train Acc: 0.5885714285714285\n",
      "epoch: 166 training Loss: 0.9886250309142012 validation Loss: 0.996342428398439  valid acc: 0.54  train Acc: 0.5928571428571429\n",
      "epoch: 167 training Loss: 0.9883946756525319 validation Loss: 0.995992904736632  valid acc: 0.56  train Acc: 0.6\n",
      "epoch: 168 training Loss: 0.9881651461342119 validation Loss: 0.9956444189643764  valid acc: 0.56  train Acc: 0.6057142857142858\n",
      "epoch: 169 training Loss: 0.987936414931127 validation Loss: 0.9952968296250456  valid acc: 0.5733333333333334  train Acc: 0.61\n",
      "epoch: 170 training Loss: 0.9877084551315095 validation Loss: 0.9949510975957446  valid acc: 0.58  train Acc: 0.6142857142857143\n",
      "epoch: 171 training Loss: 0.9874812403274258 validation Loss: 0.9946071830529364  valid acc: 0.5866666666666667  train Acc: 0.62\n",
      "epoch: 172 training Loss: 0.9872547446026605 validation Loss: 0.9942650468827492  valid acc: 0.5933333333333334  train Acc: 0.6228571428571429\n",
      "epoch: 173 training Loss: 0.9870289425209813 validation Loss: 0.9939246506637964  valid acc: 0.5933333333333334  train Acc: 0.6257142857142857\n",
      "epoch: 174 training Loss: 0.9868038091147718 validation Loss: 0.9935859566505203  valid acc: 0.6  train Acc: 0.6285714285714286\n",
      "epoch: 175 training Loss: 0.9865793919652875 validation Loss: 0.9932488914873928  valid acc: 0.6  train Acc: 0.6342857142857142\n",
      "epoch: 176 training Loss: 0.9863560144923612 validation Loss: 0.9929134695408038  valid acc: 0.6  train Acc: 0.6357142857142857\n",
      "epoch: 177 training Loss: 0.9861332509869677 validation Loss: 0.9925796550489298  valid acc: 0.6  train Acc: 0.6385714285714286\n",
      "epoch: 178 training Loss: 0.9859110782899649 validation Loss: 0.9922474128638434  valid acc: 0.6  train Acc: 0.6428571428571429\n",
      "epoch: 179 training Loss: 0.9856894736585653 validation Loss: 0.9919167084371903  valid acc: 0.6066666666666667  train Acc: 0.6471428571428571\n",
      "epoch: 180 training Loss: 0.9854683529587269 validation Loss: 0.991587312422557  valid acc: 0.6133333333333333  train Acc: 0.6485714285714286\n",
      "epoch: 181 training Loss: 0.9852476937174007 validation Loss: 0.9912591360859517  valid acc: 0.62  train Acc: 0.65\n",
      "epoch: 182 training Loss: 0.9850275583543752 validation Loss: 0.9909325588989164  valid acc: 0.62  train Acc: 0.65\n",
      "epoch: 183 training Loss: 0.9848079574659067 validation Loss: 0.9906073974623097  valid acc: 0.62  train Acc: 0.6514285714285715\n",
      "epoch: 184 training Loss: 0.9845884651002188 validation Loss: 0.9902835265276152  valid acc: 0.62  train Acc: 0.6528571428571428\n",
      "epoch: 185 training Loss: 0.9843687502919745 validation Loss: 0.9899609868412633  valid acc: 0.62  train Acc: 0.6542857142857142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 186 training Loss: 0.9841494294829293 validation Loss: 0.9896397474526426  valid acc: 0.6266666666666667  train Acc: 0.6557142857142857\n",
      "epoch: 187 training Loss: 0.9839303681974856 validation Loss: 0.989319682708123  valid acc: 0.6266666666666667  train Acc: 0.6614285714285715\n",
      "epoch: 188 training Loss: 0.9837106816843408 validation Loss: 0.9890008360812628  valid acc: 0.6266666666666667  train Acc: 0.6642857142857143\n",
      "epoch: 189 training Loss: 0.9834913021485227 validation Loss: 0.9886831780734767  valid acc: 0.6266666666666667  train Acc: 0.67\n",
      "epoch: 190 training Loss: 0.9832721934643591 validation Loss: 0.9883671643388228  valid acc: 0.6266666666666667  train Acc: 0.67\n",
      "epoch: 191 training Loss: 0.9830534145145576 validation Loss: 0.9880531208961559  valid acc: 0.6266666666666667  train Acc: 0.6714285714285714\n",
      "epoch: 192 training Loss: 0.9828347942401693 validation Loss: 0.9877402169341565  valid acc: 0.6266666666666667  train Acc: 0.6714285714285714\n",
      "epoch: 193 training Loss: 0.9826169023553463 validation Loss: 0.9874284382000382  valid acc: 0.6266666666666667  train Acc: 0.6685714285714286\n",
      "epoch: 194 training Loss: 0.9823992445564909 validation Loss: 0.9871177573996249  valid acc: 0.6266666666666667  train Acc: 0.6685714285714286\n",
      "epoch: 195 training Loss: 0.9821817768327729 validation Loss: 0.9868097361509304  valid acc: 0.6266666666666667  train Acc: 0.6714285714285714\n",
      "epoch: 196 training Loss: 0.9819639334395037 validation Loss: 0.9865032451515693  valid acc: 0.6266666666666667  train Acc: 0.6714285714285714\n",
      "epoch: 197 training Loss: 0.9817462522680208 validation Loss: 0.9861977739346408  valid acc: 0.6266666666666667  train Acc: 0.6714285714285714\n",
      "epoch: 198 training Loss: 0.9815289251647868 validation Loss: 0.9858932817399185  valid acc: 0.6266666666666667  train Acc: 0.6714285714285714\n",
      "epoch: 199 training Loss: 0.9813124392517439 validation Loss: 0.9855897994263891  valid acc: 0.6266666666666667  train Acc: 0.6728571428571428\n",
      "epoch: 200 training Loss: 0.981096088880145 validation Loss: 0.9852873020509666  valid acc: 0.62  train Acc: 0.6714285714285714\n",
      "epoch: 201 training Loss: 0.9808800218983456 validation Loss: 0.9849857265096551  valid acc: 0.6133333333333333  train Acc: 0.6728571428571428\n",
      "epoch: 202 training Loss: 0.9806651140041085 validation Loss: 0.9846850021812547  valid acc: 0.6133333333333333  train Acc: 0.6728571428571428\n",
      "epoch: 203 training Loss: 0.9804507216852816 validation Loss: 0.9843852249655193  valid acc: 0.6133333333333333  train Acc: 0.6714285714285714\n",
      "epoch: 204 training Loss: 0.9802365481368218 validation Loss: 0.984086161698887  valid acc: 0.6133333333333333  train Acc: 0.67\n",
      "epoch: 205 training Loss: 0.9800229948340762 validation Loss: 0.9837880207673053  valid acc: 0.6133333333333333  train Acc: 0.6685714285714286\n",
      "epoch: 206 training Loss: 0.9798095488617149 validation Loss: 0.9834907792484311  valid acc: 0.6133333333333333  train Acc: 0.6685714285714286\n",
      "epoch: 207 training Loss: 0.9795961956859803 validation Loss: 0.9831944145572765  valid acc: 0.6133333333333333  train Acc: 0.67\n",
      "epoch: 208 training Loss: 0.9793829210066599 validation Loss: 0.9828998812667576  valid acc: 0.6133333333333333  train Acc: 0.6671428571428571\n",
      "epoch: 209 training Loss: 0.9791697107529025 validation Loss: 0.982606899577259  valid acc: 0.6133333333333333  train Acc: 0.6671428571428571\n",
      "epoch: 210 training Loss: 0.9789565974653549 validation Loss: 0.9823147836286459  valid acc: 0.6133333333333333  train Acc: 0.6657142857142857\n",
      "epoch: 211 training Loss: 0.9787445093019645 validation Loss: 0.9820233459541491  valid acc: 0.6133333333333333  train Acc: 0.6642857142857143\n",
      "epoch: 212 training Loss: 0.9785333988114089 validation Loss: 0.9817327257017746  valid acc: 0.6066666666666667  train Acc: 0.6614285714285715\n",
      "epoch: 213 training Loss: 0.9783230449016017 validation Loss: 0.9814430381872842  valid acc: 0.6066666666666667  train Acc: 0.6585714285714286\n",
      "epoch: 214 training Loss: 0.9781127467337498 validation Loss: 0.9811541484408811  valid acc: 0.6066666666666667  train Acc: 0.6571428571428571\n",
      "epoch: 215 training Loss: 0.9779024916199295 validation Loss: 0.9808660367671223  valid acc: 0.6066666666666667  train Acc: 0.6571428571428571\n",
      "epoch: 216 training Loss: 0.9776922268295698 validation Loss: 0.9805789411894151  valid acc: 0.6066666666666667  train Acc: 0.6557142857142857\n",
      "epoch: 217 training Loss: 0.9774813302061361 validation Loss: 0.9802925298411053  valid acc: 0.6066666666666667  train Acc: 0.6571428571428571\n",
      "epoch: 218 training Loss: 0.9772700923923922 validation Loss: 0.9800068037066393  valid acc: 0.6  train Acc: 0.6557142857142857\n",
      "epoch: 219 training Loss: 0.9770582675672993 validation Loss: 0.9797216607510525  valid acc: 0.6066666666666667  train Acc: 0.6542857142857142\n",
      "epoch: 220 training Loss: 0.9768467636165399 validation Loss: 0.9794371908810243  valid acc: 0.6066666666666667  train Acc: 0.6514285714285715\n",
      "epoch: 221 training Loss: 0.9766338778346952 validation Loss: 0.9791533445854573  valid acc: 0.6066666666666667  train Acc: 0.65\n",
      "epoch: 222 training Loss: 0.9764208921842128 validation Loss: 0.9788701038994012  valid acc: 0.6066666666666667  train Acc: 0.6471428571428571\n",
      "epoch: 223 training Loss: 0.9762081101111925 validation Loss: 0.9785872969977031  valid acc: 0.6066666666666667  train Acc: 0.6442857142857142\n",
      "epoch: 224 training Loss: 0.9759957344860443 validation Loss: 0.9783050783563232  valid acc: 0.6066666666666667  train Acc: 0.6442857142857142\n",
      "epoch: 225 training Loss: 0.9757832454426011 validation Loss: 0.978023430723058  valid acc: 0.6066666666666667  train Acc: 0.6442857142857142\n",
      "epoch: 226 training Loss: 0.9755708878372814 validation Loss: 0.9777421806358653  valid acc: 0.6066666666666667  train Acc: 0.6428571428571429\n",
      "epoch: 227 training Loss: 0.9753589808775102 validation Loss: 0.977462200871413  valid acc: 0.6  train Acc: 0.6414285714285715\n",
      "epoch: 228 training Loss: 0.9751470186022457 validation Loss: 0.9771844047758477  valid acc: 0.6  train Acc: 0.64\n",
      "epoch: 229 training Loss: 0.9749358545641279 validation Loss: 0.9769070397034699  valid acc: 0.5933333333333334  train Acc: 0.64\n",
      "epoch: 230 training Loss: 0.9747254735008855 validation Loss: 0.9766293068971615  valid acc: 0.5933333333333334  train Acc: 0.6385714285714286\n",
      "epoch: 231 training Loss: 0.9745149780878875 validation Loss: 0.976349489856907  valid acc: 0.6  train Acc: 0.6371428571428571\n",
      "epoch: 232 training Loss: 0.9743043583963854 validation Loss: 0.97607016861558  valid acc: 0.6  train Acc: 0.6357142857142857\n",
      "epoch: 233 training Loss: 0.9740942244876265 validation Loss: 0.975793088994364  valid acc: 0.6066666666666667  train Acc: 0.6328571428571429\n",
      "epoch: 234 training Loss: 0.9738849410946137 validation Loss: 0.9755193436879165  valid acc: 0.6066666666666667  train Acc: 0.6314285714285715\n",
      "epoch: 235 training Loss: 0.9736755394718523 validation Loss: 0.9752461088024968  valid acc: 0.6066666666666667  train Acc: 0.6314285714285715\n",
      "epoch: 236 training Loss: 0.9734657063811918 validation Loss: 0.9749735574167513  valid acc: 0.6066666666666667  train Acc: 0.63\n",
      "epoch: 237 training Loss: 0.9732544516621442 validation Loss: 0.9747015098150996  valid acc: 0.6066666666666667  train Acc: 0.6314285714285715\n",
      "epoch: 238 training Loss: 0.9730430240660372 validation Loss: 0.9744299513057714  valid acc: 0.6066666666666667  train Acc: 0.6314285714285715\n",
      "epoch: 239 training Loss: 0.9728314145614884 validation Loss: 0.9741588673814975  valid acc: 0.6066666666666667  train Acc: 0.63\n",
      "epoch: 240 training Loss: 0.9726196371797934 validation Loss: 0.9738879527240264  valid acc: 0.6066666666666667  train Acc: 0.63\n",
      "epoch: 241 training Loss: 0.9724084367999051 validation Loss: 0.9736175019074422  valid acc: 0.6066666666666667  train Acc: 0.6285714285714286\n",
      "epoch: 242 training Loss: 0.9721974473504118 validation Loss: 0.9733470138063244  valid acc: 0.6066666666666667  train Acc: 0.6271428571428571\n",
      "epoch: 243 training Loss: 0.9719880817473232 validation Loss: 0.9730767152454215  valid acc: 0.6066666666666667  train Acc: 0.6257142857142857\n",
      "epoch: 244 training Loss: 0.9717790140972954 validation Loss: 0.9728068947260118  valid acc: 0.6066666666666667  train Acc: 0.6257142857142857\n",
      "epoch: 245 training Loss: 0.9715697901560939 validation Loss: 0.9725375387159276  valid acc: 0.6066666666666667  train Acc: 0.6242857142857143\n",
      "epoch: 246 training Loss: 0.971360401684256 validation Loss: 0.9722686338481981  valid acc: 0.6066666666666667  train Acc: 0.6242857142857143\n",
      "epoch: 247 training Loss: 0.9711508405693137 validation Loss: 0.9720001669185819  valid acc: 0.6  train Acc: 0.6257142857142857\n",
      "epoch: 248 training Loss: 0.9709412857837109 validation Loss: 0.9717323597079235  valid acc: 0.6  train Acc: 0.6257142857142857\n",
      "epoch: 249 training Loss: 0.9707318845068292 validation Loss: 0.971464966001557  valid acc: 0.6  train Acc: 0.6242857142857143\n",
      "epoch: 250 training Loss: 0.9705222900887096 validation Loss: 0.9711979730055214  valid acc: 0.6  train Acc: 0.6228571428571429\n",
      "epoch: 251 training Loss: 0.9703124948475028 validation Loss: 0.9709313680790659  valid acc: 0.5933333333333334  train Acc: 0.6228571428571429\n",
      "epoch: 252 training Loss: 0.9701022389201035 validation Loss: 0.9706652976439603  valid acc: 0.5933333333333334  train Acc: 0.6228571428571429\n",
      "epoch: 253 training Loss: 0.9698903850450441 validation Loss: 0.970399354479988  valid acc: 0.5933333333333334  train Acc: 0.6214285714285714\n",
      "epoch: 254 training Loss: 0.9696793824829621 validation Loss: 0.9701337580184827  valid acc: 0.5933333333333334  train Acc: 0.6214285714285714\n",
      "epoch: 255 training Loss: 0.969468220876869 validation Loss: 0.9698681854450141  valid acc: 0.5933333333333334  train Acc: 0.6228571428571429\n",
      "epoch: 256 training Loss: 0.9692581350639102 validation Loss: 0.9696027322289887  valid acc: 0.6  train Acc: 0.6228571428571429\n",
      "epoch: 257 training Loss: 0.9690484599715319 validation Loss: 0.9693376320190464  valid acc: 0.6  train Acc: 0.6185714285714285\n",
      "epoch: 258 training Loss: 0.9688382220432626 validation Loss: 0.9690730484412581  valid acc: 0.6  train Acc: 0.6185714285714285\n",
      "epoch: 259 training Loss: 0.9686263511346092 validation Loss: 0.9688087688049181  valid acc: 0.6  train Acc: 0.6185714285714285\n",
      "epoch: 260 training Loss: 0.968414213964423 validation Loss: 0.9685447816774105  valid acc: 0.6066666666666667  train Acc: 0.6157142857142858\n",
      "epoch: 261 training Loss: 0.9682018038031601 validation Loss: 0.968281075760079  valid acc: 0.6066666666666667  train Acc: 0.6157142857142858\n",
      "epoch: 262 training Loss: 0.9679891190697792 validation Loss: 0.9680173259930381  valid acc: 0.6133333333333333  train Acc: 0.6157142857142858\n",
      "epoch: 263 training Loss: 0.9677769805628915 validation Loss: 0.9677538546521953  valid acc: 0.62  train Acc: 0.6157142857142858\n",
      "epoch: 264 training Loss: 0.9675645674779113 validation Loss: 0.9674906508007265  valid acc: 0.62  train Acc: 0.6157142857142858\n",
      "epoch: 265 training Loss: 0.9673518734510698 validation Loss: 0.9672277036289563  valid acc: 0.62  train Acc: 0.6142857142857143\n",
      "epoch: 266 training Loss: 0.9671388922196509 validation Loss: 0.9669650024527713  valid acc: 0.62  train Acc: 0.6142857142857143\n",
      "epoch: 267 training Loss: 0.9669257831170953 validation Loss: 0.9667022185045706  valid acc: 0.62  train Acc: 0.6142857142857143\n",
      "epoch: 268 training Loss: 0.9667132537032042 validation Loss: 0.9664394742793252  valid acc: 0.62  train Acc: 0.6142857142857143\n",
      "epoch: 269 training Loss: 0.9665015265341756 validation Loss: 0.9661769848554885  valid acc: 0.62  train Acc: 0.6142857142857143\n",
      "epoch: 270 training Loss: 0.9662895266416327 validation Loss: 0.9659147399945494  valid acc: 0.62  train Acc: 0.6142857142857143\n",
      "epoch: 271 training Loss: 0.9660772481604875 validation Loss: 0.965652729574349  valid acc: 0.62  train Acc: 0.6142857142857143\n",
      "epoch: 272 training Loss: 0.9658646853200891 validation Loss: 0.9653909435877195  valid acc: 0.62  train Acc: 0.6142857142857143\n",
      "epoch: 273 training Loss: 0.9656518324433494 validation Loss: 0.9651293721411475  valid acc: 0.62  train Acc: 0.6142857142857143\n",
      "epoch: 274 training Loss: 0.9654389946362637 validation Loss: 0.9648677772831504  valid acc: 0.62  train Acc: 0.6142857142857143\n",
      "epoch: 275 training Loss: 0.9652268503835104 validation Loss: 0.9646064000995181  valid acc: 0.62  train Acc: 0.6128571428571429\n",
      "epoch: 276 training Loss: 0.9650144200855024 validation Loss: 0.9643452224274291  valid acc: 0.62  train Acc: 0.6128571428571429\n",
      "epoch: 277 training Loss: 0.9648016970978929 validation Loss: 0.9640842443021783  valid acc: 0.62  train Acc: 0.6128571428571429\n",
      "epoch: 278 training Loss: 0.964588679038897 validation Loss: 0.9638234562466276  valid acc: 0.62  train Acc: 0.6128571428571429\n",
      "epoch: 279 training Loss: 0.9643758316522907 validation Loss: 0.9635626105346569  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 280 training Loss: 0.9641634979435456 validation Loss: 0.9633044800037389  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 281 training Loss: 0.9639508748548311 validation Loss: 0.963047067839154  valid acc: 0.62  train Acc: 0.6085714285714285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 282 training Loss: 0.9637383630263932 validation Loss: 0.9627895263035949  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 283 training Loss: 0.9635264391840968 validation Loss: 0.9625322077995618  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 284 training Loss: 0.963314232028595 validation Loss: 0.9622751032835617  valid acc: 0.62  train Acc: 0.6057142857142858\n",
      "epoch: 285 training Loss: 0.9631017366319181 validation Loss: 0.9620182038100381  valid acc: 0.62  train Acc: 0.6042857142857143\n",
      "epoch: 286 training Loss: 0.9628889481503888 validation Loss: 0.9617615005303528  valid acc: 0.62  train Acc: 0.6028571428571429\n",
      "epoch: 287 training Loss: 0.9626758618239127 validation Loss: 0.9615049846917926  valid acc: 0.62  train Acc: 0.6028571428571429\n",
      "epoch: 288 training Loss: 0.9624624729752754 validation Loss: 0.9612486476365996  valid acc: 0.62  train Acc: 0.6028571428571429\n",
      "epoch: 289 training Loss: 0.9622479507516081 validation Loss: 0.9609924656006565  valid acc: 0.62  train Acc: 0.6028571428571429\n",
      "epoch: 290 training Loss: 0.9620329556302777 validation Loss: 0.9607364399060327  valid acc: 0.62  train Acc: 0.6042857142857143\n",
      "epoch: 291 training Loss: 0.9618176313545093 validation Loss: 0.9604728816658  valid acc: 0.62  train Acc: 0.6042857142857143\n",
      "epoch: 292 training Loss: 0.9616019735376459 validation Loss: 0.9602075240650898  valid acc: 0.62  train Acc: 0.6042857142857143\n",
      "epoch: 293 training Loss: 0.9613859778720706 validation Loss: 0.9599421882849951  valid acc: 0.62  train Acc: 0.6057142857142858\n",
      "epoch: 294 training Loss: 0.9611696401285779 validation Loss: 0.9596768659149966  valid acc: 0.62  train Acc: 0.6057142857142858\n",
      "epoch: 295 training Loss: 0.9609529561557513 validation Loss: 0.9594115486342151  valid acc: 0.62  train Acc: 0.6057142857142858\n",
      "epoch: 296 training Loss: 0.9607359218793502 validation Loss: 0.9591462282106299  valid acc: 0.62  train Acc: 0.6057142857142858\n",
      "epoch: 297 training Loss: 0.960518346592983 validation Loss: 0.958881425837493  valid acc: 0.62  train Acc: 0.6057142857142858\n",
      "epoch: 298 training Loss: 0.9602987434528488 validation Loss: 0.9586164762587468  valid acc: 0.62  train Acc: 0.6057142857142858\n",
      "epoch: 299 training Loss: 0.9600766205191094 validation Loss: 0.9583512985652006  valid acc: 0.62  train Acc: 0.6057142857142858\n",
      "epoch: 300 training Loss: 0.9598527161769311 validation Loss: 0.9580859942508004  valid acc: 0.62  train Acc: 0.6071428571428571\n",
      "epoch: 301 training Loss: 0.959628357245895 validation Loss: 0.9578205557687761  valid acc: 0.62  train Acc: 0.6071428571428571\n",
      "epoch: 302 training Loss: 0.959403540221003 validation Loss: 0.9575549756556666  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 303 training Loss: 0.959178389454955 validation Loss: 0.9572891684809616  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 304 training Loss: 0.9589542354999429 validation Loss: 0.9570232319058141  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 305 training Loss: 0.9587296345007147 validation Loss: 0.9567571585908553  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 306 training Loss: 0.9585045831187219 validation Loss: 0.9564909412778513  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 307 training Loss: 0.9582792173455794 validation Loss: 0.9562242564205223  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 308 training Loss: 0.9580545965021806 validation Loss: 0.9559573431415406  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 309 training Loss: 0.957830775765067 validation Loss: 0.9556903157156398  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 310 training Loss: 0.9576065560728592 validation Loss: 0.9554228570374503  valid acc: 0.62  train Acc: 0.6114285714285714\n",
      "epoch: 311 training Loss: 0.9573829031034078 validation Loss: 0.9551552944732983  valid acc: 0.62  train Acc: 0.6114285714285714\n",
      "epoch: 312 training Loss: 0.9571588355519389 validation Loss: 0.9548876209417131  valid acc: 0.62  train Acc: 0.6128571428571429\n",
      "epoch: 313 training Loss: 0.9569343502550594 validation Loss: 0.9546198294389251  valid acc: 0.62  train Acc: 0.6128571428571429\n",
      "epoch: 314 training Loss: 0.9567097707727338 validation Loss: 0.9543518192356468  valid acc: 0.62  train Acc: 0.6128571428571429\n",
      "epoch: 315 training Loss: 0.95648594656536 validation Loss: 0.9540837048844172  valid acc: 0.62  train Acc: 0.6114285714285714\n",
      "epoch: 316 training Loss: 0.95626171581724 validation Loss: 0.953815479474744  valid acc: 0.62  train Acc: 0.6114285714285714\n",
      "epoch: 317 training Loss: 0.9560370754999706 validation Loss: 0.9535471361718582  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 318 training Loss: 0.9558120226511295 validation Loss: 0.953278668216235  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 319 training Loss: 0.9555865543738082 validation Loss: 0.9530100689231309  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 320 training Loss: 0.955360667836149 validation Loss: 0.9527413316821249  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 321 training Loss: 0.9551351101447654 validation Loss: 0.9524726244565496  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 322 training Loss: 0.9549098763620054 validation Loss: 0.9522058328471142  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 323 training Loss: 0.9546847158325595 validation Loss: 0.9519385235795371  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 324 training Loss: 0.9544599970291012 validation Loss: 0.951670966016579  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 325 training Loss: 0.9542360994858731 validation Loss: 0.9514033457610571  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 326 training Loss: 0.9540118241635189 validation Loss: 0.9511356562026775  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 327 training Loss: 0.9537871682686473 validation Loss: 0.9508678908009508  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 328 training Loss: 0.9535624452561634 validation Loss: 0.9505998562330911  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 329 training Loss: 0.9533389757574882 validation Loss: 0.9503311274846006  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 330 training Loss: 0.9531171293056949 validation Loss: 0.9500605680175348  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 331 training Loss: 0.9528972811742494 validation Loss: 0.9497870035055844  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 332 training Loss: 0.9526771327813652 validation Loss: 0.949513421290535  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 333 training Loss: 0.9524577874006706 validation Loss: 0.9492398263487015  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 334 training Loss: 0.9522394638074291 validation Loss: 0.9489662596068615  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 335 training Loss: 0.952020868821921 validation Loss: 0.948692713870535  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 336 training Loss: 0.9518019991008506 validation Loss: 0.9484208915838795  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 337 training Loss: 0.9515828513636552 validation Loss: 0.9481501244113607  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 338 training Loss: 0.9513634223921109 validation Loss: 0.9478793856082357  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 339 training Loss: 0.9511437090299322 validation Loss: 0.9476086681734875  valid acc: 0.62  train Acc: 0.6114285714285714\n",
      "epoch: 340 training Loss: 0.9509237081823705 validation Loss: 0.9473379651686219  valid acc: 0.62  train Acc: 0.6114285714285714\n",
      "epoch: 341 training Loss: 0.9507034168158078 validation Loss: 0.9470672697174312  valid acc: 0.62  train Acc: 0.6114285714285714\n",
      "epoch: 342 training Loss: 0.9504828319573504 validation Loss: 0.9467965750057571  valid acc: 0.62  train Acc: 0.6114285714285714\n",
      "epoch: 343 training Loss: 0.9502628683424126 validation Loss: 0.9465256025345888  valid acc: 0.62  train Acc: 0.6114285714285714\n",
      "epoch: 344 training Loss: 0.9500440138082032 validation Loss: 0.9462546783495622  valid acc: 0.62  train Acc: 0.6114285714285714\n",
      "epoch: 345 training Loss: 0.9498248939242977 validation Loss: 0.9459837954511288  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 346 training Loss: 0.949605505585092 validation Loss: 0.94571294690009  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 347 training Loss: 0.949385845744932 validation Loss: 0.9454421258174147  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 348 training Loss: 0.9491659114177191 validation Loss: 0.9451713253840598  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 349 training Loss: 0.9489456996765161 validation Loss: 0.9449005388407937  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 350 training Loss: 0.9487252076531455 validation Loss: 0.9446297594880222  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 351 training Loss: 0.9485044325377853 validation Loss: 0.9443589806856166  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 352 training Loss: 0.9482833715785575 validation Loss: 0.9440881958527431  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 353 training Loss: 0.9480621636139483 validation Loss: 0.9438169353320499  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 354 training Loss: 0.9478412876450056 validation Loss: 0.9435456851428089  valid acc: 0.62  train Acc: 0.6085714285714285\n",
      "epoch: 355 training Loss: 0.9476199153957183 validation Loss: 0.9432753498630194  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 356 training Loss: 0.9473978620241532 validation Loss: 0.9430049783151977  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 357 training Loss: 0.947175506262984 validation Loss: 0.9427345642059455  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 358 training Loss: 0.9469533462235485 validation Loss: 0.9424649901035513  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 359 training Loss: 0.9467308908010279 validation Loss: 0.9421953594545314  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 360 training Loss: 0.9465081238662925 validation Loss: 0.9419250470760947  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 361 training Loss: 0.9462850443445703 validation Loss: 0.9416532318889291  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 362 training Loss: 0.9460616480861269 validation Loss: 0.941381311842404  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 363 training Loss: 0.945837932848851 validation Loss: 0.9411092808829127  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 364 training Loss: 0.9456138964425632 validation Loss: 0.9408371330147389  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 365 training Loss: 0.9453895367285648 validation Loss: 0.9405648622998993  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 366 training Loss: 0.945164926988367 validation Loss: 0.9402922155996153  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 367 training Loss: 0.9449400860777237 validation Loss: 0.9400193487914369  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 368 training Loss: 0.9447165013424648 validation Loss: 0.9397463976227598  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 369 training Loss: 0.9444926088334614 validation Loss: 0.9394733559073006  valid acc: 0.62  train Acc: 0.61\n",
      "epoch: 370 training Loss: 0.9442678972980866 validation Loss: 0.9392000091392231  valid acc: 0.62  train Acc: 0.6114285714285714\n",
      "epoch: 371 training Loss: 0.9440414736371362 validation Loss: 0.9389265173498593  valid acc: 0.62  train Acc: 0.6114285714285714\n",
      "epoch: 372 training Loss: 0.9438147092431041 validation Loss: 0.9386528748079825  valid acc: 0.62  train Acc: 0.6114285714285714\n",
      "epoch: 373 training Loss: 0.9435876021506442 validation Loss: 0.938379075838836  valid acc: 0.62  train Acc: 0.6114285714285714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 374 training Loss: 0.9433601504429078 validation Loss: 0.9381051148239874  valid acc: 0.62  train Acc: 0.6142857142857143\n",
      "epoch: 375 training Loss: 0.9431323522510905 validation Loss: 0.9378309862011823  valid acc: 0.62  train Acc: 0.6142857142857143\n",
      "epoch: 376 training Loss: 0.9429049115825504 validation Loss: 0.9375563439266776  valid acc: 0.62  train Acc: 0.6142857142857143\n",
      "epoch: 377 training Loss: 0.942679726225381 validation Loss: 0.9372816238570503  valid acc: 0.62  train Acc: 0.6157142857142858\n",
      "epoch: 378 training Loss: 0.9424542373961449 validation Loss: 0.9370068198073587  valid acc: 0.62  train Acc: 0.6157142857142858\n",
      "epoch: 379 training Loss: 0.9422284427885771 validation Loss: 0.9367319256502455  valid acc: 0.62  train Acc: 0.6157142857142858\n",
      "epoch: 380 training Loss: 0.9420023401475656 validation Loss: 0.9364569353158244  valid acc: 0.62  train Acc: 0.6157142857142858\n",
      "epoch: 381 training Loss: 0.9417759272686889 validation Loss: 0.9361818427915548  valid acc: 0.62  train Acc: 0.6157142857142858\n",
      "epoch: 382 training Loss: 0.9415492019977525 validation Loss: 0.9359066421221243  valid acc: 0.62  train Acc: 0.6157142857142858\n",
      "epoch: 383 training Loss: 0.9413221622303201 validation Loss: 0.9356313274093232  valid acc: 0.62  train Acc: 0.6142857142857143\n",
      "epoch: 384 training Loss: 0.9410948059112411 validation Loss: 0.9353558928119211  valid acc: 0.62  train Acc: 0.6128571428571429\n",
      "epoch: 385 training Loss: 0.9408671310341742 validation Loss: 0.9350803325455403  valid acc: 0.62  train Acc: 0.6128571428571429\n",
      "epoch: 386 training Loss: 0.9406391356411098 validation Loss: 0.9348046408825291  valid acc: 0.62  train Acc: 0.6128571428571429\n",
      "epoch: 387 training Loss: 0.9404109881955284 validation Loss: 0.9345285681054398  valid acc: 0.62  train Acc: 0.6128571428571429\n",
      "epoch: 388 training Loss: 0.9401832064249712 validation Loss: 0.934252387202672  valid acc: 0.62  train Acc: 0.6128571428571429\n",
      "epoch: 389 training Loss: 0.9399551137030516 validation Loss: 0.9339760923366985  valid acc: 0.62  train Acc: 0.6128571428571429\n",
      "epoch: 390 training Loss: 0.9397267080516807 validation Loss: 0.9336996777272121  valid acc: 0.62  train Acc: 0.6128571428571429\n",
      "epoch: 391 training Loss: 0.9394979875408278 validation Loss: 0.9334231376509985  valid acc: 0.62  train Acc: 0.6128571428571429\n",
      "epoch: 392 training Loss: 0.9392689502880246 validation Loss: 0.9331464664418059  valid acc: 0.62  train Acc: 0.6128571428571429\n",
      "epoch: 393 training Loss: 0.9390395944578618 validation Loss: 0.932869658490215  valid acc: 0.62  train Acc: 0.6128571428571429\n",
      "epoch: 394 training Loss: 0.938809918261488 validation Loss: 0.9325927082435046  valid acc: 0.62  train Acc: 0.6128571428571429\n",
      "epoch: 395 training Loss: 0.9385799199561016 validation Loss: 0.9323156102055166  valid acc: 0.62  train Acc: 0.6128571428571429\n",
      "epoch: 396 training Loss: 0.9383489837192431 validation Loss: 0.9320381854842843  valid acc: 0.6266666666666667  train Acc: 0.6142857142857143\n",
      "epoch: 397 training Loss: 0.9381163637791574 validation Loss: 0.9317605698580915  valid acc: 0.6266666666666667  train Acc: 0.6142857142857143\n",
      "epoch: 398 training Loss: 0.9378833922934002 validation Loss: 0.9314827447629519  valid acc: 0.6266666666666667  train Acc: 0.6142857142857143\n",
      "epoch: 399 training Loss: 0.9376500679385056 validation Loss: 0.9312047053125078  valid acc: 0.6266666666666667  train Acc: 0.6142857142857143\n",
      "epoch: 400 training Loss: 0.9374163894317112 validation Loss: 0.9309264466743652  valid acc: 0.6266666666666667  train Acc: 0.6142857142857143\n",
      "epoch: 401 training Loss: 0.9371823555304738 validation Loss: 0.9306479640699578  valid acc: 0.6266666666666667  train Acc: 0.6142857142857143\n",
      "epoch: 402 training Loss: 0.9369479650319841 validation Loss: 0.9303692527744075  valid acc: 0.6266666666666667  train Acc: 0.6142857142857143\n",
      "epoch: 403 training Loss: 0.9367132167726852 validation Loss: 0.9300903081163894  valid acc: 0.6266666666666667  train Acc: 0.6128571428571429\n",
      "epoch: 404 training Loss: 0.9364781096277881 validation Loss: 0.9298111254779898  valid acc: 0.6266666666666667  train Acc: 0.6128571428571429\n",
      "epoch: 405 training Loss: 0.9362426425107916 validation Loss: 0.9295317002945694  valid acc: 0.6266666666666667  train Acc: 0.6128571428571429\n",
      "epoch: 406 training Loss: 0.9360068143729996 validation Loss: 0.9292520280546197  valid acc: 0.6266666666666667  train Acc: 0.6128571428571429\n",
      "epoch: 407 training Loss: 0.9357708833119796 validation Loss: 0.9289720731714824  valid acc: 0.6266666666666667  train Acc: 0.6142857142857143\n",
      "epoch: 408 training Loss: 0.9355356604913035 validation Loss: 0.9286918997256068  valid acc: 0.6266666666666667  train Acc: 0.6142857142857143\n",
      "epoch: 409 training Loss: 0.9353000875525156 validation Loss: 0.9284115029815985  valid acc: 0.6266666666666667  train Acc: 0.6142857142857143\n",
      "epoch: 410 training Loss: 0.9350642045561293 validation Loss: 0.9281306107307933  valid acc: 0.6266666666666667  train Acc: 0.6142857142857143\n",
      "epoch: 411 training Loss: 0.934828811500126 validation Loss: 0.927849523418981  valid acc: 0.6266666666666667  train Acc: 0.6142857142857143\n",
      "epoch: 412 training Loss: 0.9345930784540467 validation Loss: 0.9275682361198047  valid acc: 0.6266666666666667  train Acc: 0.6157142857142858\n",
      "epoch: 413 training Loss: 0.9343570041771901 validation Loss: 0.9272867439629999  valid acc: 0.6266666666666667  train Acc: 0.6157142857142858\n",
      "epoch: 414 training Loss: 0.934120587468605 validation Loss: 0.9270050421342281  valid acc: 0.6266666666666667  train Acc: 0.6157142857142858\n",
      "epoch: 415 training Loss: 0.9338838271665598 validation Loss: 0.9267231258749079  valid acc: 0.6266666666666667  train Acc: 0.6157142857142858\n",
      "epoch: 416 training Loss: 0.9336467221480089 validation Loss: 0.9264409904820464  valid acc: 0.6266666666666667  train Acc: 0.6157142857142858\n",
      "epoch: 417 training Loss: 0.9334092713280633 validation Loss: 0.9261586313080677  valid acc: 0.6266666666666667  train Acc: 0.6157142857142858\n",
      "epoch: 418 training Loss: 0.933171473659461 validation Loss: 0.9258760437606413  valid acc: 0.6266666666666667  train Acc: 0.6157142857142858\n",
      "epoch: 419 training Loss: 0.9329334430672952 validation Loss: 0.9255929598663403  valid acc: 0.6266666666666667  train Acc: 0.6157142857142858\n",
      "epoch: 420 training Loss: 0.9326958101578999 validation Loss: 0.925311277261092  valid acc: 0.6266666666666667  train Acc: 0.6157142857142858\n",
      "epoch: 421 training Loss: 0.9324575649140401 validation Loss: 0.9250307006556477  valid acc: 0.6266666666666667  train Acc: 0.6142857142857143\n",
      "epoch: 422 training Loss: 0.9322184963101745 validation Loss: 0.9247498852183775  valid acc: 0.6266666666666667  train Acc: 0.6157142857142858\n",
      "epoch: 423 training Loss: 0.93197907078479 validation Loss: 0.9244688265481897  valid acc: 0.6266666666666667  train Acc: 0.6157142857142858\n",
      "epoch: 424 training Loss: 0.9317392874974497 validation Loss: 0.9241875202966798  valid acc: 0.6266666666666667  train Acc: 0.6171428571428571\n",
      "epoch: 425 training Loss: 0.9314991456408861 validation Loss: 0.9239059621679835  valid acc: 0.6266666666666667  train Acc: 0.6171428571428571\n",
      "epoch: 426 training Loss: 0.931258644440505 validation Loss: 0.9236241479186261  valid acc: 0.6266666666666667  train Acc: 0.6171428571428571\n",
      "epoch: 427 training Loss: 0.9310178251385355 validation Loss: 0.9233419671973737  valid acc: 0.6266666666666667  train Acc: 0.6171428571428571\n",
      "epoch: 428 training Loss: 0.9307779934500376 validation Loss: 0.9230595622736864  valid acc: 0.6266666666666667  train Acc: 0.6171428571428571\n",
      "epoch: 429 training Loss: 0.9305377833785192 validation Loss: 0.9227768352977546  valid acc: 0.6266666666666667  train Acc: 0.6171428571428571\n",
      "epoch: 430 training Loss: 0.9302950306184055 validation Loss: 0.9224938225251701  valid acc: 0.6266666666666667  train Acc: 0.6171428571428571\n",
      "epoch: 431 training Loss: 0.9300519051752343 validation Loss: 0.9222105200662231  valid acc: 0.6266666666666667  train Acc: 0.6171428571428571\n",
      "epoch: 432 training Loss: 0.9298085081501036 validation Loss: 0.9219265021411664  valid acc: 0.6266666666666667  train Acc: 0.6171428571428571\n",
      "epoch: 433 training Loss: 0.929565660992885 validation Loss: 0.9216422278522445  valid acc: 0.6266666666666667  train Acc: 0.6157142857142858\n",
      "epoch: 434 training Loss: 0.9293224513338987 validation Loss: 0.9213576930183528  valid acc: 0.6266666666666667  train Acc: 0.6171428571428571\n",
      "epoch: 435 training Loss: 0.9290778434485424 validation Loss: 0.9210728212221639  valid acc: 0.6266666666666667  train Acc: 0.6171428571428571\n",
      "epoch: 436 training Loss: 0.928831462656793 validation Loss: 0.9207876270912917  valid acc: 0.6266666666666667  train Acc: 0.6171428571428571\n",
      "epoch: 437 training Loss: 0.9285846954230675 validation Loss: 0.9205021072008835  valid acc: 0.6266666666666667  train Acc: 0.6185714285714285\n",
      "epoch: 438 training Loss: 0.9283375414122689 validation Loss: 0.9202162581747931  valid acc: 0.6266666666666667  train Acc: 0.62\n",
      "epoch: 439 training Loss: 0.9280901411721826 validation Loss: 0.9199296128677158  valid acc: 0.6266666666666667  train Acc: 0.62\n",
      "epoch: 440 training Loss: 0.9278431964100767 validation Loss: 0.9196426736460173  valid acc: 0.6266666666666667  train Acc: 0.62\n",
      "epoch: 441 training Loss: 0.927595874977546 validation Loss: 0.9193554368211854  valid acc: 0.6266666666666667  train Acc: 0.62\n",
      "epoch: 442 training Loss: 0.9273487333515158 validation Loss: 0.9190676286921393  valid acc: 0.6266666666666667  train Acc: 0.62\n",
      "epoch: 443 training Loss: 0.9271020561780665 validation Loss: 0.9187795585888274  valid acc: 0.6266666666666667  train Acc: 0.62\n",
      "epoch: 444 training Loss: 0.9268550029662581 validation Loss: 0.9184921295224637  valid acc: 0.6266666666666667  train Acc: 0.62\n",
      "epoch: 445 training Loss: 0.9266074688863385 validation Loss: 0.9182041889227761  valid acc: 0.6266666666666667  train Acc: 0.62\n",
      "epoch: 446 training Loss: 0.9263602776902008 validation Loss: 0.9179159619683651  valid acc: 0.6266666666666667  train Acc: 0.62\n",
      "epoch: 447 training Loss: 0.9261127136839383 validation Loss: 0.9176274447545473  valid acc: 0.6333333333333333  train Acc: 0.62\n",
      "epoch: 448 training Loss: 0.9258647763099462 validation Loss: 0.9173386334300483  valid acc: 0.6333333333333333  train Acc: 0.62\n",
      "epoch: 449 training Loss: 0.9256164650387358 validation Loss: 0.9170495241968043  valid acc: 0.6333333333333333  train Acc: 0.62\n",
      "epoch: 450 training Loss: 0.925367779368472 validation Loss: 0.9167624895498628  valid acc: 0.6333333333333333  train Acc: 0.62\n",
      "epoch: 451 training Loss: 0.9251188594714943 validation Loss: 0.9164748608927552  valid acc: 0.6333333333333333  train Acc: 0.62\n",
      "epoch: 452 training Loss: 0.9248708182222303 validation Loss: 0.916186984057095  valid acc: 0.6333333333333333  train Acc: 0.62\n",
      "epoch: 453 training Loss: 0.9246224120584637 validation Loss: 0.9158988547235171  valid acc: 0.6333333333333333  train Acc: 0.6214285714285714\n",
      "epoch: 454 training Loss: 0.9243736403206558 validation Loss: 0.9156104686293626  valid acc: 0.6333333333333333  train Acc: 0.6214285714285714\n",
      "epoch: 455 training Loss: 0.9241245023790459 validation Loss: 0.9153218215684598  valid acc: 0.6333333333333333  train Acc: 0.6228571428571429\n",
      "epoch: 456 training Loss: 0.9238749976331332 validation Loss: 0.9150329093909014  valid acc: 0.6333333333333333  train Acc: 0.6228571428571429\n",
      "epoch: 457 training Loss: 0.9236251255111688 validation Loss: 0.9147437280028233  valid acc: 0.6333333333333333  train Acc: 0.6257142857142857\n",
      "epoch: 458 training Loss: 0.9233748854696504 validation Loss: 0.9144542733661832  valid acc: 0.6333333333333333  train Acc: 0.6257142857142857\n",
      "epoch: 459 training Loss: 0.9231246487586993 validation Loss: 0.9141641328828402  valid acc: 0.6333333333333333  train Acc: 0.6257142857142857\n",
      "epoch: 460 training Loss: 0.9228753842534901 validation Loss: 0.9138733644243725  valid acc: 0.6333333333333333  train Acc: 0.6257142857142857\n",
      "epoch: 461 training Loss: 0.9226265125369497 validation Loss: 0.9135824024030738  valid acc: 0.6333333333333333  train Acc: 0.6257142857142857\n",
      "epoch: 462 training Loss: 0.922377292745311 validation Loss: 0.9132912417081739  valid acc: 0.6333333333333333  train Acc: 0.6257142857142857\n",
      "epoch: 463 training Loss: 0.9221277239027303 validation Loss: 0.9129998772952812  valid acc: 0.6333333333333333  train Acc: 0.6257142857142857\n",
      "epoch: 464 training Loss: 0.921877805068938 validation Loss: 0.9127083041860445  valid acc: 0.6333333333333333  train Acc: 0.6257142857142857\n",
      "epoch: 465 training Loss: 0.9216275353385766 validation Loss: 0.9124165174678162  valid acc: 0.6333333333333333  train Acc: 0.6257142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 466 training Loss: 0.9213773038140691 validation Loss: 0.912127508232061  valid acc: 0.6333333333333333  train Acc: 0.6257142857142857\n",
      "epoch: 467 training Loss: 0.9211275062694132 validation Loss: 0.9118402303291078  valid acc: 0.6333333333333333  train Acc: 0.6257142857142857\n",
      "epoch: 468 training Loss: 0.9208773665332878 validation Loss: 0.9115528056069233  valid acc: 0.6333333333333333  train Acc: 0.6271428571428571\n",
      "epoch: 469 training Loss: 0.9206268835139653 validation Loss: 0.9112652282860639  valid acc: 0.6333333333333333  train Acc: 0.6271428571428571\n",
      "epoch: 470 training Loss: 0.9203760561575203 validation Loss: 0.9109774926583001  valid acc: 0.6333333333333333  train Acc: 0.6271428571428571\n",
      "epoch: 471 training Loss: 0.9201248834471021 validation Loss: 0.9106895930862652  valid acc: 0.6333333333333333  train Acc: 0.6271428571428571\n",
      "epoch: 472 training Loss: 0.9198733644022151 validation Loss: 0.9104015240031068  valid acc: 0.6333333333333333  train Acc: 0.6271428571428571\n",
      "epoch: 473 training Loss: 0.9196214980780084 validation Loss: 0.9101132799121358  valid acc: 0.6333333333333333  train Acc: 0.6285714285714286\n",
      "epoch: 474 training Loss: 0.9193692835645756 validation Loss: 0.9098248553864728  valid acc: 0.6333333333333333  train Acc: 0.6285714285714286\n",
      "epoch: 475 training Loss: 0.9191167199862659 validation Loss: 0.9095362450687018  valid acc: 0.6333333333333333  train Acc: 0.6285714285714286\n",
      "epoch: 476 training Loss: 0.9188638065010015 validation Loss: 0.9092474436705118  valid acc: 0.6333333333333333  train Acc: 0.6285714285714286\n",
      "epoch: 477 training Loss: 0.9186105849858193 validation Loss: 0.9089576060283904  valid acc: 0.64  train Acc: 0.6285714285714286\n",
      "epoch: 478 training Loss: 0.9183580707526143 validation Loss: 0.9086676181929844  valid acc: 0.64  train Acc: 0.6285714285714286\n",
      "epoch: 479 training Loss: 0.9181052147586838 validation Loss: 0.908377474277651  valid acc: 0.64  train Acc: 0.6285714285714286\n",
      "epoch: 480 training Loss: 0.9178524042795295 validation Loss: 0.9080863656890862  valid acc: 0.64  train Acc: 0.6285714285714286\n",
      "epoch: 481 training Loss: 0.9175999337529165 validation Loss: 0.90779514114736  valid acc: 0.64  train Acc: 0.63\n",
      "epoch: 482 training Loss: 0.9173471292373443 validation Loss: 0.9075037941486246  valid acc: 0.64  train Acc: 0.63\n",
      "epoch: 483 training Loss: 0.9170939894427996 validation Loss: 0.9072123182734371  valid acc: 0.64  train Acc: 0.63\n",
      "epoch: 484 training Loss: 0.9168405131215681 validation Loss: 0.9069207071862148  valid acc: 0.6333333333333333  train Acc: 0.63\n",
      "epoch: 485 training Loss: 0.9165866990673472 validation Loss: 0.9066289546346854  valid acc: 0.6333333333333333  train Acc: 0.63\n",
      "epoch: 486 training Loss: 0.9163325461143732 validation Loss: 0.9063370544493466  valid acc: 0.6333333333333333  train Acc: 0.63\n",
      "epoch: 487 training Loss: 0.9160780531365615 validation Loss: 0.9060450005429186  valid acc: 0.6333333333333333  train Acc: 0.63\n",
      "epoch: 488 training Loss: 0.9158232190466604 validation Loss: 0.9057527869098069  valid acc: 0.6333333333333333  train Acc: 0.63\n",
      "epoch: 489 training Loss: 0.9155680427954189 validation Loss: 0.9054604076255619  valid acc: 0.6333333333333333  train Acc: 0.63\n",
      "epoch: 490 training Loss: 0.9153125233707688 validation Loss: 0.9051678568463414  valid acc: 0.6333333333333333  train Acc: 0.63\n",
      "epoch: 491 training Loss: 0.9150566761760572 validation Loss: 0.9048749638896318  valid acc: 0.6333333333333333  train Acc: 0.63\n",
      "epoch: 492 training Loss: 0.914800502508501 validation Loss: 0.9045818677349564  valid acc: 0.6333333333333333  train Acc: 0.63\n",
      "epoch: 493 training Loss: 0.9145439838911599 validation Loss: 0.9042885838522697  valid acc: 0.6333333333333333  train Acc: 0.6314285714285715\n",
      "epoch: 494 training Loss: 0.9142871194467834 validation Loss: 0.9039951066971526  valid acc: 0.6333333333333333  train Acc: 0.6314285714285715\n",
      "epoch: 495 training Loss: 0.9140299083315104 validation Loss: 0.9037014308030048  valid acc: 0.6333333333333333  train Acc: 0.6314285714285715\n",
      "epoch: 496 training Loss: 0.9137726980642307 validation Loss: 0.9034067149266757  valid acc: 0.6333333333333333  train Acc: 0.63\n",
      "epoch: 497 training Loss: 0.9135158213991745 validation Loss: 0.9031118443264045  valid acc: 0.6333333333333333  train Acc: 0.63\n",
      "epoch: 498 training Loss: 0.9132586048021953 validation Loss: 0.9028186737579881  valid acc: 0.6333333333333333  train Acc: 0.63\n",
      "epoch: 499 training Loss: 0.9130010472015787 validation Loss: 0.9025256739271296  valid acc: 0.6333333333333333  train Acc: 0.63\n",
      "epoch: 500 training Loss: 0.9127431397432108 validation Loss: 0.9022323333024213  valid acc: 0.6333333333333333  train Acc: 0.63\n",
      "epoch: 501 training Loss: 0.9124848449954692 validation Loss: 0.9019388284787503  valid acc: 0.6333333333333333  train Acc: 0.63\n",
      "epoch: 502 training Loss: 0.9122262057244825 validation Loss: 0.9016451533514441  valid acc: 0.6333333333333333  train Acc: 0.63\n",
      "epoch: 503 training Loss: 0.9119672209899099 validation Loss: 0.9013513019018925  valid acc: 0.6333333333333333  train Acc: 0.63\n",
      "epoch: 504 training Loss: 0.9117078898862169 validation Loss: 0.9010572681969236  valid acc: 0.6333333333333333  train Acc: 0.63\n",
      "epoch: 505 training Loss: 0.9114482115418764 validation Loss: 0.9007630463881806  valid acc: 0.6333333333333333  train Acc: 0.6285714285714286\n",
      "epoch: 506 training Loss: 0.9111881851185861 validation Loss: 0.9004686307115057  valid acc: 0.6333333333333333  train Acc: 0.6285714285714286\n",
      "epoch: 507 training Loss: 0.9109278098105028 validation Loss: 0.9001740154863236  valid acc: 0.64  train Acc: 0.63\n",
      "epoch: 508 training Loss: 0.9106670509101767 validation Loss: 0.8998793060878076  valid acc: 0.64  train Acc: 0.63\n",
      "epoch: 509 training Loss: 0.9104058826453041 validation Loss: 0.8995843829468597  valid acc: 0.64  train Acc: 0.63\n",
      "epoch: 510 training Loss: 0.9101443609996848 validation Loss: 0.8992892406533758  valid acc: 0.64  train Acc: 0.63\n",
      "epoch: 511 training Loss: 0.9098824852907692 validation Loss: 0.8989938738780802  valid acc: 0.64  train Acc: 0.63\n",
      "epoch: 512 training Loss: 0.9096202548646608 validation Loss: 0.8986982773719353  valid acc: 0.64  train Acc: 0.63\n",
      "epoch: 513 training Loss: 0.9093576753753135 validation Loss: 0.8984023322177276  valid acc: 0.64  train Acc: 0.6314285714285715\n",
      "epoch: 514 training Loss: 0.9090947708136254 validation Loss: 0.8981061494122732  valid acc: 0.64  train Acc: 0.6314285714285715\n",
      "epoch: 515 training Loss: 0.9088315106967255 validation Loss: 0.8978097239185304  valid acc: 0.64  train Acc: 0.6328571428571429\n",
      "epoch: 516 training Loss: 0.9085678944705186 validation Loss: 0.8975130507776271  valid acc: 0.64  train Acc: 0.6328571428571429\n",
      "epoch: 517 training Loss: 0.9083039216066002 validation Loss: 0.8972161251082842  valid acc: 0.64  train Acc: 0.6342857142857142\n",
      "epoch: 518 training Loss: 0.9080395916016732 validation Loss: 0.8969189421062433  valid acc: 0.64  train Acc: 0.6342857142857142\n",
      "epoch: 519 training Loss: 0.9077749039769779 validation Loss: 0.8966214970436988  valid acc: 0.64  train Acc: 0.6357142857142857\n",
      "epoch: 520 training Loss: 0.9075098582777392 validation Loss: 0.8963237852687337  valid acc: 0.64  train Acc: 0.6357142857142857\n",
      "epoch: 521 training Loss: 0.9072444540726272 validation Loss: 0.8960258022047576  valid acc: 0.64  train Acc: 0.6357142857142857\n",
      "epoch: 522 training Loss: 0.906978690953235 validation Loss: 0.8957293244488403  valid acc: 0.64  train Acc: 0.6357142857142857\n",
      "epoch: 523 training Loss: 0.9067125685335701 validation Loss: 0.8954328033278401  valid acc: 0.64  train Acc: 0.6357142857142857\n",
      "epoch: 524 training Loss: 0.9064460682433821 validation Loss: 0.8951361288964172  valid acc: 0.64  train Acc: 0.6357142857142857\n",
      "epoch: 525 training Loss: 0.906179101042631 validation Loss: 0.8948391719560129  valid acc: 0.64  train Acc: 0.6357142857142857\n",
      "epoch: 526 training Loss: 0.9059117708682884 validation Loss: 0.8945419281460397  valid acc: 0.64  train Acc: 0.6357142857142857\n",
      "epoch: 527 training Loss: 0.9056440774179861 validation Loss: 0.8942443931784823  valid acc: 0.64  train Acc: 0.6357142857142857\n",
      "epoch: 528 training Loss: 0.9053760204091765 validation Loss: 0.893946562837377  valid acc: 0.64  train Acc: 0.6357142857142857\n",
      "epoch: 529 training Loss: 0.9051075995787171 validation Loss: 0.8936484329782917  valid acc: 0.64  train Acc: 0.6357142857142857\n",
      "epoch: 530 training Loss: 0.9048388146824703 validation Loss: 0.8933499995278089  valid acc: 0.64  train Acc: 0.6357142857142857\n",
      "epoch: 531 training Loss: 0.9045696654949142 validation Loss: 0.8930523934048569  valid acc: 0.64  train Acc: 0.6357142857142857\n",
      "epoch: 532 training Loss: 0.9043001518087688 validation Loss: 0.8927553180320001  valid acc: 0.64  train Acc: 0.6357142857142857\n",
      "epoch: 533 training Loss: 0.9040302734346313 validation Loss: 0.8924579365119882  valid acc: 0.64  train Acc: 0.6357142857142857\n",
      "epoch: 534 training Loss: 0.9037602593014125 validation Loss: 0.8921586441734056  valid acc: 0.64  train Acc: 0.6357142857142857\n",
      "epoch: 535 training Loss: 0.9034902707277866 validation Loss: 0.8918611847264596  valid acc: 0.64  train Acc: 0.6357142857142857\n",
      "epoch: 536 training Loss: 0.9032189959275059 validation Loss: 0.8915615158742173  valid acc: 0.64  train Acc: 0.6357142857142857\n",
      "epoch: 537 training Loss: 0.9029477893634609 validation Loss: 0.8912615713067121  valid acc: 0.64  train Acc: 0.6357142857142857\n",
      "epoch: 538 training Loss: 0.9026762181131268 validation Loss: 0.8909613464925451  valid acc: 0.6466666666666666  train Acc: 0.6357142857142857\n",
      "epoch: 539 training Loss: 0.9024042819369554 validation Loss: 0.890660836979993  valid acc: 0.6466666666666666  train Acc: 0.6357142857142857\n",
      "epoch: 540 training Loss: 0.9021319806160327 validation Loss: 0.8903600383963656  valid acc: 0.6466666666666666  train Acc: 0.6342857142857142\n",
      "epoch: 541 training Loss: 0.9018593139516536 validation Loss: 0.8900589464473742  valid acc: 0.6466666666666666  train Acc: 0.6342857142857142\n",
      "epoch: 542 training Loss: 0.9015862817649135 validation Loss: 0.8897575569165008  valid acc: 0.6466666666666666  train Acc: 0.6342857142857142\n",
      "epoch: 543 training Loss: 0.9013128838963136 validation Loss: 0.8894558656643741  valid acc: 0.6466666666666666  train Acc: 0.6342857142857142\n",
      "epoch: 544 training Loss: 0.9010391202053799 validation Loss: 0.8891538686281475  valid acc: 0.6466666666666666  train Acc: 0.6342857142857142\n",
      "epoch: 545 training Loss: 0.9007649905702978 validation Loss: 0.8888515618208828  valid acc: 0.6466666666666666  train Acc: 0.6357142857142857\n",
      "epoch: 546 training Loss: 0.90049049488756 validation Loss: 0.88854894133094  valid acc: 0.6466666666666666  train Acc: 0.6357142857142857\n",
      "epoch: 547 training Loss: 0.9002156330716242 validation Loss: 0.8882460033213677  valid acc: 0.6466666666666666  train Acc: 0.6357142857142857\n",
      "epoch: 548 training Loss: 0.8999410004701669 validation Loss: 0.8879457709335444  valid acc: 0.6466666666666666  train Acc: 0.6371428571428571\n",
      "epoch: 549 training Loss: 0.8996670710407313 validation Loss: 0.8876469842172937  valid acc: 0.6466666666666666  train Acc: 0.6371428571428571\n",
      "epoch: 550 training Loss: 0.8993923112156764 validation Loss: 0.8873533338821471  valid acc: 0.6466666666666666  train Acc: 0.64\n",
      "epoch: 551 training Loss: 0.8991163976229087 validation Loss: 0.887059477724332  valid acc: 0.6466666666666666  train Acc: 0.64\n",
      "epoch: 552 training Loss: 0.8988400811617407 validation Loss: 0.8867689475333683  valid acc: 0.6466666666666666  train Acc: 0.6414285714285715\n",
      "epoch: 553 training Loss: 0.8985632725624354 validation Loss: 0.8864780142254881  valid acc: 0.6466666666666666  train Acc: 0.6428571428571429\n",
      "epoch: 554 training Loss: 0.8982867067987704 validation Loss: 0.8861892082376319  valid acc: 0.6466666666666666  train Acc: 0.6428571428571429\n",
      "epoch: 555 training Loss: 0.8980107867211053 validation Loss: 0.8858999857856978  valid acc: 0.6466666666666666  train Acc: 0.6442857142857142\n",
      "epoch: 556 training Loss: 0.8977347806240482 validation Loss: 0.8856129909390408  valid acc: 0.6466666666666666  train Acc: 0.6442857142857142\n",
      "epoch: 557 training Loss: 0.897459686242685 validation Loss: 0.8853255675603754  valid acc: 0.6466666666666666  train Acc: 0.6442857142857142\n",
      "epoch: 558 training Loss: 0.8971849813837686 validation Loss: 0.8850400771884396  valid acc: 0.6466666666666666  train Acc: 0.6442857142857142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 559 training Loss: 0.896910714161644 validation Loss: 0.8847541478480044  valid acc: 0.6466666666666666  train Acc: 0.6457142857142857\n",
      "epoch: 560 training Loss: 0.8966361540616817 validation Loss: 0.8844677802458402  valid acc: 0.6466666666666666  train Acc: 0.6457142857142857\n",
      "epoch: 561 training Loss: 0.8963613012052382 validation Loss: 0.8841809750980989  valid acc: 0.6466666666666666  train Acc: 0.6457142857142857\n",
      "epoch: 562 training Loss: 0.8960862580693868 validation Loss: 0.8838999006664858  valid acc: 0.6466666666666666  train Acc: 0.6457142857142857\n",
      "epoch: 563 training Loss: 0.8958105770626923 validation Loss: 0.8836181163818594  valid acc: 0.6466666666666666  train Acc: 0.6457142857142857\n",
      "epoch: 564 training Loss: 0.8955350113528825 validation Loss: 0.8833416454188034  valid acc: 0.6466666666666666  train Acc: 0.6457142857142857\n",
      "epoch: 565 training Loss: 0.8952616432436721 validation Loss: 0.8830657742056948  valid acc: 0.6466666666666666  train Acc: 0.6471428571428571\n",
      "epoch: 566 training Loss: 0.8949888091298708 validation Loss: 0.8827910078305164  valid acc: 0.6466666666666666  train Acc: 0.6471428571428571\n",
      "epoch: 567 training Loss: 0.894715744789243 validation Loss: 0.8825157801823569  valid acc: 0.6466666666666666  train Acc: 0.6485714285714286\n",
      "epoch: 568 training Loss: 0.8944424500945177 validation Loss: 0.8822400935306337  valid acc: 0.6466666666666666  train Acc: 0.65\n",
      "epoch: 569 training Loss: 0.8941691444557173 validation Loss: 0.8819641398813521  valid acc: 0.6466666666666666  train Acc: 0.65\n",
      "epoch: 570 training Loss: 0.893897536757228 validation Loss: 0.881687804334656  valid acc: 0.6466666666666666  train Acc: 0.65\n",
      "epoch: 571 training Loss: 0.893625721043592 validation Loss: 0.8814110879916213  valid acc: 0.6466666666666666  train Acc: 0.65\n",
      "epoch: 572 training Loss: 0.893353697239034 validation Loss: 0.8811339919526067  valid acc: 0.6466666666666666  train Acc: 0.65\n",
      "epoch: 573 training Loss: 0.8930810278517833 validation Loss: 0.8808602950236631  valid acc: 0.6466666666666666  train Acc: 0.6514285714285715\n",
      "epoch: 574 training Loss: 0.8928071136621002 validation Loss: 0.8805887690511167  valid acc: 0.6466666666666666  train Acc: 0.6514285714285715\n",
      "epoch: 575 training Loss: 0.8925337102610326 validation Loss: 0.8803167345989402  valid acc: 0.6466666666666666  train Acc: 0.6514285714285715\n",
      "epoch: 576 training Loss: 0.8922601180253907 validation Loss: 0.8800441952662571  valid acc: 0.6466666666666666  train Acc: 0.6514285714285715\n",
      "epoch: 577 training Loss: 0.8919859143077293 validation Loss: 0.8797742513689807  valid acc: 0.6466666666666666  train Acc: 0.6514285714285715\n",
      "epoch: 578 training Loss: 0.8917103864291577 validation Loss: 0.8795062328089854  valid acc: 0.6466666666666666  train Acc: 0.6528571428571428\n",
      "epoch: 579 training Loss: 0.8914355268621962 validation Loss: 0.879237598280443  valid acc: 0.6466666666666666  train Acc: 0.6528571428571428\n",
      "epoch: 580 training Loss: 0.8911605115415131 validation Loss: 0.8789683537219972  valid acc: 0.6466666666666666  train Acc: 0.6514285714285715\n",
      "epoch: 581 training Loss: 0.8908856479528383 validation Loss: 0.8786988144710388  valid acc: 0.6466666666666666  train Acc: 0.6514285714285715\n",
      "epoch: 582 training Loss: 0.8906127792719755 validation Loss: 0.8784311081123796  valid acc: 0.6466666666666666  train Acc: 0.6514285714285715\n",
      "epoch: 583 training Loss: 0.8903402493534842 validation Loss: 0.8781628726800409  valid acc: 0.6466666666666666  train Acc: 0.6514285714285715\n",
      "epoch: 584 training Loss: 0.8900679642902394 validation Loss: 0.8778966609751758  valid acc: 0.6466666666666666  train Acc: 0.6528571428571428\n",
      "epoch: 585 training Loss: 0.8897959259569858 validation Loss: 0.8776301083830594  valid acc: 0.6533333333333333  train Acc: 0.6542857142857142\n",
      "epoch: 586 training Loss: 0.8895236266286198 validation Loss: 0.8773630291446411  valid acc: 0.6533333333333333  train Acc: 0.6557142857142857\n",
      "epoch: 587 training Loss: 0.8892512373452405 validation Loss: 0.8770954284036824  valid acc: 0.6533333333333333  train Acc: 0.6557142857142857\n",
      "epoch: 588 training Loss: 0.8889787572343234 validation Loss: 0.8768273112582445  valid acc: 0.6533333333333333  train Acc: 0.6557142857142857\n",
      "epoch: 589 training Loss: 0.8887061854529895 validation Loss: 0.8765586827598539  valid acc: 0.6533333333333333  train Acc: 0.6571428571428571\n",
      "epoch: 590 training Loss: 0.8884335345958883 validation Loss: 0.8762893669348185  valid acc: 0.6533333333333333  train Acc: 0.6571428571428571\n",
      "epoch: 591 training Loss: 0.8881608657197984 validation Loss: 0.876019555554059  valid acc: 0.6533333333333333  train Acc: 0.6571428571428571\n",
      "epoch: 592 training Loss: 0.887888103482226 validation Loss: 0.875749253438515  valid acc: 0.6533333333333333  train Acc: 0.6571428571428571\n",
      "epoch: 593 training Loss: 0.8876152471746486 validation Loss: 0.8754784653604808  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 594 training Loss: 0.8873422961136053 validation Loss: 0.8752071960430767  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 595 training Loss: 0.88706924964007 validation Loss: 0.8749354501597664  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 596 training Loss: 0.8867961071188422 validation Loss: 0.874663232333927  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 597 training Loss: 0.8865228679379596 validation Loss: 0.8743905471384661  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 598 training Loss: 0.8862495315081262 validation Loss: 0.8741173990954852  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 599 training Loss: 0.8859764423441794 validation Loss: 0.8738412822281477  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 600 training Loss: 0.8857039339468811 validation Loss: 0.8735652971932814  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 601 training Loss: 0.8854313244638581 validation Loss: 0.8732904547914209  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 602 training Loss: 0.8851586135942954 validation Loss: 0.8730148097759142  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 603 training Loss: 0.8848858010450225 validation Loss: 0.8727377156064148  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 604 training Loss: 0.8846128865305033 validation Loss: 0.8724603353813344  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 605 training Loss: 0.8843401007734863 validation Loss: 0.8721850150022464  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 606 training Loss: 0.8840678409676259 validation Loss: 0.8719093989485978  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 607 training Loss: 0.883795504763835 validation Loss: 0.8716334899704457  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 608 training Loss: 0.8835230917625363 validation Loss: 0.8713572907832611  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 609 training Loss: 0.8832506432767894 validation Loss: 0.8710835417427816  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 610 training Loss: 0.8829788134688178 validation Loss: 0.8708110537810599  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 611 training Loss: 0.8827069319136968 validation Loss: 0.8705383031539259  valid acc: 0.6533333333333333  train Acc: 0.6585714285714286\n",
      "epoch: 612 training Loss: 0.882435042411063 validation Loss: 0.8702673351080504  valid acc: 0.6533333333333333  train Acc: 0.6585714285714286\n",
      "epoch: 613 training Loss: 0.8821637865703349 validation Loss: 0.8699968849780418  valid acc: 0.6533333333333333  train Acc: 0.6585714285714286\n",
      "epoch: 614 training Loss: 0.8818925194317191 validation Loss: 0.8697282694115271  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 615 training Loss: 0.8816213105367485 validation Loss: 0.8694599097488172  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 616 training Loss: 0.8813500760682103 validation Loss: 0.86919132739316  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 617 training Loss: 0.8810788154118842 validation Loss: 0.8689225257152153  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 618 training Loss: 0.8808075279652757 validation Loss: 0.8686535080317742  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 619 training Loss: 0.8805362131374123 validation Loss: 0.8683842776060571  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 620 training Loss: 0.8802648703486478 validation Loss: 0.8681148376480368  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 621 training Loss: 0.879993644481335 validation Loss: 0.8678475094205873  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 622 training Loss: 0.8797230280002407 validation Loss: 0.8675819628946267  valid acc: 0.6533333333333333  train Acc: 0.66\n",
      "epoch: 623 training Loss: 0.879452855602114 validation Loss: 0.8673161857303283  valid acc: 0.6533333333333333  train Acc: 0.6614285714285715\n",
      "epoch: 624 training Loss: 0.8791829641512056 validation Loss: 0.8670542181101943  valid acc: 0.6533333333333333  train Acc: 0.6614285714285715\n",
      "epoch: 625 training Loss: 0.878913729820646 validation Loss: 0.8667920001887948  valid acc: 0.6533333333333333  train Acc: 0.6614285714285715\n",
      "epoch: 626 training Loss: 0.8786445795095663 validation Loss: 0.8665295371865755  valid acc: 0.6533333333333333  train Acc: 0.6628571428571428\n",
      "epoch: 627 training Loss: 0.8783758314410787 validation Loss: 0.8662666787273023  valid acc: 0.6533333333333333  train Acc: 0.6628571428571428\n",
      "epoch: 628 training Loss: 0.8781077254653956 validation Loss: 0.8660036625521285  valid acc: 0.6533333333333333  train Acc: 0.6628571428571428\n",
      "epoch: 629 training Loss: 0.8778397228350842 validation Loss: 0.8657404919876531  valid acc: 0.6533333333333333  train Acc: 0.6642857142857143\n",
      "epoch: 630 training Loss: 0.8775722137090163 validation Loss: 0.865472590621752  valid acc: 0.6533333333333333  train Acc: 0.6642857142857143\n",
      "epoch: 631 training Loss: 0.8773052308947903 validation Loss: 0.8652060444957022  valid acc: 0.6533333333333333  train Acc: 0.6642857142857143\n",
      "epoch: 632 training Loss: 0.8770386149212671 validation Loss: 0.8649392264095161  valid acc: 0.6533333333333333  train Acc: 0.6642857142857143\n",
      "epoch: 633 training Loss: 0.876772326165056 validation Loss: 0.8646707592344692  valid acc: 0.6533333333333333  train Acc: 0.6642857142857143\n",
      "epoch: 634 training Loss: 0.8765065998676762 validation Loss: 0.8644023493505029  valid acc: 0.6533333333333333  train Acc: 0.6642857142857143\n",
      "epoch: 635 training Loss: 0.8762410049964824 validation Loss: 0.8641339951303579  valid acc: 0.6533333333333333  train Acc: 0.6642857142857143\n",
      "epoch: 636 training Loss: 0.8759755404117368 validation Loss: 0.8638656949650798  valid acc: 0.66  train Acc: 0.6642857142857143\n",
      "epoch: 637 training Loss: 0.8757102049657223 validation Loss: 0.8635974472638965  valid acc: 0.66  train Acc: 0.6642857142857143\n",
      "epoch: 638 training Loss: 0.8754449975031945 validation Loss: 0.863329250454099  valid acc: 0.66  train Acc: 0.6642857142857143\n",
      "epoch: 639 training Loss: 0.8751799168618309 validation Loss: 0.8630611029809273  valid acc: 0.66  train Acc: 0.6642857142857143\n",
      "epoch: 640 training Loss: 0.8749149618726763 validation Loss: 0.8627930033074572  valid acc: 0.66  train Acc: 0.6642857142857143\n",
      "epoch: 641 training Loss: 0.8746501765198997 validation Loss: 0.8625272762836037  valid acc: 0.66  train Acc: 0.6657142857142857\n",
      "epoch: 642 training Loss: 0.8743857169449695 validation Loss: 0.862261579155166  valid acc: 0.66  train Acc: 0.6671428571428571\n",
      "epoch: 643 training Loss: 0.8741213745337182 validation Loss: 0.8619961594016735  valid acc: 0.66  train Acc: 0.6671428571428571\n",
      "epoch: 644 training Loss: 0.8738568577659691 validation Loss: 0.8617307560962475  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 645 training Loss: 0.8735924856884864 validation Loss: 0.8614653686777096  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 646 training Loss: 0.8733282569463008 validation Loss: 0.8611999965800767  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 647 training Loss: 0.8730641701798222 validation Loss: 0.8609346392327996  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 648 training Loss: 0.8728000824567602 validation Loss: 0.8606695593977955  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 649 training Loss: 0.8725358831960284 validation Loss: 0.8604044808028258  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 650 training Loss: 0.8722718160541344 validation Loss: 0.8601394031083404  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 651 training Loss: 0.8720078796290237 validation Loss: 0.8598743259660593  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 652 training Loss: 0.8717440725166768 validation Loss: 0.8596092490192908  valid acc: 0.6533333333333333  train Acc: 0.67\n",
      "epoch: 653 training Loss: 0.8714803933115045 validation Loss: 0.8593452985287428  valid acc: 0.6533333333333333  train Acc: 0.67\n",
      "epoch: 654 training Loss: 0.8712169433514149 validation Loss: 0.8590837450022932  valid acc: 0.6533333333333333  train Acc: 0.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 655 training Loss: 0.8709539282286093 validation Loss: 0.8588244367907686  valid acc: 0.6533333333333333  train Acc: 0.67\n",
      "epoch: 656 training Loss: 0.8706910750233828 validation Loss: 0.8585669078273174  valid acc: 0.6533333333333333  train Acc: 0.67\n",
      "epoch: 657 training Loss: 0.8704277653909217 validation Loss: 0.8583148348464729  valid acc: 0.6533333333333333  train Acc: 0.67\n",
      "epoch: 658 training Loss: 0.8701634410269994 validation Loss: 0.8580625388701941  valid acc: 0.6533333333333333  train Acc: 0.67\n",
      "epoch: 659 training Loss: 0.8698993185636055 validation Loss: 0.8578100258716138  valid acc: 0.6533333333333333  train Acc: 0.67\n",
      "epoch: 660 training Loss: 0.869635177137641 validation Loss: 0.857557567372603  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 661 training Loss: 0.8693711077392887 validation Loss: 0.8573021904658921  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 662 training Loss: 0.8691079008339935 validation Loss: 0.857046721314022  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 663 training Loss: 0.8688448791560904 validation Loss: 0.856791162476451  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 664 training Loss: 0.8685820405934659 validation Loss: 0.8565355164328002  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 665 training Loss: 0.868319383045014 validation Loss: 0.8562797855841457  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 666 training Loss: 0.8680569044205433 validation Loss: 0.8560239722543264  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 667 training Loss: 0.867794771294406 validation Loss: 0.8557654454531475  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 668 training Loss: 0.8675333435555285 validation Loss: 0.855506715052832  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 669 training Loss: 0.8672721765297728 validation Loss: 0.8552480396450185  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 670 training Loss: 0.8670111852948177 validation Loss: 0.8549894176721178  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 671 training Loss: 0.8667503678954497 validation Loss: 0.8547308475859431  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 672 training Loss: 0.8664897223770418 validation Loss: 0.8544723278478671  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 673 training Loss: 0.8662292467860113 validation Loss: 0.8542138569289766  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 674 training Loss: 0.8659689391702727 validation Loss: 0.8539554333102287  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 675 training Loss: 0.8657087975796813 validation Loss: 0.8536970554826079  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 676 training Loss: 0.8654488200664708 validation Loss: 0.8534387219472832  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 677 training Loss: 0.8651890046856817 validation Loss: 0.8531805679990863  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 678 training Loss: 0.8649295438100882 validation Loss: 0.8529200040454015  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 679 training Loss: 0.8646706109866215 validation Loss: 0.8526596142955402  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 680 training Loss: 0.8644118417754653 validation Loss: 0.8523993934459139  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 681 training Loss: 0.8641533255259862 validation Loss: 0.8521374258542228  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 682 training Loss: 0.8638951340729386 validation Loss: 0.8518757427886069  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 683 training Loss: 0.863637107474684 validation Loss: 0.8516143353091439  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 684 training Loss: 0.8633792433336076 validation Loss: 0.8513531946880191  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 685 training Loss: 0.8631215392756756 validation Loss: 0.8510923124046919  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 686 training Loss: 0.8628639929496986 validation Loss: 0.8508316801411803  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 687 training Loss: 0.8626066020266542 validation Loss: 0.8505712897774623  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 688 training Loss: 0.8623493641990615 validation Loss: 0.8503111333869869  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 689 training Loss: 0.8620922771804079 validation Loss: 0.8500512032322975  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 690 training Loss: 0.8618354914489501 validation Loss: 0.8497947831060629  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 691 training Loss: 0.8615791483254958 validation Loss: 0.849538545705653  valid acc: 0.66  train Acc: 0.6685714285714286\n",
      "epoch: 692 training Loss: 0.8613229807245947 validation Loss: 0.8492827603630316  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 693 training Loss: 0.8610669863200289 validation Loss: 0.8490277934396714  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 694 training Loss: 0.8608113572302246 validation Loss: 0.8487756861670005  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 695 training Loss: 0.8605561616164735 validation Loss: 0.8485262134505206  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 696 training Loss: 0.8603014823194071 validation Loss: 0.8482768846241286  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 697 training Loss: 0.8600470357742852 validation Loss: 0.8480276967106436  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 698 training Loss: 0.8597928193149879 validation Loss: 0.8477786467697563  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 699 training Loss: 0.8595388921042812 validation Loss: 0.8475326868395618  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 700 training Loss: 0.8592853120002983 validation Loss: 0.8472868332348616  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 701 training Loss: 0.8590319897241836 validation Loss: 0.8470410842803595  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 702 training Loss: 0.858778922390019 validation Loss: 0.846795438299019  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 703 training Loss: 0.8585261071205547 validation Loss: 0.8465498936126677  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 704 training Loss: 0.858273261607586 validation Loss: 0.8463048031408775  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 705 training Loss: 0.8580202903935904 validation Loss: 0.8460597928709016  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 706 training Loss: 0.8577675544209986 validation Loss: 0.8458148615703526  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 707 training Loss: 0.8575150508232803 validation Loss: 0.8455700079959053  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 708 training Loss: 0.8572627767455775 validation Loss: 0.8453252308941296  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 709 training Loss: 0.8570107293450941 validation Loss: 0.8450805290023111  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 710 training Loss: 0.8567589420710741 validation Loss: 0.8448393394900298  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 711 training Loss: 0.8565075024328955 validation Loss: 0.8445981928031993  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 712 training Loss: 0.8562563169843527 validation Loss: 0.8443570889023084  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 713 training Loss: 0.8560053826310656 validation Loss: 0.8441160277012223  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 714 training Loss: 0.8557546962951459 validation Loss: 0.8438750090688629  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 715 training Loss: 0.8555043167214333 validation Loss: 0.8436337149931921  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 716 training Loss: 0.8552543323177206 validation Loss: 0.8433924767915627  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 717 training Loss: 0.8550045892025244 validation Loss: 0.8431512938034588  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 718 training Loss: 0.8547550843770318 validation Loss: 0.8429101653410359  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 719 training Loss: 0.8545058148589981 validation Loss: 0.8426690906904385  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 720 training Loss: 0.8542569099530111 validation Loss: 0.8424305653448183  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 721 training Loss: 0.8540084771096441 validation Loss: 0.8421907819260641  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 722 training Loss: 0.8537603088260399 validation Loss: 0.8419510088826625  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 723 training Loss: 0.853512401824134 validation Loss: 0.8417112466505308  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 724 training Loss: 0.853264752847767 validation Loss: 0.8414714956030132  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 725 training Loss: 0.8530173586627471 validation Loss: 0.8412317560530841  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 726 training Loss: 0.8527702160569179 validation Loss: 0.840992028255518  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 727 training Loss: 0.8525229928324228 validation Loss: 0.840752573854521  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 728 training Loss: 0.8522756283622971 validation Loss: 0.8405131107697817  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 729 training Loss: 0.8520284975085071 validation Loss: 0.8402736396359776  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 730 training Loss: 0.8517815971085199 validation Loss: 0.8400341610221393  valid acc: 0.6666666666666666  train Acc: 0.6685714285714286\n",
      "epoch: 731 training Loss: 0.8515349240238909 validation Loss: 0.8397946754340111  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 732 training Loss: 0.8512885844736289 validation Loss: 0.8395583233300374  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 733 training Loss: 0.851042748015801 validation Loss: 0.8393219313791683  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 734 training Loss: 0.8507971677690468 validation Loss: 0.8390855013724942  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 735 training Loss: 0.8505518402787912 validation Loss: 0.8388490349977062  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 736 training Loss: 0.8503069048930498 validation Loss: 0.838611998771076  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 737 training Loss: 0.8500626426616266 validation Loss: 0.8383749617573265  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 738 training Loss: 0.8498186289108961 validation Loss: 0.8381379243628452  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 739 training Loss: 0.849574860346367 validation Loss: 0.8379008869326875  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 740 training Loss: 0.8493313331958682 validation Loss: 0.8376628432267992  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 741 training Loss: 0.8490876233781358 validation Loss: 0.8374248356812358  valid acc: 0.6666666666666666  train Acc: 0.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 742 training Loss: 0.8488441455307584 validation Loss: 0.8371868632566861  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 743 training Loss: 0.848600896468522 validation Loss: 0.8369489248987599  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 744 training Loss: 0.8483578730295517 validation Loss: 0.8367110195392401  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 745 training Loss: 0.8481146877077548 validation Loss: 0.8364726522016296  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 746 training Loss: 0.8478705870459622 validation Loss: 0.8362343141349177  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 747 training Loss: 0.8476266866034179 validation Loss: 0.8359960040952741  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 748 training Loss: 0.8473830927288973 validation Loss: 0.8357597516989572  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 749 training Loss: 0.8471402725214762 validation Loss: 0.8355216970087292  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 750 training Loss: 0.8468977599138413 validation Loss: 0.8352848491788258  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 751 training Loss: 0.8466556192596507 validation Loss: 0.8350516557100285  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 752 training Loss: 0.8464139720430873 validation Loss: 0.8348189067263724  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 753 training Loss: 0.8461725907402897 validation Loss: 0.8345869202107844  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 754 training Loss: 0.8459314713779295 validation Loss: 0.8343552224401917  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 755 training Loss: 0.845690610015226 validation Loss: 0.8341238056759004  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 756 training Loss: 0.8454500027437093 validation Loss: 0.8338926623675129  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 757 training Loss: 0.8452096456870022 validation Loss: 0.833661785147978  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 758 training Loss: 0.8449692028317957 validation Loss: 0.8334292210141138  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 759 training Loss: 0.8447285222158183 validation Loss: 0.8331961511417713  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 760 training Loss: 0.844488073803611 validation Loss: 0.8329648931163958  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 761 training Loss: 0.8442480255914923 validation Loss: 0.8327366717896597  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 762 training Loss: 0.8440080701233629 validation Loss: 0.83250764208713  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 763 training Loss: 0.8437677169421678 validation Loss: 0.8322788521128394  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 764 training Loss: 0.8435276486776129 validation Loss: 0.8320499682015418  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 765 training Loss: 0.843288004164844 validation Loss: 0.8318213301942342  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 766 training Loss: 0.8430486007766343 validation Loss: 0.8315929312582504  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 767 training Loss: 0.8428094344970253 validation Loss: 0.831364764715398  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 768 training Loss: 0.8425705369562078 validation Loss: 0.8311398417601642  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 769 training Loss: 0.8423321959412315 validation Loss: 0.8309150952589477  valid acc: 0.6666666666666666  train Acc: 0.67\n",
      "epoch: 770 training Loss: 0.8420941192248065 validation Loss: 0.830690521060442  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 771 training Loss: 0.8418563025498772 validation Loss: 0.8304661150697814  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 772 training Loss: 0.8416187416908552 validation Loss: 0.8302418732479042  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 773 training Loss: 0.8413814736906183 validation Loss: 0.8300174587766037  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 774 training Loss: 0.8411446226691296 validation Loss: 0.8297932200787992  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 775 training Loss: 0.8409080179495543 validation Loss: 0.8295691526045776  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 776 training Loss: 0.840671655426823 validation Loss: 0.8293452518809247  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 777 training Loss: 0.8404352888477771 validation Loss: 0.8291204559152635  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 778 training Loss: 0.8401982082923801 validation Loss: 0.8288962730544974  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 779 training Loss: 0.8399613479737565 validation Loss: 0.8286723742666169  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 780 training Loss: 0.8397247286840109 validation Loss: 0.8284483367032212  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 781 training Loss: 0.8394882761930071 validation Loss: 0.8282232163821367  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 782 training Loss: 0.8392510562686849 validation Loss: 0.8279983241490307  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 783 training Loss: 0.8390140359030176 validation Loss: 0.8277736526958318  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 784 training Loss: 0.8387772111124511 validation Loss: 0.8275491949024344  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 785 training Loss: 0.8385405779500198 validation Loss: 0.8273249438313729  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 786 training Loss: 0.838303622893421 validation Loss: 0.827099913338634  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 787 training Loss: 0.8380659791353053 validation Loss: 0.826873685987946  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 788 training Loss: 0.8378285047454127 validation Loss: 0.8266472598045028  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 789 training Loss: 0.8375911958486485 validation Loss: 0.8264210242686864  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 790 training Loss: 0.8373540486074064 validation Loss: 0.8261949721925796  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 791 training Loss: 0.8371170592211945 validation Loss: 0.8259679320111212  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 792 training Loss: 0.8368802239262867 validation Loss: 0.8257388063050279  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 793 training Loss: 0.8366435389953897 validation Loss: 0.8255098228895942  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 794 training Loss: 0.8364070007373307 validation Loss: 0.8252809752429248  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 795 training Loss: 0.8361706054967603 validation Loss: 0.8250522570145582  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 796 training Loss: 0.8359344646886533 validation Loss: 0.824824078143361  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 797 training Loss: 0.835698633287193 validation Loss: 0.824596001372534  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 798 training Loss: 0.8354629348779995 validation Loss: 0.8243680214463062  valid acc: 0.66  train Acc: 0.67\n",
      "epoch: 799 training Loss: 0.8352273659695116 validation Loss: 0.8241401332382124  valid acc: 0.66  train Acc: 0.6714285714285714\n",
      "epoch: 800 training Loss: 0.834991896945997 validation Loss: 0.823911061576509  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 801 training Loss: 0.8347550028973622 validation Loss: 0.8236836045027479  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 802 training Loss: 0.8345182157742852 validation Loss: 0.8234566452799569  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 803 training Loss: 0.8342815925805381 validation Loss: 0.8232295492049173  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 804 training Loss: 0.8340451464828996 validation Loss: 0.8230026076364688  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 805 training Loss: 0.8338087998642836 validation Loss: 0.822775814113874  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 806 training Loss: 0.8335729534485391 validation Loss: 0.8225535035789995  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 807 training Loss: 0.833339131643315 validation Loss: 0.8223313838330584  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 808 training Loss: 0.8331054809076333 validation Loss: 0.8221094474547705  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 809 training Loss: 0.8328719969888324 validation Loss: 0.8218876872168668  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 810 training Loss: 0.8326386756764481 validation Loss: 0.8216660960802398  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 811 training Loss: 0.8324055128018495 validation Loss: 0.8214446671882908  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 812 training Loss: 0.8321725042378934 validation Loss: 0.8212233938614695  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 813 training Loss: 0.831939645898592 validation Loss: 0.8210022695919952  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 814 training Loss: 0.8317069337387923 validation Loss: 0.8207812880387586  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 815 training Loss: 0.8314743637538669 validation Loss: 0.820560443022391  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 816 training Loss: 0.8312420035586118 validation Loss: 0.820340221499064  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 817 training Loss: 0.8310101901170852 validation Loss: 0.8201205773386882  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 818 training Loss: 0.8307788842003262 validation Loss: 0.8199014943745634  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 819 training Loss: 0.8305480427222783 validation Loss: 0.8196824791958991  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 820 training Loss: 0.8303173288013926 validation Loss: 0.8194635283091297  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 821 training Loss: 0.8300870437455321 validation Loss: 0.8192478521080871  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 822 training Loss: 0.8298573861355051 validation Loss: 0.8190357886767665  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 823 training Loss: 0.8296284797953085 validation Loss: 0.818823381012726  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 824 training Loss: 0.8293998996769796 validation Loss: 0.8186109380834135  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 825 training Loss: 0.8291715075276264 validation Loss: 0.818398868489455  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 826 training Loss: 0.8289432987855074 validation Loss: 0.8181871223488479  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 827 training Loss: 0.8287152689417134 validation Loss: 0.817975346406143  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 828 training Loss: 0.8284874135393737 validation Loss: 0.8177635414695007  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 829 training Loss: 0.828259699150939 validation Loss: 0.8175504103309699  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 830 training Loss: 0.8280304099187236 validation Loss: 0.8173372821902343  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 831 training Loss: 0.8278012676831888 validation Loss: 0.8171241561018839  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 832 training Loss: 0.827572219856188 validation Loss: 0.8169196228443244  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 833 training Loss: 0.8273408667048362 validation Loss: 0.8167133138247138  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 834 training Loss: 0.8271099228800108 validation Loss: 0.8165106212676531  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 835 training Loss: 0.8268795371058393 validation Loss: 0.8163117097357712  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 836 training Loss: 0.8266494594123528 validation Loss: 0.8161122878355398  valid acc: 0.66  train Acc: 0.6728571428571428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 837 training Loss: 0.8264195726818007 validation Loss: 0.8159123740599459  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 838 training Loss: 0.8261898703278034 validation Loss: 0.8157119861440149  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 839 training Loss: 0.8259601010473638 validation Loss: 0.8155110355939554  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 840 training Loss: 0.8257298346019522 validation Loss: 0.8153096112038946  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 841 training Loss: 0.8254997259267809 validation Loss: 0.8151077295899201  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 842 training Loss: 0.8252692508265813 validation Loss: 0.8149040636868272  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 843 training Loss: 0.825037447701242 validation Loss: 0.8147000030417195  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 844 training Loss: 0.8248057664437654 validation Loss: 0.8144955608212364  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 845 training Loss: 0.8245742017084295 validation Loss: 0.814290749606198  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 846 training Loss: 0.8243429817671137 validation Loss: 0.8140876021273514  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 847 training Loss: 0.8241103839303162 validation Loss: 0.8138841046548473  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 848 training Loss: 0.8238779142312296 validation Loss: 0.8136802686611446  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 849 training Loss: 0.8236455671247909 validation Loss: 0.8134761050836672  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 850 training Loss: 0.8234133371845596 validation Loss: 0.8132716243442555  valid acc: 0.66  train Acc: 0.6728571428571428\n",
      "epoch: 851 training Loss: 0.8231812190965702 validation Loss: 0.8130668363681609  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 852 training Loss: 0.8229492076536462 validation Loss: 0.8128617506025712  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 853 training Loss: 0.822717297750144 validation Loss: 0.8126573098911385  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 854 training Loss: 0.8224854843770866 validation Loss: 0.8124542327457066  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 855 training Loss: 0.8222537626176641 validation Loss: 0.8122509374888573  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 856 training Loss: 0.8220221276430643 validation Loss: 0.812047431437846  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 857 training Loss: 0.8217905747086103 validation Loss: 0.8118437215243065  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 858 training Loss: 0.8215590991501825 validation Loss: 0.8116398143096765  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 859 training Loss: 0.8213276963808988 validation Loss: 0.811435716000155  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 860 training Loss: 0.8210965334807199 validation Loss: 0.8112317750510003  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 861 training Loss: 0.8208657040681682 validation Loss: 0.8110276360738469  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 862 training Loss: 0.8206349363168307 validation Loss: 0.8108233050759837  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 863 training Loss: 0.8204042258911592 validation Loss: 0.8106187877335671  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 864 training Loss: 0.8201735685234701 validation Loss: 0.8104140894054366  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 865 training Loss: 0.8199429600114523 validation Loss: 0.8102092151464778  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 866 training Loss: 0.8197123962158473 validation Loss: 0.810004169720542  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 867 training Loss: 0.8194818730582897 validation Loss: 0.8097978900878928  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 868 training Loss: 0.819251386519292 validation Loss: 0.8095909719457022  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 869 training Loss: 0.8190210953804704 validation Loss: 0.8093799709695598  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 870 training Loss: 0.8187915116181061 validation Loss: 0.8091690272435774  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 871 training Loss: 0.8185619591505555 validation Loss: 0.8089581345432969  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 872 training Loss: 0.8183323354800632 validation Loss: 0.8087518109848016  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 873 training Loss: 0.8181024925400355 validation Loss: 0.8085453990491583  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 874 training Loss: 0.817872100604626 validation Loss: 0.8083375585169309  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 875 training Loss: 0.8176400495512952 validation Loss: 0.8081300324021038  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 876 training Loss: 0.8174086573360586 validation Loss: 0.8079185378089947  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 877 training Loss: 0.8171776877208417 validation Loss: 0.8077071934854398  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 878 training Loss: 0.8169467383557784 validation Loss: 0.8074966001109178  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 879 training Loss: 0.8167158047788423 validation Loss: 0.807286862294134  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 880 training Loss: 0.8164838465647616 validation Loss: 0.8070760154523067  valid acc: 0.66  train Acc: 0.6742857142857143\n",
      "epoch: 881 training Loss: 0.8162498446693754 validation Loss: 0.8068650722330198  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 882 training Loss: 0.8160151652085376 validation Loss: 0.8066542308806484  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 883 training Loss: 0.815780454177189 validation Loss: 0.806444587032374  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 884 training Loss: 0.81554592008988 validation Loss: 0.8062357964135229  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 885 training Loss: 0.8153112573750635 validation Loss: 0.8060261766298111  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 886 training Loss: 0.8150746644262417 validation Loss: 0.8058164199537612  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 887 training Loss: 0.8148369651632007 validation Loss: 0.8056067158249466  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 888 training Loss: 0.8145991864457018 validation Loss: 0.80539705683481  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 889 training Loss: 0.8143613242038467 validation Loss: 0.805187435807715  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 890 training Loss: 0.8141233744101195 validation Loss: 0.8049765891748089  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 891 training Loss: 0.8138853330786362 validation Loss: 0.8047608700187955  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 892 training Loss: 0.8136471962644273 validation Loss: 0.8045451346192724  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 893 training Loss: 0.8134089600627482 validation Loss: 0.8043293765179125  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 894 training Loss: 0.8131706206084203 validation Loss: 0.8041139126494385  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 895 training Loss: 0.8129321740751921 validation Loss: 0.8038985674024691  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 896 training Loss: 0.8126936166751294 validation Loss: 0.8036831850740985  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 897 training Loss: 0.812454944658021 validation Loss: 0.8034677599251767  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 898 training Loss: 0.8122161543108093 validation Loss: 0.8032522863816196  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 899 training Loss: 0.8119772419570342 validation Loss: 0.8030367590282153  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 900 training Loss: 0.8117382039562968 validation Loss: 0.8028211726026849  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 901 training Loss: 0.8114990367037371 validation Loss: 0.802605521989988  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 902 training Loss: 0.8112599660897415 validation Loss: 0.8023903167963016  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 903 training Loss: 0.8110211176722376 validation Loss: 0.802175015432609  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 904 training Loss: 0.8107826320367 validation Loss: 0.8019628954474515  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 905 training Loss: 0.8105447102283816 validation Loss: 0.8017505825458554  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 906 training Loss: 0.8103066773251023 validation Loss: 0.8015380779560841  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 907 training Loss: 0.8100685294159214 validation Loss: 0.8013253827535526  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 908 training Loss: 0.8098302626328728 validation Loss: 0.8011124978686035  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 909 training Loss: 0.8095918731501174 validation Loss: 0.8008994240939573  valid acc: 0.66  train Acc: 0.6757142857142857\n",
      "epoch: 910 training Loss: 0.8093533571831362 validation Loss: 0.8006861620918478  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 911 training Loss: 0.8091150763010844 validation Loss: 0.8004774680688459  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 912 training Loss: 0.8088770405495006 validation Loss: 0.8002684854607657  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 913 training Loss: 0.8086388915848635 validation Loss: 0.8000595942776113  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 914 training Loss: 0.8083998255813211 validation Loss: 0.7998503642028174  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 915 training Loss: 0.8081608685671927 validation Loss: 0.7996443096544386  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 916 training Loss: 0.8079224900523511 validation Loss: 0.7994378384765914  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 917 training Loss: 0.8076840182552784 validation Loss: 0.7992309614669135  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 918 training Loss: 0.8074454868148992 validation Loss: 0.799024249418002  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 919 training Loss: 0.8072074339922422 validation Loss: 0.7988171280711158  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 920 training Loss: 0.8069692768465376 validation Loss: 0.798607176766163  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 921 training Loss: 0.8067300520380359 validation Loss: 0.7983948143609751  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 922 training Loss: 0.8064895377844836 validation Loss: 0.7981819865081701  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 923 training Loss: 0.8062489218086312 validation Loss: 0.7979695274706391  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 924 training Loss: 0.8060081987482843 validation Loss: 0.7977576235143803  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 925 training Loss: 0.8057673633426017 validation Loss: 0.7975453354324138  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 926 training Loss: 0.8055257521074067 validation Loss: 0.7973325791748888  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 927 training Loss: 0.805283167455346 validation Loss: 0.7971194136721361  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 928 training Loss: 0.8050404428829159 validation Loss: 0.7969058482062937  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 929 training Loss: 0.8047978076274223 validation Loss: 0.7966958840283257  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 930 training Loss: 0.8045556764454957 validation Loss: 0.7964854729403202  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 931 training Loss: 0.8043125870425443 validation Loss: 0.7962831602805069  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 932 training Loss: 0.8040652636735116 validation Loss: 0.7960729562452642  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 933 training Loss: 0.8038158456559205 validation Loss: 0.7958653489513327  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 934 training Loss: 0.8035665340684758 validation Loss: 0.7956567263282461  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 935 training Loss: 0.8033164979461603 validation Loss: 0.7954448798323837  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 936 training Loss: 0.8030649586300832 validation Loss: 0.7952330086090789  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 937 training Loss: 0.8028136859888967 validation Loss: 0.7950202534367308  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 938 training Loss: 0.8025622471217471 validation Loss: 0.7948066419985415  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 939 training Loss: 0.8023106347260606 validation Loss: 0.7945922006377241  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 940 training Loss: 0.8020588417823065 validation Loss: 0.794376954409507  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 941 training Loss: 0.8018074049133173 validation Loss: 0.7941651900135893  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 942 training Loss: 0.8015566605597767 validation Loss: 0.7939525476403548  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 943 training Loss: 0.8013052556099777 validation Loss: 0.79373891909032  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 944 training Loss: 0.8010526257941728 validation Loss: 0.793524427744684  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 945 training Loss: 0.8008000538780564 validation Loss: 0.7933094564351822  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 946 training Loss: 0.8005480891006979 validation Loss: 0.793093288630007  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 947 training Loss: 0.8002962720134357 validation Loss: 0.7928763859650684  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 948 training Loss: 0.8000443979190183 validation Loss: 0.7926635497191438  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 949 training Loss: 0.799792387470686 validation Loss: 0.7924499078892817  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 950 training Loss: 0.7995389274506001 validation Loss: 0.7922327594721705  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 951 training Loss: 0.7992829663383458 validation Loss: 0.7920148999208599  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 952 training Loss: 0.7990260558350052 validation Loss: 0.7917937284562636  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 953 training Loss: 0.7987661530210858 validation Loss: 0.7915719335910917  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 954 training Loss: 0.7985061048721258 validation Loss: 0.7913503336001296  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 955 training Loss: 0.7982470219400316 validation Loss: 0.7911240570698591  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 956 training Loss: 0.7979882260245575 validation Loss: 0.7908974299039349  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 957 training Loss: 0.7977284770821139 validation Loss: 0.7906628907831287  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 958 training Loss: 0.7974675905050703 validation Loss: 0.7904261507799252  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 959 training Loss: 0.7972064271752948 validation Loss: 0.7901889693231463  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 960 training Loss: 0.7969449817097062 validation Loss: 0.7899513475574336  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 961 training Loss: 0.7966832487855767 validation Loss: 0.7897132864330528  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 962 training Loss: 0.7964212231393428 validation Loss: 0.7894747867167325  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 963 training Loss: 0.7961588995654737 validation Loss: 0.7892358490020102  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 964 training Loss: 0.7958962729154 validation Loss: 0.788996473719101  valid acc: 0.6666666666666666  train Acc: 0.6757142857142857\n",
      "epoch: 965 training Loss: 0.7956333380964941 validation Loss: 0.7887566611443119  valid acc: 0.6733333333333333  train Acc: 0.6757142857142857\n",
      "epoch: 966 training Loss: 0.7953700900710979 validation Loss: 0.7885131387146513  valid acc: 0.6733333333333333  train Acc: 0.6757142857142857\n",
      "epoch: 967 training Loss: 0.7951047712141253 validation Loss: 0.7882743110105865  valid acc: 0.6733333333333333  train Acc: 0.6757142857142857\n",
      "epoch: 968 training Loss: 0.7948359645869763 validation Loss: 0.7880355081123398  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 969 training Loss: 0.7945676214597597 validation Loss: 0.7877967586756934  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 970 training Loss: 0.7942995378930665 validation Loss: 0.7875599900932418  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 971 training Loss: 0.7940303295752944 validation Loss: 0.7873184635827521  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 972 training Loss: 0.7937592455255611 validation Loss: 0.78707191684168  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 973 training Loss: 0.7934887515003239 validation Loss: 0.7868245903188845  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 974 training Loss: 0.7932179065214817 validation Loss: 0.7865764967977739  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 975 training Loss: 0.7929467052331154 validation Loss: 0.7863276483248043  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 976 training Loss: 0.7926740915879168 validation Loss: 0.7860905940294772  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 977 training Loss: 0.7923968703694219 validation Loss: 0.7858521529617523  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 978 training Loss: 0.7921192896832927 validation Loss: 0.7856123658387695  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 979 training Loss: 0.791841382533765 validation Loss: 0.7853724514614276  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 980 training Loss: 0.7915629985174126 validation Loss: 0.7851426940543073  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 981 training Loss: 0.7912775526407203 validation Loss: 0.7849123407193239  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 982 training Loss: 0.7909926142401762 validation Loss: 0.7846801571235417  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 983 training Loss: 0.7907073319868323 validation Loss: 0.7844446577774257  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 984 training Loss: 0.7904216937354691 validation Loss: 0.78420463733788  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 985 training Loss: 0.7901356882370316 validation Loss: 0.7839628817247579  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 986 training Loss: 0.789849305052326 validation Loss: 0.7837194474499092  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 987 training Loss: 0.7895625344741901 validation Loss: 0.7834743883897831  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 988 training Loss: 0.7892753674573182 validation Loss: 0.7832279199008658  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 989 training Loss: 0.7889880108537989 validation Loss: 0.7829874206922395  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 990 training Loss: 0.7887004405954461 validation Loss: 0.782745309235341  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 991 training Loss: 0.7884125612356148 validation Loss: 0.7825010432616061  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 992 training Loss: 0.7881245270757465 validation Loss: 0.7822553116441208  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 993 training Loss: 0.7878357519918043 validation Loss: 0.7819996481996554  valid acc: 0.6733333333333333  train Acc: 0.6771428571428572\n",
      "epoch: 994 training Loss: 0.7875446905699947 validation Loss: 0.7817423767749432  valid acc: 0.6733333333333333  train Acc: 0.6785714285714286\n",
      "epoch: 995 training Loss: 0.7872534445920366 validation Loss: 0.7814774098801643  valid acc: 0.6733333333333333  train Acc: 0.6785714285714286\n",
      "epoch: 996 training Loss: 0.786961610628828 validation Loss: 0.7812030789021619  valid acc: 0.6733333333333333  train Acc: 0.6785714285714286\n",
      "epoch: 997 training Loss: 0.786664666767367 validation Loss: 0.780927718944118  valid acc: 0.6733333333333333  train Acc: 0.6785714285714286\n",
      "epoch: 998 training Loss: 0.7863660964570857 validation Loss: 0.7806556061820519  valid acc: 0.6733333333333333  train Acc: 0.6785714285714286\n",
      "epoch: 999 training Loss: 0.7860675200050876 validation Loss: 0.7803795827495388  valid acc: 0.6733333333333333  train Acc: 0.6785714285714286\n",
      "epoch: 1000 training Loss: 0.7857684497930414 validation Loss: 0.7800939369444816  valid acc: 0.6733333333333333  train Acc: 0.6785714285714286\n",
      "epoch: 1001 training Loss: 0.7854691392082344 validation Loss: 0.7798086919122211  valid acc: 0.6733333333333333  train Acc: 0.6785714285714286\n",
      "epoch: 1002 training Loss: 0.7851693883163754 validation Loss: 0.7795222056511909  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1003 training Loss: 0.7848682531050986 validation Loss: 0.7792303670799807  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1004 training Loss: 0.7845667902184749 validation Loss: 0.7789376670332816  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1005 training Loss: 0.7842630721564672 validation Loss: 0.778639986345668  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1006 training Loss: 0.7839541612379672 validation Loss: 0.7783420342488958  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1007 training Loss: 0.7836446723184146 validation Loss: 0.7780432592381124  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1008 training Loss: 0.7833331070025497 validation Loss: 0.7777561276776989  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1009 training Loss: 0.7830145468012477 validation Loss: 0.7774674507310876  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1010 training Loss: 0.782695402679091 validation Loss: 0.7771772661925441  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1011 training Loss: 0.782376102633378 validation Loss: 0.7768872042143691  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1012 training Loss: 0.7820547371460795 validation Loss: 0.7765905851248402  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1013 training Loss: 0.7817298324536738 validation Loss: 0.7762925952312643  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1014 training Loss: 0.7814031836343064 validation Loss: 0.775988270287084  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1015 training Loss: 0.7810665680155915 validation Loss: 0.7756951944995925  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1016 training Loss: 0.7807215957937823 validation Loss: 0.7754034541162995  valid acc: 0.6733333333333333  train Acc: 0.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1017 training Loss: 0.7803773947391167 validation Loss: 0.7751112311604055  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1018 training Loss: 0.7800323305307701 validation Loss: 0.7748187639804773  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1019 training Loss: 0.7796875936671763 validation Loss: 0.7745242865825084  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1020 training Loss: 0.779342745970425 validation Loss: 0.7742296892602519  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1021 training Loss: 0.7789980510135199 validation Loss: 0.7739342047660089  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1022 training Loss: 0.7786550791148815 validation Loss: 0.7736331421541924  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1023 training Loss: 0.778309844093989 validation Loss: 0.7733110390236497  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1024 training Loss: 0.777961015857831 validation Loss: 0.7729879730334712  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1025 training Loss: 0.7776114944155084 validation Loss: 0.7726643209110016  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1026 training Loss: 0.7772612747290039 validation Loss: 0.7723417416496519  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1027 training Loss: 0.7769105234336879 validation Loss: 0.7720129645401594  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1028 training Loss: 0.7765610835791174 validation Loss: 0.7716835599550975  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1029 training Loss: 0.7762109344404254 validation Loss: 0.7713535072080019  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1030 training Loss: 0.7758601076576912 validation Loss: 0.7710245669829223  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1031 training Loss: 0.7755099991536372 validation Loss: 0.7706970089422924  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1032 training Loss: 0.7751599391276107 validation Loss: 0.7703686717477914  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1033 training Loss: 0.7748091612501256 validation Loss: 0.7700395427745577  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1034 training Loss: 0.7744576607246074 validation Loss: 0.7697096099921424  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1035 training Loss: 0.7741054328310614 validation Loss: 0.7693788619364251  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1036 training Loss: 0.7737524729245641 validation Loss: 0.7690438789444416  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1037 training Loss: 0.773396623473481 validation Loss: 0.7686926541238659  valid acc: 0.6733333333333333  train Acc: 0.68\n",
      "epoch: 1038 training Loss: 0.7730372213492137 validation Loss: 0.7683406455780628  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1039 training Loss: 0.772675842505938 validation Loss: 0.7679901466545014  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1040 training Loss: 0.7723150814930965 validation Loss: 0.7676455249052833  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1041 training Loss: 0.7719540994188095 validation Loss: 0.7673021933177029  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1042 training Loss: 0.7715921603587281 validation Loss: 0.7669514283411376  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1043 training Loss: 0.7712256387906015 validation Loss: 0.7665996522428488  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1044 training Loss: 0.7708583118206376 validation Loss: 0.7662468608635721  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1045 training Loss: 0.7704901751525232 validation Loss: 0.7658939300261964  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1046 training Loss: 0.7701216127721534 validation Loss: 0.7655434870344419  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1047 training Loss: 0.7697531969718369 validation Loss: 0.7651919226176922  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1048 training Loss: 0.7693839924918987 validation Loss: 0.7648392394722654  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1049 training Loss: 0.7690139950138278 validation Loss: 0.764485440127967  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1050 training Loss: 0.7686431858798031 validation Loss: 0.7641318407163459  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1051 training Loss: 0.7682696897218869 validation Loss: 0.7637791785875477  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1052 training Loss: 0.7678958284515134 validation Loss: 0.7634277453055766  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1053 training Loss: 0.767521978309354 validation Loss: 0.7630774597235063  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1054 training Loss: 0.7671484951893313 validation Loss: 0.7627259537069873  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1055 training Loss: 0.7667741935548562 validation Loss: 0.7623732353980601  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1056 training Loss: 0.7663991325749723 validation Loss: 0.762021589160946  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1057 training Loss: 0.7660235384121982 validation Loss: 0.7616566793715831  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1058 training Loss: 0.7656446389035958 validation Loss: 0.7612935534429304  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1059 training Loss: 0.765264870559014 validation Loss: 0.7609178404478019  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1060 training Loss: 0.7648823321263609 validation Loss: 0.7605442900503996  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1061 training Loss: 0.7644998549577404 validation Loss: 0.7601702404533269  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1062 training Loss: 0.7641164965457458 validation Loss: 0.7597956591244499  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1063 training Loss: 0.7637322519773015 validation Loss: 0.7594205153409276  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1064 training Loss: 0.7633471165255206 validation Loss: 0.75904490924749  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1065 training Loss: 0.7629610856375869 validation Loss: 0.7586712887339856  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1066 training Loss: 0.7625733982463831 validation Loss: 0.7582972721183083  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1067 training Loss: 0.7621835926717689 validation Loss: 0.7579225468636973  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1068 training Loss: 0.7617928641753832 validation Loss: 0.7575470919750729  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1069 training Loss: 0.7613994224752436 validation Loss: 0.7571643154425813  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1070 training Loss: 0.7610009023540634 validation Loss: 0.7567830990009574  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1071 training Loss: 0.7606025166508629 validation Loss: 0.7563994089605199  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1072 training Loss: 0.7602034937964313 validation Loss: 0.7560049927943897  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1073 training Loss: 0.7598037481638019 validation Loss: 0.7556125357632522  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1074 training Loss: 0.7594030694910066 validation Loss: 0.7552138629968588  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1075 training Loss: 0.7590026981168024 validation Loss: 0.7548145172822909  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1076 training Loss: 0.7586013455898147 validation Loss: 0.7544144705205702  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1077 training Loss: 0.7581992858235616 validation Loss: 0.754016551428455  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1078 training Loss: 0.7577973903105044 validation Loss: 0.7536178262097761  valid acc: 0.6733333333333333  train Acc: 0.6814285714285714\n",
      "epoch: 1079 training Loss: 0.7573945817612152 validation Loss: 0.7532224626243035  valid acc: 0.6733333333333333  train Acc: 0.6828571428571428\n",
      "epoch: 1080 training Loss: 0.7569909107608733 validation Loss: 0.7528226775918099  valid acc: 0.6733333333333333  train Acc: 0.6828571428571428\n",
      "epoch: 1081 training Loss: 0.7565862380850584 validation Loss: 0.7524199692253186  valid acc: 0.6733333333333333  train Acc: 0.6828571428571428\n",
      "epoch: 1082 training Loss: 0.7561806773955271 validation Loss: 0.7520188155334061  valid acc: 0.6733333333333333  train Acc: 0.6828571428571428\n",
      "epoch: 1083 training Loss: 0.7557755107075583 validation Loss: 0.7516189849531755  valid acc: 0.6733333333333333  train Acc: 0.6828571428571428\n",
      "epoch: 1084 training Loss: 0.7553660168699168 validation Loss: 0.751205586213037  valid acc: 0.6733333333333333  train Acc: 0.6828571428571428\n",
      "epoch: 1085 training Loss: 0.7549520118648557 validation Loss: 0.7507917664966932  valid acc: 0.6733333333333333  train Acc: 0.6828571428571428\n",
      "epoch: 1086 training Loss: 0.7545370256435763 validation Loss: 0.7503774766067882  valid acc: 0.6733333333333333  train Acc: 0.6828571428571428\n",
      "epoch: 1087 training Loss: 0.7541210520823441 validation Loss: 0.7499626702529266  valid acc: 0.6733333333333333  train Acc: 0.6857142857142857\n",
      "epoch: 1088 training Loss: 0.7537040854064376 validation Loss: 0.7495473038846565  valid acc: 0.6733333333333333  train Acc: 0.6857142857142857\n",
      "epoch: 1089 training Loss: 0.753286120161837 validation Loss: 0.749131336534928  valid acc: 0.6733333333333333  train Acc: 0.6857142857142857\n",
      "epoch: 1090 training Loss: 0.7528671511899632 validation Loss: 0.7487147296732977  valid acc: 0.6733333333333333  train Acc: 0.6857142857142857\n",
      "epoch: 1091 training Loss: 0.7524471736051315 validation Loss: 0.7482974470682096  valid acc: 0.6733333333333333  train Acc: 0.6857142857142857\n",
      "epoch: 1092 training Loss: 0.752026182774435 validation Loss: 0.7478794546577356  valid acc: 0.6733333333333333  train Acc: 0.6857142857142857\n",
      "epoch: 1093 training Loss: 0.7516041742997878 validation Loss: 0.747460720428203  valid acc: 0.6733333333333333  train Acc: 0.6857142857142857\n",
      "epoch: 1094 training Loss: 0.7511784599229371 validation Loss: 0.7470511341463925  valid acc: 0.6733333333333333  train Acc: 0.6857142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1095 training Loss: 0.7507460574048014 validation Loss: 0.7466312742986821  valid acc: 0.6733333333333333  train Acc: 0.6857142857142857\n",
      "epoch: 1096 training Loss: 0.7503126364346885 validation Loss: 0.7462097322393914  valid acc: 0.6733333333333333  train Acc: 0.6857142857142857\n",
      "epoch: 1097 training Loss: 0.749878194009171 validation Loss: 0.7457865280681327  valid acc: 0.6733333333333333  train Acc: 0.6857142857142857\n",
      "epoch: 1098 training Loss: 0.7494427272632186 validation Loss: 0.745361680936452  valid acc: 0.6733333333333333  train Acc: 0.6857142857142857\n",
      "epoch: 1099 training Loss: 0.7490062334646216 validation Loss: 0.744935209101094  valid acc: 0.6733333333333333  train Acc: 0.6857142857142857\n",
      "epoch: 1100 training Loss: 0.7485688455629811 validation Loss: 0.7445125679044222  valid acc: 0.6733333333333333  train Acc: 0.6857142857142857\n",
      "epoch: 1101 training Loss: 0.7481309794905171 validation Loss: 0.7440881206195772  valid acc: 0.6733333333333333  train Acc: 0.6857142857142857\n",
      "epoch: 1102 training Loss: 0.7476921008034147 validation Loss: 0.7436618948695265  valid acc: 0.6733333333333333  train Acc: 0.6857142857142857\n",
      "epoch: 1103 training Loss: 0.7472493306709131 validation Loss: 0.7432482378370167  valid acc: 0.6733333333333333  train Acc: 0.6857142857142857\n",
      "epoch: 1104 training Loss: 0.7467976966362762 validation Loss: 0.7428247485672058  valid acc: 0.6733333333333333  train Acc: 0.6857142857142857\n",
      "epoch: 1105 training Loss: 0.746348423302629 validation Loss: 0.7424025342986237  valid acc: 0.6733333333333333  train Acc: 0.6857142857142857\n",
      "epoch: 1106 training Loss: 0.7458988227135345 validation Loss: 0.7419827279704537  valid acc: 0.6733333333333333  train Acc: 0.6857142857142857\n",
      "epoch: 1107 training Loss: 0.7454474734383075 validation Loss: 0.7415764692094516  valid acc: 0.6733333333333333  train Acc: 0.6857142857142857\n",
      "epoch: 1108 training Loss: 0.7449832254460947 validation Loss: 0.7411523237935023  valid acc: 0.6733333333333333  train Acc: 0.6871428571428572\n",
      "epoch: 1109 training Loss: 0.7445091015990025 validation Loss: 0.7407294331508041  valid acc: 0.6733333333333333  train Acc: 0.6871428571428572\n",
      "epoch: 1110 training Loss: 0.7440355573298816 validation Loss: 0.7403074223094509  valid acc: 0.6733333333333333  train Acc: 0.6871428571428572\n",
      "epoch: 1111 training Loss: 0.7435618115229782 validation Loss: 0.7398829332440927  valid acc: 0.6733333333333333  train Acc: 0.6871428571428572\n",
      "epoch: 1112 training Loss: 0.7430870743978384 validation Loss: 0.7394560329250318  valid acc: 0.6733333333333333  train Acc: 0.6871428571428572\n",
      "epoch: 1113 training Loss: 0.7426108340070735 validation Loss: 0.7390181969618086  valid acc: 0.6733333333333333  train Acc: 0.6871428571428572\n",
      "epoch: 1114 training Loss: 0.7421282155252786 validation Loss: 0.7385782089412133  valid acc: 0.6733333333333333  train Acc: 0.6871428571428572\n",
      "epoch: 1115 training Loss: 0.7416435983677069 validation Loss: 0.7381364508386946  valid acc: 0.6733333333333333  train Acc: 0.6871428571428572\n",
      "epoch: 1116 training Loss: 0.7411569335819511 validation Loss: 0.7376920020248491  valid acc: 0.6733333333333333  train Acc: 0.6871428571428572\n",
      "epoch: 1117 training Loss: 0.7406692144037726 validation Loss: 0.7372417816344482  valid acc: 0.6733333333333333  train Acc: 0.6871428571428572\n",
      "epoch: 1118 training Loss: 0.740180767277394 validation Loss: 0.7367947385460606  valid acc: 0.6733333333333333  train Acc: 0.6871428571428572\n",
      "epoch: 1119 training Loss: 0.7396896680562808 validation Loss: 0.7363367448144233  valid acc: 0.6733333333333333  train Acc: 0.6871428571428572\n",
      "epoch: 1120 training Loss: 0.7391874088976812 validation Loss: 0.735908963798763  valid acc: 0.6733333333333333  train Acc: 0.6871428571428572\n",
      "epoch: 1121 training Loss: 0.7386649638505892 validation Loss: 0.7354756037835742  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1122 training Loss: 0.7381428627529157 validation Loss: 0.735038896911142  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1123 training Loss: 0.7376199000393727 validation Loss: 0.7345989614541863  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1124 training Loss: 0.7370960632135971 validation Loss: 0.7341619182885354  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1125 training Loss: 0.7365713413931759 validation Loss: 0.7337285854982152  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1126 training Loss: 0.7360464876806336 validation Loss: 0.7332902696379222  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1127 training Loss: 0.735521889070104 validation Loss: 0.7328492534287655  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1128 training Loss: 0.7349962748916012 validation Loss: 0.7324057731634693  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1129 training Loss: 0.7344682495219105 validation Loss: 0.7319597253117938  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1130 training Loss: 0.7339399021640138 validation Loss: 0.7315214068398866  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1131 training Loss: 0.7334122282603172 validation Loss: 0.7310835464841415  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1132 training Loss: 0.7328836597204633 validation Loss: 0.7306430536927871  valid acc: 0.6733333333333333  train Acc: 0.6871428571428572\n",
      "epoch: 1133 training Loss: 0.7323541919928425 validation Loss: 0.7301866258562817  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1134 training Loss: 0.7318238213390305 validation Loss: 0.7297257629419405  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1135 training Loss: 0.731293108823083 validation Loss: 0.7292657826978537  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1136 training Loss: 0.7307617830383405 validation Loss: 0.7288073675579418  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1137 training Loss: 0.7302302487878257 validation Loss: 0.728344323662173  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1138 training Loss: 0.7297002076733492 validation Loss: 0.7278703482566493  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1139 training Loss: 0.7291726715705421 validation Loss: 0.7273882449844126  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1140 training Loss: 0.7286455213921181 validation Loss: 0.726904869940655  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1141 training Loss: 0.7281174344465735 validation Loss: 0.7264191378594123  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1142 training Loss: 0.727588412803375 validation Loss: 0.7259180479647114  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1143 training Loss: 0.7270584586149393 validation Loss: 0.7254156533097499  valid acc: 0.6666666666666666  train Acc: 0.6871428571428572\n",
      "epoch: 1144 training Loss: 0.7265248917283227 validation Loss: 0.7248917447780029  valid acc: 0.6666666666666666  train Acc: 0.6885714285714286\n",
      "epoch: 1145 training Loss: 0.7259798830139489 validation Loss: 0.7243566158465661  valid acc: 0.6666666666666666  train Acc: 0.6885714285714286\n",
      "epoch: 1146 training Loss: 0.7254347693886128 validation Loss: 0.7237996184348324  valid acc: 0.6666666666666666  train Acc: 0.6885714285714286\n",
      "epoch: 1147 training Loss: 0.7248903575505644 validation Loss: 0.7232460713813151  valid acc: 0.6666666666666666  train Acc: 0.6885714285714286\n",
      "epoch: 1148 training Loss: 0.7243461225896984 validation Loss: 0.7226917616258101  valid acc: 0.6666666666666666  train Acc: 0.6885714285714286\n",
      "epoch: 1149 training Loss: 0.7238009623936097 validation Loss: 0.7221366750741085  valid acc: 0.6666666666666666  train Acc: 0.6885714285714286\n",
      "epoch: 1150 training Loss: 0.7232548796936445 validation Loss: 0.721580798869594  valid acc: 0.6666666666666666  train Acc: 0.69\n",
      "epoch: 1151 training Loss: 0.7227078773283359 validation Loss: 0.7210241213232115  valid acc: 0.6666666666666666  train Acc: 0.69\n",
      "epoch: 1152 training Loss: 0.7221600617022844 validation Loss: 0.7204704799353637  valid acc: 0.6666666666666666  train Acc: 0.69\n",
      "epoch: 1153 training Loss: 0.7216121422516323 validation Loss: 0.7199162332651639  valid acc: 0.6666666666666666  train Acc: 0.69\n",
      "epoch: 1154 training Loss: 0.7210630330622413 validation Loss: 0.7193652270729798  valid acc: 0.6666666666666666  train Acc: 0.69\n",
      "epoch: 1155 training Loss: 0.7205142146281158 validation Loss: 0.718813155800136  valid acc: 0.6666666666666666  train Acc: 0.69\n",
      "epoch: 1156 training Loss: 0.7199644767112566 validation Loss: 0.7182628512949691  valid acc: 0.6666666666666666  train Acc: 0.69\n",
      "epoch: 1157 training Loss: 0.7194138225480243 validation Loss: 0.7177135631746576  valid acc: 0.6666666666666666  train Acc: 0.69\n",
      "epoch: 1158 training Loss: 0.718862125986256 validation Loss: 0.7171634687551839  valid acc: 0.6666666666666666  train Acc: 0.69\n",
      "epoch: 1159 training Loss: 0.7183080296278461 validation Loss: 0.7166150199875365  valid acc: 0.6666666666666666  train Acc: 0.6914285714285714\n",
      "epoch: 1160 training Loss: 0.7177530165135535 validation Loss: 0.7160660073957043  valid acc: 0.6666666666666666  train Acc: 0.6914285714285714\n",
      "epoch: 1161 training Loss: 0.717197090080864 validation Loss: 0.7155147732631795  valid acc: 0.6666666666666666  train Acc: 0.6914285714285714\n",
      "epoch: 1162 training Loss: 0.7166405534116194 validation Loss: 0.7149688113999254  valid acc: 0.6666666666666666  train Acc: 0.6914285714285714\n",
      "epoch: 1163 training Loss: 0.7160797917498442 validation Loss: 0.7144343697411297  valid acc: 0.6666666666666666  train Acc: 0.6914285714285714\n",
      "epoch: 1164 training Loss: 0.7155115321433632 validation Loss: 0.7138983378393933  valid acc: 0.6666666666666666  train Acc: 0.6914285714285714\n",
      "epoch: 1165 training Loss: 0.7149417623571479 validation Loss: 0.7133541504933244  valid acc: 0.6666666666666666  train Acc: 0.6914285714285714\n",
      "epoch: 1166 training Loss: 0.7143732726223747 validation Loss: 0.7128080938494892  valid acc: 0.6666666666666666  train Acc: 0.6914285714285714\n",
      "epoch: 1167 training Loss: 0.7138039592595143 validation Loss: 0.7122570918383213  valid acc: 0.6666666666666666  train Acc: 0.6914285714285714\n",
      "epoch: 1168 training Loss: 0.7132338257740259 validation Loss: 0.7117070285082031  valid acc: 0.6666666666666666  train Acc: 0.6914285714285714\n",
      "epoch: 1169 training Loss: 0.7126604094806026 validation Loss: 0.7111493009844099  valid acc: 0.6666666666666666  train Acc: 0.6914285714285714\n",
      "epoch: 1170 training Loss: 0.7120820759919995 validation Loss: 0.710591348677471  valid acc: 0.6666666666666666  train Acc: 0.6914285714285714\n",
      "epoch: 1171 training Loss: 0.7115033460247157 validation Loss: 0.7100301229492615  valid acc: 0.6733333333333333  train Acc: 0.6914285714285714\n",
      "epoch: 1172 training Loss: 0.7109251526927616 validation Loss: 0.709465748319881  valid acc: 0.6733333333333333  train Acc: 0.6914285714285714\n",
      "epoch: 1173 training Loss: 0.710347225730735 validation Loss: 0.7089006041149315  valid acc: 0.68  train Acc: 0.6914285714285714\n",
      "epoch: 1174 training Loss: 0.7097685282662342 validation Loss: 0.7083381416123944  valid acc: 0.68  train Acc: 0.6914285714285714\n",
      "epoch: 1175 training Loss: 0.7091848660665818 validation Loss: 0.707763636262952  valid acc: 0.68  train Acc: 0.6914285714285714\n",
      "epoch: 1176 training Loss: 0.708570012610014 validation Loss: 0.7071881184283696  valid acc: 0.68  train Acc: 0.6914285714285714\n",
      "epoch: 1177 training Loss: 0.7079544644072097 validation Loss: 0.7066131080351687  valid acc: 0.68  train Acc: 0.6914285714285714\n",
      "epoch: 1178 training Loss: 0.7073382258293991 validation Loss: 0.7060376433127482  valid acc: 0.68  train Acc: 0.6914285714285714\n",
      "epoch: 1179 training Loss: 0.7067213013277462 validation Loss: 0.7054612547018488  valid acc: 0.6866666666666666  train Acc: 0.6928571428571428\n",
      "epoch: 1180 training Loss: 0.7061037876638919 validation Loss: 0.7048819602335421  valid acc: 0.6866666666666666  train Acc: 0.6928571428571428\n",
      "epoch: 1181 training Loss: 0.7054863639198306 validation Loss: 0.7043019366435623  valid acc: 0.6933333333333334  train Acc: 0.6928571428571428\n",
      "epoch: 1182 training Loss: 0.7048682852908286 validation Loss: 0.7037211960899199  valid acc: 0.6933333333333334  train Acc: 0.6928571428571428\n",
      "epoch: 1183 training Loss: 0.7042495565386054 validation Loss: 0.7031397502682168  valid acc: 0.7  train Acc: 0.6928571428571428\n",
      "epoch: 1184 training Loss: 0.7036301824686614 validation Loss: 0.7025576104383627  valid acc: 0.7  train Acc: 0.6971428571428572\n",
      "epoch: 1185 training Loss: 0.7030101679286904 validation Loss: 0.7019747874497597  valid acc: 0.7  train Acc: 0.6971428571428572\n",
      "epoch: 1186 training Loss: 0.7023895178070811 validation Loss: 0.7013912917650366  valid acc: 0.7  train Acc: 0.7028571428571428\n",
      "epoch: 1187 training Loss: 0.7017679652601478 validation Loss: 0.7008072372065499  valid acc: 0.7  train Acc: 0.7042857142857143\n",
      "epoch: 1188 training Loss: 0.701144712276951 validation Loss: 0.7002224868895267  valid acc: 0.7  train Acc: 0.7042857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1189 training Loss: 0.7005209064895691 validation Loss: 0.6996306462386872  valid acc: 0.7  train Acc: 0.7057142857142857\n",
      "epoch: 1190 training Loss: 0.699896853714171 validation Loss: 0.6990241513573561  valid acc: 0.7  train Acc: 0.7071428571428572\n",
      "epoch: 1191 training Loss: 0.6992617458317674 validation Loss: 0.6983986007525333  valid acc: 0.7  train Acc: 0.7071428571428572\n",
      "epoch: 1192 training Loss: 0.6986128499478333 validation Loss: 0.6977658817713955  valid acc: 0.7  train Acc: 0.7085714285714285\n",
      "epoch: 1193 training Loss: 0.6979563126177386 validation Loss: 0.6971353456350236  valid acc: 0.7  train Acc: 0.7114285714285714\n",
      "epoch: 1194 training Loss: 0.6972996824540486 validation Loss: 0.6965003706019435  valid acc: 0.7  train Acc: 0.7128571428571429\n",
      "epoch: 1195 training Loss: 0.6966440922636665 validation Loss: 0.6958719065973114  valid acc: 0.7  train Acc: 0.7142857142857143\n",
      "epoch: 1196 training Loss: 0.6959884743891424 validation Loss: 0.695239062202866  valid acc: 0.7  train Acc: 0.7157142857142857\n",
      "epoch: 1197 training Loss: 0.6953337128693612 validation Loss: 0.6946083231063308  valid acc: 0.7  train Acc: 0.7157142857142857\n",
      "epoch: 1198 training Loss: 0.694678509382161 validation Loss: 0.6939725231679856  valid acc: 0.7  train Acc: 0.7185714285714285\n",
      "epoch: 1199 training Loss: 0.694022846570355 validation Loss: 0.6933270795237636  valid acc: 0.7066666666666667  train Acc: 0.7185714285714285\n",
      "epoch: 1200 training Loss: 0.6933667097645053 validation Loss: 0.6926832151790316  valid acc: 0.7066666666666667  train Acc: 0.7185714285714285\n",
      "epoch: 1201 training Loss: 0.6927100866739131 validation Loss: 0.6920407818629198  valid acc: 0.7066666666666667  train Acc: 0.7185714285714285\n",
      "epoch: 1202 training Loss: 0.692052967113629 validation Loss: 0.6914013333246959  valid acc: 0.7066666666666667  train Acc: 0.7185714285714285\n",
      "epoch: 1203 training Loss: 0.6913952416777219 validation Loss: 0.6907650634834368  valid acc: 0.7066666666666667  train Acc: 0.7214285714285714\n",
      "epoch: 1204 training Loss: 0.6907356533913502 validation Loss: 0.6901297802315921  valid acc: 0.7133333333333334  train Acc: 0.7214285714285714\n",
      "epoch: 1205 training Loss: 0.6900755383043031 validation Loss: 0.689495374814683  valid acc: 0.7133333333333334  train Acc: 0.7228571428571429\n",
      "epoch: 1206 training Loss: 0.6894148927405241 validation Loss: 0.6888617460397569  valid acc: 0.7133333333333334  train Acc: 0.7228571428571429\n",
      "epoch: 1207 training Loss: 0.6887537141415899 validation Loss: 0.688232255370815  valid acc: 0.7133333333333334  train Acc: 0.7257142857142858\n",
      "epoch: 1208 training Loss: 0.6880920009408464 validation Loss: 0.6876029584347384  valid acc: 0.7133333333333334  train Acc: 0.7285714285714285\n",
      "epoch: 1209 training Loss: 0.6874302469764921 validation Loss: 0.6869670794112905  valid acc: 0.7133333333333334  train Acc: 0.73\n",
      "epoch: 1210 training Loss: 0.6867695647814628 validation Loss: 0.6863356930238828  valid acc: 0.7133333333333334  train Acc: 0.7314285714285714\n",
      "epoch: 1211 training Loss: 0.6861111356731594 validation Loss: 0.6857064488665943  valid acc: 0.7133333333333334  train Acc: 0.7328571428571429\n",
      "epoch: 1212 training Loss: 0.6854554235411418 validation Loss: 0.6850775853819333  valid acc: 0.72  train Acc: 0.7342857142857143\n",
      "epoch: 1213 training Loss: 0.6847996144202508 validation Loss: 0.6844694908827668  valid acc: 0.72  train Acc: 0.7357142857142858\n",
      "epoch: 1214 training Loss: 0.6841326045963415 validation Loss: 0.6838753394886843  valid acc: 0.72  train Acc: 0.7371428571428571\n",
      "epoch: 1215 training Loss: 0.6834549702983407 validation Loss: 0.6832778354297117  valid acc: 0.72  train Acc: 0.7385714285714285\n",
      "epoch: 1216 training Loss: 0.6827775405368647 validation Loss: 0.6826791257962217  valid acc: 0.72  train Acc: 0.7385714285714285\n",
      "epoch: 1217 training Loss: 0.6820997607831045 validation Loss: 0.6820792527464021  valid acc: 0.72  train Acc: 0.7385714285714285\n",
      "epoch: 1218 training Loss: 0.681421237630739 validation Loss: 0.6814783215001334  valid acc: 0.72  train Acc: 0.7385714285714285\n",
      "epoch: 1219 training Loss: 0.6807415233369326 validation Loss: 0.6808700497504891  valid acc: 0.72  train Acc: 0.7385714285714285\n",
      "epoch: 1220 training Loss: 0.6800627630519731 validation Loss: 0.6802686614858099  valid acc: 0.7266666666666667  train Acc: 0.7414285714285714\n",
      "epoch: 1221 training Loss: 0.6793839648635688 validation Loss: 0.6796566304066813  valid acc: 0.7266666666666667  train Acc: 0.7457142857142857\n",
      "epoch: 1222 training Loss: 0.6786975487509699 validation Loss: 0.6790439647686112  valid acc: 0.7333333333333333  train Acc: 0.7485714285714286\n",
      "epoch: 1223 training Loss: 0.6780108545807852 validation Loss: 0.6784306842512229  valid acc: 0.7333333333333333  train Acc: 0.7485714285714286\n",
      "epoch: 1224 training Loss: 0.6773238868551616 validation Loss: 0.6778168075570715  valid acc: 0.7333333333333333  train Acc: 0.7514285714285714\n",
      "epoch: 1225 training Loss: 0.6766359625883337 validation Loss: 0.6772026424674329  valid acc: 0.7333333333333333  train Acc: 0.7542857142857143\n",
      "epoch: 1226 training Loss: 0.675946951163795 validation Loss: 0.6765878636587307  valid acc: 0.7333333333333333  train Acc: 0.7542857142857143\n",
      "epoch: 1227 training Loss: 0.6752576704737155 validation Loss: 0.6759724895825182  valid acc: 0.74  train Acc: 0.7557142857142857\n",
      "epoch: 1228 training Loss: 0.6745681251859268 validation Loss: 0.6753525465535678  valid acc: 0.74  train Acc: 0.7571428571428571\n",
      "epoch: 1229 training Loss: 0.6738791467535332 validation Loss: 0.6747059831175303  valid acc: 0.7466666666666667  train Acc: 0.7571428571428571\n",
      "epoch: 1230 training Loss: 0.6731841917058444 validation Loss: 0.6740637617725158  valid acc: 0.7466666666666667  train Acc: 0.7571428571428571\n",
      "epoch: 1231 training Loss: 0.6724901225008635 validation Loss: 0.6734214040194917  valid acc: 0.7466666666666667  train Acc: 0.76\n",
      "epoch: 1232 training Loss: 0.6717958015304761 validation Loss: 0.672778902295333  valid acc: 0.7466666666666667  train Acc: 0.76\n",
      "epoch: 1233 training Loss: 0.6711012340215129 validation Loss: 0.6721362497084614  valid acc: 0.7533333333333333  train Acc: 0.7614285714285715\n",
      "epoch: 1234 training Loss: 0.6704060331963096 validation Loss: 0.6714826189843269  valid acc: 0.7533333333333333  train Acc: 0.7614285714285715\n",
      "epoch: 1235 training Loss: 0.6697018479567008 validation Loss: 0.6708226204250797  valid acc: 0.7533333333333333  train Acc: 0.7614285714285715\n",
      "epoch: 1236 training Loss: 0.6689990751347905 validation Loss: 0.670164462757658  valid acc: 0.7533333333333333  train Acc: 0.7614285714285715\n",
      "epoch: 1237 training Loss: 0.6682923977437476 validation Loss: 0.6694992610030877  valid acc: 0.76  train Acc: 0.7628571428571429\n",
      "epoch: 1238 training Loss: 0.6675808997575204 validation Loss: 0.668836151688988  valid acc: 0.7666666666666667  train Acc: 0.7642857142857142\n",
      "epoch: 1239 training Loss: 0.6668692994829449 validation Loss: 0.6681739592703487  valid acc: 0.7666666666666667  train Acc: 0.7671428571428571\n",
      "epoch: 1240 training Loss: 0.666157597667813 validation Loss: 0.6675126241342584  valid acc: 0.7666666666666667  train Acc: 0.7671428571428571\n",
      "epoch: 1241 training Loss: 0.6654463628412947 validation Loss: 0.6668567967266368  valid acc: 0.7666666666666667  train Acc: 0.77\n",
      "epoch: 1242 training Loss: 0.6647371183503513 validation Loss: 0.6662015409387075  valid acc: 0.7666666666666667  train Acc: 0.77\n",
      "epoch: 1243 training Loss: 0.6640277528629562 validation Loss: 0.6655468167980049  valid acc: 0.7666666666666667  train Acc: 0.77\n",
      "epoch: 1244 training Loss: 0.6633182690058591 validation Loss: 0.6648700030471796  valid acc: 0.7666666666666667  train Acc: 0.77\n",
      "epoch: 1245 training Loss: 0.6626086695755424 validation Loss: 0.6641916925546059  valid acc: 0.7666666666666667  train Acc: 0.7714285714285715\n",
      "epoch: 1246 training Loss: 0.6618991126357398 validation Loss: 0.6635185500332451  valid acc: 0.7666666666666667  train Acc: 0.7714285714285715\n",
      "epoch: 1247 training Loss: 0.6611916861275546 validation Loss: 0.6628457643315674  valid acc: 0.7666666666666667  train Acc: 0.7728571428571429\n",
      "epoch: 1248 training Loss: 0.6604850309510715 validation Loss: 0.6621673069378744  valid acc: 0.7666666666666667  train Acc: 0.7728571428571429\n",
      "epoch: 1249 training Loss: 0.6597795152820192 validation Loss: 0.6614895845068399  valid acc: 0.7666666666666667  train Acc: 0.7742857142857142\n",
      "epoch: 1250 training Loss: 0.6590738507305244 validation Loss: 0.6608125504897099  valid acc: 0.7666666666666667  train Acc: 0.7757142857142857\n",
      "epoch: 1251 training Loss: 0.6583684876953607 validation Loss: 0.6601371584659135  valid acc: 0.7666666666666667  train Acc: 0.7757142857142857\n",
      "epoch: 1252 training Loss: 0.657664297790735 validation Loss: 0.6594431998344689  valid acc: 0.7666666666666667  train Acc: 0.7757142857142857\n",
      "epoch: 1253 training Loss: 0.65695994688086 validation Loss: 0.658749757266332  valid acc: 0.7666666666666667  train Acc: 0.7757142857142857\n",
      "epoch: 1254 training Loss: 0.6562554390273182 validation Loss: 0.6580568047640757  valid acc: 0.7666666666666667  train Acc: 0.7785714285714286\n",
      "epoch: 1255 training Loss: 0.655550778395046 validation Loss: 0.6573647253798263  valid acc: 0.7666666666666667  train Acc: 0.78\n",
      "epoch: 1256 training Loss: 0.6548459692416736 validation Loss: 0.6566740531806958  valid acc: 0.7666666666666667  train Acc: 0.7814285714285715\n",
      "epoch: 1257 training Loss: 0.654141015908064 validation Loss: 0.6559720756830177  valid acc: 0.7666666666666667  train Acc: 0.7814285714285715\n",
      "epoch: 1258 training Loss: 0.6534363973064865 validation Loss: 0.6552649113265515  valid acc: 0.7666666666666667  train Acc: 0.7814285714285715\n",
      "epoch: 1259 training Loss: 0.6527328248754969 validation Loss: 0.6545627416297651  valid acc: 0.7666666666666667  train Acc: 0.7814285714285715\n",
      "epoch: 1260 training Loss: 0.6520270982197803 validation Loss: 0.6538498682744822  valid acc: 0.7666666666666667  train Acc: 0.7814285714285715\n",
      "epoch: 1261 training Loss: 0.6513166281816073 validation Loss: 0.6531243428499313  valid acc: 0.7733333333333333  train Acc: 0.7814285714285715\n",
      "epoch: 1262 training Loss: 0.6506045824639409 validation Loss: 0.6523810460862437  valid acc: 0.78  train Acc: 0.7814285714285715\n",
      "epoch: 1263 training Loss: 0.6498861529897494 validation Loss: 0.65163591680328  valid acc: 0.78  train Acc: 0.7814285714285715\n",
      "epoch: 1264 training Loss: 0.6491689372878858 validation Loss: 0.6508918591823433  valid acc: 0.78  train Acc: 0.7814285714285715\n",
      "epoch: 1265 training Loss: 0.6484516983862313 validation Loss: 0.6501488398755525  valid acc: 0.78  train Acc: 0.7828571428571428\n",
      "epoch: 1266 training Loss: 0.6477344392510848 validation Loss: 0.6494068276052811  valid acc: 0.78  train Acc: 0.7828571428571428\n",
      "epoch: 1267 training Loss: 0.6470171629843158 validation Loss: 0.6486657930317593  valid acc: 0.78  train Acc: 0.7828571428571428\n",
      "epoch: 1268 training Loss: 0.6462997518016378 validation Loss: 0.6479283240047693  valid acc: 0.78  train Acc: 0.7828571428571428\n",
      "epoch: 1269 training Loss: 0.6455818525224634 validation Loss: 0.6471783017018808  valid acc: 0.78  train Acc: 0.7828571428571428\n",
      "epoch: 1270 training Loss: 0.6448626455255954 validation Loss: 0.6464075354243632  valid acc: 0.78  train Acc: 0.7828571428571428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1271 training Loss: 0.644134543408231 validation Loss: 0.6456265173516982  valid acc: 0.78  train Acc: 0.7828571428571428\n",
      "epoch: 1272 training Loss: 0.643396308194391 validation Loss: 0.644847421661913  valid acc: 0.78  train Acc: 0.7857142857142857\n",
      "epoch: 1273 training Loss: 0.6426536743958526 validation Loss: 0.6440388610983371  valid acc: 0.78  train Acc: 0.7857142857142857\n",
      "epoch: 1274 training Loss: 0.6418997694847849 validation Loss: 0.6432264520047655  valid acc: 0.78  train Acc: 0.7857142857142857\n",
      "epoch: 1275 training Loss: 0.6411378760783099 validation Loss: 0.642411813113506  valid acc: 0.78  train Acc: 0.7871428571428571\n",
      "epoch: 1276 training Loss: 0.6403777817723599 validation Loss: 0.6415996732950638  valid acc: 0.7933333333333333  train Acc: 0.7885714285714286\n",
      "epoch: 1277 training Loss: 0.6396204070532892 validation Loss: 0.640790953331256  valid acc: 0.7933333333333333  train Acc: 0.7885714285714286\n",
      "epoch: 1278 training Loss: 0.6388633321225508 validation Loss: 0.6399856515106829  valid acc: 0.7933333333333333  train Acc: 0.79\n",
      "epoch: 1279 training Loss: 0.6381056472382836 validation Loss: 0.6391833945109101  valid acc: 0.7933333333333333  train Acc: 0.79\n",
      "epoch: 1280 training Loss: 0.6373487092591208 validation Loss: 0.6383865350570864  valid acc: 0.8  train Acc: 0.79\n",
      "epoch: 1281 training Loss: 0.6365937448247757 validation Loss: 0.6375947021315913  valid acc: 0.8066666666666666  train Acc: 0.79\n",
      "epoch: 1282 training Loss: 0.6358387635287702 validation Loss: 0.636805031251419  valid acc: 0.8066666666666666  train Acc: 0.79\n",
      "epoch: 1283 training Loss: 0.6350841376635286 validation Loss: 0.6360028839700231  valid acc: 0.8066666666666666  train Acc: 0.79\n",
      "epoch: 1284 training Loss: 0.634325870035087 validation Loss: 0.6351932113252269  valid acc: 0.8066666666666666  train Acc: 0.7914285714285715\n",
      "epoch: 1285 training Loss: 0.6335599487516411 validation Loss: 0.6343728400115616  valid acc: 0.8066666666666666  train Acc: 0.7914285714285715\n",
      "epoch: 1286 training Loss: 0.6327823499105332 validation Loss: 0.6335608739432566  valid acc: 0.8066666666666666  train Acc: 0.7928571428571428\n",
      "epoch: 1287 training Loss: 0.6320083187828067 validation Loss: 0.6327540069337219  valid acc: 0.8066666666666666  train Acc: 0.7928571428571428\n",
      "epoch: 1288 training Loss: 0.6312356877928557 validation Loss: 0.6319500691270514  valid acc: 0.8066666666666666  train Acc: 0.7942857142857143\n",
      "epoch: 1289 training Loss: 0.630463558629822 validation Loss: 0.6311489060273558  valid acc: 0.8066666666666666  train Acc: 0.7957142857142857\n",
      "epoch: 1290 training Loss: 0.6296919169925002 validation Loss: 0.6303503747571252  valid acc: 0.8066666666666666  train Acc: 0.7957142857142857\n",
      "epoch: 1291 training Loss: 0.6289207509598996 validation Loss: 0.6295543431398789  valid acc: 0.8066666666666666  train Acc: 0.7971428571428572\n",
      "epoch: 1292 training Loss: 0.6281500506802795 validation Loss: 0.6287606888618814  valid acc: 0.8133333333333334  train Acc: 0.7985714285714286\n",
      "epoch: 1293 training Loss: 0.6273798081004172 validation Loss: 0.6279692987052952  valid acc: 0.8133333333333334  train Acc: 0.7985714285714286\n",
      "epoch: 1294 training Loss: 0.6266100641340796 validation Loss: 0.6271843753980575  valid acc: 0.82  train Acc: 0.7985714285714286\n",
      "epoch: 1295 training Loss: 0.6258428680146139 validation Loss: 0.6264075789475815  valid acc: 0.82  train Acc: 0.7985714285714286\n",
      "epoch: 1296 training Loss: 0.6250697472818667 validation Loss: 0.6256318237272193  valid acc: 0.82  train Acc: 0.7985714285714286\n",
      "epoch: 1297 training Loss: 0.6242965639059608 validation Loss: 0.6248572770091175  valid acc: 0.82  train Acc: 0.7985714285714286\n",
      "epoch: 1298 training Loss: 0.6235219899795179 validation Loss: 0.6240727681364668  valid acc: 0.82  train Acc: 0.7985714285714286\n",
      "epoch: 1299 training Loss: 0.6227412260347248 validation Loss: 0.6232951986135089  valid acc: 0.82  train Acc: 0.7985714285714286\n",
      "epoch: 1300 training Loss: 0.621963962932181 validation Loss: 0.6225235370928954  valid acc: 0.82  train Acc: 0.7985714285714286\n",
      "epoch: 1301 training Loss: 0.6211876843696682 validation Loss: 0.6217529205166167  valid acc: 0.82  train Acc: 0.8\n",
      "epoch: 1302 training Loss: 0.6204119924913202 validation Loss: 0.6209873945389416  valid acc: 0.82  train Acc: 0.8\n",
      "epoch: 1303 training Loss: 0.619638632976414 validation Loss: 0.6202227079678752  valid acc: 0.82  train Acc: 0.8\n",
      "epoch: 1304 training Loss: 0.618865801173482 validation Loss: 0.6194588644649849  valid acc: 0.82  train Acc: 0.8014285714285714\n",
      "epoch: 1305 training Loss: 0.6180935003511822 validation Loss: 0.6186958674172047  valid acc: 0.82  train Acc: 0.8014285714285714\n",
      "epoch: 1306 training Loss: 0.617321733764916 validation Loss: 0.617933719954076  valid acc: 0.82  train Acc: 0.8014285714285714\n",
      "epoch: 1307 training Loss: 0.6165505046563993 validation Loss: 0.6171724249639204  valid acc: 0.82  train Acc: 0.8014285714285714\n",
      "epoch: 1308 training Loss: 0.6157798162532692 validation Loss: 0.6164119851090096  valid acc: 0.82  train Acc: 0.8014285714285714\n",
      "epoch: 1309 training Loss: 0.6150096717687223 validation Loss: 0.6156524028397914  valid acc: 0.82  train Acc: 0.8014285714285714\n",
      "epoch: 1310 training Loss: 0.6142409551972505 validation Loss: 0.6149027058253463  valid acc: 0.82  train Acc: 0.8014285714285714\n",
      "epoch: 1311 training Loss: 0.6134744393821437 validation Loss: 0.6141533716395202  valid acc: 0.82  train Acc: 0.8014285714285714\n",
      "epoch: 1312 training Loss: 0.6127084935180945 validation Loss: 0.613404435308993  valid acc: 0.82  train Acc: 0.8014285714285714\n",
      "epoch: 1313 training Loss: 0.6119433565522397 validation Loss: 0.6126599688946921  valid acc: 0.82  train Acc: 0.8028571428571428\n",
      "epoch: 1314 training Loss: 0.6111801254720127 validation Loss: 0.6119157664112428  valid acc: 0.82  train Acc: 0.8028571428571428\n",
      "epoch: 1315 training Loss: 0.6104174573868406 validation Loss: 0.6111718672968838  valid acc: 0.82  train Acc: 0.8028571428571428\n",
      "epoch: 1316 training Loss: 0.6096557214030026 validation Loss: 0.6104261057414523  valid acc: 0.82  train Acc: 0.8028571428571428\n",
      "epoch: 1317 training Loss: 0.6088948052143214 validation Loss: 0.6096857539943296  valid acc: 0.82  train Acc: 0.8028571428571428\n",
      "epoch: 1318 training Loss: 0.6081251714684222 validation Loss: 0.6089500779687647  valid acc: 0.82  train Acc: 0.8042857142857143\n",
      "epoch: 1319 training Loss: 0.6073514156408064 validation Loss: 0.6082130059179554  valid acc: 0.82  train Acc: 0.8042857142857143\n",
      "epoch: 1320 training Loss: 0.6065785683714664 validation Loss: 0.6074728675291714  valid acc: 0.82  train Acc: 0.8071428571428572\n",
      "epoch: 1321 training Loss: 0.6058035722190936 validation Loss: 0.6067223698087733  valid acc: 0.82  train Acc: 0.8071428571428572\n",
      "epoch: 1322 training Loss: 0.6050244454220985 validation Loss: 0.6059720816626388  valid acc: 0.82  train Acc: 0.8085714285714286\n",
      "epoch: 1323 training Loss: 0.6042462388158606 validation Loss: 0.6052220713418333  valid acc: 0.82  train Acc: 0.8085714285714286\n",
      "epoch: 1324 training Loss: 0.6034698782273838 validation Loss: 0.604465932425563  valid acc: 0.82  train Acc: 0.81\n",
      "epoch: 1325 training Loss: 0.6026962056081218 validation Loss: 0.6037135472063386  valid acc: 0.82  train Acc: 0.81\n",
      "epoch: 1326 training Loss: 0.6019256405167898 validation Loss: 0.6029621710851842  valid acc: 0.82  train Acc: 0.81\n",
      "epoch: 1327 training Loss: 0.6011559182839586 validation Loss: 0.6022118119487073  valid acc: 0.82  train Acc: 0.81\n",
      "epoch: 1328 training Loss: 0.6003870386659793 validation Loss: 0.601463356968498  valid acc: 0.82  train Acc: 0.8114285714285714\n",
      "epoch: 1329 training Loss: 0.5996192803047595 validation Loss: 0.6007128901342259  valid acc: 0.82  train Acc: 0.8114285714285714\n",
      "epoch: 1330 training Loss: 0.598854314426808 validation Loss: 0.5999639101056338  valid acc: 0.82  train Acc: 0.8114285714285714\n",
      "epoch: 1331 training Loss: 0.5980902208685899 validation Loss: 0.5992163922496712  valid acc: 0.82  train Acc: 0.8114285714285714\n",
      "epoch: 1332 training Loss: 0.5973269980227417 validation Loss: 0.5984703130893001  valid acc: 0.82  train Acc: 0.8114285714285714\n",
      "epoch: 1333 training Loss: 0.5965646443965353 validation Loss: 0.5977256502363256  valid acc: 0.82  train Acc: 0.8128571428571428\n",
      "epoch: 1334 training Loss: 0.5958024335083134 validation Loss: 0.596972386086211  valid acc: 0.82  train Acc: 0.8128571428571428\n",
      "epoch: 1335 training Loss: 0.5950339943353307 validation Loss: 0.5962209595604328  valid acc: 0.82  train Acc: 0.8128571428571428\n",
      "epoch: 1336 training Loss: 0.59426487661214 validation Loss: 0.5954589669515489  valid acc: 0.82  train Acc: 0.8128571428571428\n",
      "epoch: 1337 training Loss: 0.5934917872324544 validation Loss: 0.5946994498620317  valid acc: 0.82  train Acc: 0.8128571428571428\n",
      "epoch: 1338 training Loss: 0.5927197248226397 validation Loss: 0.5939423194382706  valid acc: 0.82  train Acc: 0.8128571428571428\n",
      "epoch: 1339 training Loss: 0.5919459583608656 validation Loss: 0.5931779834678554  valid acc: 0.82  train Acc: 0.8128571428571428\n",
      "epoch: 1340 training Loss: 0.5911656267713056 validation Loss: 0.592402876170194  valid acc: 0.82  train Acc: 0.8128571428571428\n",
      "epoch: 1341 training Loss: 0.5903863801348722 validation Loss: 0.5916117304431943  valid acc: 0.82  train Acc: 0.8142857142857143\n",
      "epoch: 1342 training Loss: 0.5896091999848047 validation Loss: 0.5908203585469387  valid acc: 0.82  train Acc: 0.8142857142857143\n",
      "epoch: 1343 training Loss: 0.5888351375225039 validation Loss: 0.5900358835892554  valid acc: 0.82  train Acc: 0.8142857142857143\n",
      "epoch: 1344 training Loss: 0.5880620387076725 validation Loss: 0.5892600527986993  valid acc: 0.82  train Acc: 0.8142857142857143\n",
      "epoch: 1345 training Loss: 0.5872841219755434 validation Loss: 0.5884857249362644  valid acc: 0.82  train Acc: 0.8142857142857143\n",
      "epoch: 1346 training Loss: 0.586507372784376 validation Loss: 0.5877128955864026  valid acc: 0.82  train Acc: 0.8142857142857143\n",
      "epoch: 1347 training Loss: 0.5857317814366213 validation Loss: 0.5869415595335609  valid acc: 0.82  train Acc: 0.8142857142857143\n",
      "epoch: 1348 training Loss: 0.5849573386618319 validation Loss: 0.5861717108503488  valid acc: 0.82  train Acc: 0.8142857142857143\n",
      "epoch: 1349 training Loss: 0.5841840355940202 validation Loss: 0.5854033429788881  valid acc: 0.82  train Acc: 0.8142857142857143\n",
      "epoch: 1350 training Loss: 0.5834118637505795 validation Loss: 0.5846364488057738  valid acc: 0.82  train Acc: 0.8142857142857143\n",
      "epoch: 1351 training Loss: 0.582640815012615 validation Loss: 0.583871020731064  valid acc: 0.82  train Acc: 0.8142857142857143\n",
      "epoch: 1352 training Loss: 0.5818708816065518 validation Loss: 0.5831070507316923  valid acc: 0.82  train Acc: 0.8142857142857143\n",
      "epoch: 1353 training Loss: 0.581102056086897 validation Loss: 0.5823445304196747  valid acc: 0.82  train Acc: 0.8157142857142857\n",
      "epoch: 1354 training Loss: 0.5803343313200573 validation Loss: 0.5815862024864477  valid acc: 0.8266666666666667  train Acc: 0.8157142857142857\n",
      "epoch: 1355 training Loss: 0.5795677004691152 validation Loss: 0.5808309210542391  valid acc: 0.8266666666666667  train Acc: 0.8157142857142857\n",
      "epoch: 1356 training Loss: 0.5788021569794881 validation Loss: 0.5800770268616269  valid acc: 0.8266666666666667  train Acc: 0.8157142857142857\n",
      "epoch: 1357 training Loss: 0.5780376945653943 validation Loss: 0.5793245107840855  valid acc: 0.8266666666666667  train Acc: 0.8157142857142857\n",
      "epoch: 1358 training Loss: 0.5772736861014479 validation Loss: 0.5785771573346258  valid acc: 0.8266666666666667  train Acc: 0.8171428571428572\n",
      "epoch: 1359 training Loss: 0.5765034401737751 validation Loss: 0.577828567319249  valid acc: 0.8266666666666667  train Acc: 0.8171428571428572\n",
      "epoch: 1360 training Loss: 0.5757352041484556 validation Loss: 0.577081058015739  valid acc: 0.8266666666666667  train Acc: 0.8185714285714286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1361 training Loss: 0.5749681483025401 validation Loss: 0.576332394758141  valid acc: 0.8266666666666667  train Acc: 0.8185714285714286\n",
      "epoch: 1362 training Loss: 0.5742022652110135 validation Loss: 0.5755848034400576  valid acc: 0.8266666666666667  train Acc: 0.8185714285714286\n",
      "epoch: 1363 training Loss: 0.5734318073442065 validation Loss: 0.5748241650233816  valid acc: 0.8266666666666667  train Acc: 0.82\n",
      "epoch: 1364 training Loss: 0.5726523266284912 validation Loss: 0.5740598266108211  valid acc: 0.8266666666666667  train Acc: 0.82\n",
      "epoch: 1365 training Loss: 0.5718675258633099 validation Loss: 0.5732978259923747  valid acc: 0.8266666666666667  train Acc: 0.82\n",
      "epoch: 1366 training Loss: 0.5710842644273227 validation Loss: 0.5725359733923333  valid acc: 0.8266666666666667  train Acc: 0.82\n",
      "epoch: 1367 training Loss: 0.570303822682973 validation Loss: 0.5717765613492704  valid acc: 0.8266666666666667  train Acc: 0.82\n",
      "epoch: 1368 training Loss: 0.5695246955062766 validation Loss: 0.5710194982656259  valid acc: 0.8266666666666667  train Acc: 0.82\n",
      "epoch: 1369 training Loss: 0.5687477803584569 validation Loss: 0.5702595996631346  valid acc: 0.8266666666666667  train Acc: 0.8214285714285714\n",
      "epoch: 1370 training Loss: 0.5679742999454832 validation Loss: 0.5695025262453233  valid acc: 0.8266666666666667  train Acc: 0.8228571428571428\n",
      "epoch: 1371 training Loss: 0.5672021813542985 validation Loss: 0.5687481558453126  valid acc: 0.8266666666666667  train Acc: 0.8228571428571428\n",
      "epoch: 1372 training Loss: 0.5664314104725231 validation Loss: 0.5680012785919712  valid acc: 0.8266666666666667  train Acc: 0.8228571428571428\n",
      "epoch: 1373 training Loss: 0.5656619744462454 validation Loss: 0.5672572366453692  valid acc: 0.8266666666666667  train Acc: 0.8228571428571428\n",
      "epoch: 1374 training Loss: 0.5648911172284707 validation Loss: 0.566519668772432  valid acc: 0.8266666666666667  train Acc: 0.8228571428571428\n",
      "epoch: 1375 training Loss: 0.564114315248135 validation Loss: 0.5657756954172843  valid acc: 0.8266666666666667  train Acc: 0.8228571428571428\n",
      "epoch: 1376 training Loss: 0.5633330736694352 validation Loss: 0.5650341061773015  valid acc: 0.8266666666666667  train Acc: 0.8228571428571428\n",
      "epoch: 1377 training Loss: 0.5625533477905341 validation Loss: 0.5642948277072518  valid acc: 0.8266666666666667  train Acc: 0.8228571428571428\n",
      "epoch: 1378 training Loss: 0.5617751249421824 validation Loss: 0.5635577913157237  valid acc: 0.8266666666666667  train Acc: 0.8242857142857143\n",
      "epoch: 1379 training Loss: 0.5609983930351268 validation Loss: 0.5628229326264937  valid acc: 0.8266666666666667  train Acc: 0.8242857142857143\n",
      "epoch: 1380 training Loss: 0.5602231405090939 validation Loss: 0.5620901912664521  valid acc: 0.8266666666666667  train Acc: 0.8242857142857143\n",
      "epoch: 1381 training Loss: 0.5594503821307139 validation Loss: 0.5613652438051747  valid acc: 0.8266666666666667  train Acc: 0.8242857142857143\n",
      "epoch: 1382 training Loss: 0.5586810801209641 validation Loss: 0.5606361992846796  valid acc: 0.8266666666666667  train Acc: 0.8257142857142857\n",
      "epoch: 1383 training Loss: 0.5579157333122072 validation Loss: 0.5599075033069782  valid acc: 0.8266666666666667  train Acc: 0.8257142857142857\n",
      "epoch: 1384 training Loss: 0.5571521056166769 validation Loss: 0.5591599711321099  valid acc: 0.8266666666666667  train Acc: 0.8257142857142857\n",
      "epoch: 1385 training Loss: 0.5563899616222823 validation Loss: 0.5583996608769481  valid acc: 0.8266666666666667  train Acc: 0.8257142857142857\n",
      "epoch: 1386 training Loss: 0.5556292892434621 validation Loss: 0.5576417538269065  valid acc: 0.8266666666666667  train Acc: 0.8257142857142857\n",
      "epoch: 1387 training Loss: 0.5548698371293771 validation Loss: 0.5568865482343957  valid acc: 0.8266666666666667  train Acc: 0.8257142857142857\n",
      "epoch: 1388 training Loss: 0.5541108475932062 validation Loss: 0.5561335452549607  valid acc: 0.8266666666666667  train Acc: 0.8257142857142857\n",
      "epoch: 1389 training Loss: 0.5533532922910237 validation Loss: 0.5553826876350451  valid acc: 0.8266666666666667  train Acc: 0.8271428571428572\n",
      "epoch: 1390 training Loss: 0.5525971617934234 validation Loss: 0.5546339218766542  valid acc: 0.8266666666666667  train Acc: 0.8271428571428572\n",
      "epoch: 1391 training Loss: 0.5518424914557747 validation Loss: 0.5538841455965081  valid acc: 0.8266666666666667  train Acc: 0.8271428571428572\n",
      "epoch: 1392 training Loss: 0.5510864708252433 validation Loss: 0.5531293475422473  valid acc: 0.8266666666666667  train Acc: 0.8271428571428572\n",
      "epoch: 1393 training Loss: 0.5503289624078289 validation Loss: 0.5523860856923669  valid acc: 0.8266666666666667  train Acc: 0.8271428571428572\n",
      "epoch: 1394 training Loss: 0.5495799245852527 validation Loss: 0.5516450614911488  valid acc: 0.8266666666666667  train Acc: 0.8271428571428572\n",
      "epoch: 1395 training Loss: 0.5488323437492931 validation Loss: 0.5509062125338557  valid acc: 0.8266666666666667  train Acc: 0.8271428571428572\n",
      "epoch: 1396 training Loss: 0.5480862086467968 validation Loss: 0.5501395365361167  valid acc: 0.8266666666666667  train Acc: 0.8271428571428572\n",
      "epoch: 1397 training Loss: 0.5473385602646906 validation Loss: 0.5493649045408545  valid acc: 0.8266666666666667  train Acc: 0.8285714285714286\n",
      "epoch: 1398 training Loss: 0.5465867610895813 validation Loss: 0.5485929297586981  valid acc: 0.8266666666666667  train Acc: 0.8285714285714286\n",
      "epoch: 1399 training Loss: 0.5458364797494383 validation Loss: 0.5478235240663628  valid acc: 0.8266666666666667  train Acc: 0.8285714285714286\n",
      "epoch: 1400 training Loss: 0.5450848729711151 validation Loss: 0.5470540775467184  valid acc: 0.8266666666666667  train Acc: 0.8285714285714286\n",
      "epoch: 1401 training Loss: 0.5443284860718549 validation Loss: 0.5462879931506508  valid acc: 0.8266666666666667  train Acc: 0.8314285714285714\n",
      "epoch: 1402 training Loss: 0.5435736924651253 validation Loss: 0.545524831728336  valid acc: 0.8266666666666667  train Acc: 0.8314285714285714\n",
      "epoch: 1403 training Loss: 0.5428204731810963 validation Loss: 0.5447644852096886  valid acc: 0.8266666666666667  train Acc: 0.8314285714285714\n",
      "epoch: 1404 training Loss: 0.5420694978107488 validation Loss: 0.5439964815570761  valid acc: 0.8266666666666667  train Acc: 0.8314285714285714\n",
      "epoch: 1405 training Loss: 0.5413220199483936 validation Loss: 0.543219085480267  valid acc: 0.8266666666666667  train Acc: 0.8314285714285714\n",
      "epoch: 1406 training Loss: 0.5405761230037681 validation Loss: 0.5424446108363616  valid acc: 0.8266666666666667  train Acc: 0.8328571428571429\n",
      "epoch: 1407 training Loss: 0.5398283307925152 validation Loss: 0.5416451362621006  valid acc: 0.8266666666666667  train Acc: 0.8328571428571429\n",
      "epoch: 1408 training Loss: 0.5390716974044937 validation Loss: 0.5408526395263916  valid acc: 0.8266666666666667  train Acc: 0.8328571428571429\n",
      "epoch: 1409 training Loss: 0.5383181075455398 validation Loss: 0.5400618905271847  valid acc: 0.8266666666666667  train Acc: 0.8328571428571429\n",
      "epoch: 1410 training Loss: 0.5375670521877607 validation Loss: 0.5392749125279814  valid acc: 0.8266666666666667  train Acc: 0.8328571428571429\n",
      "epoch: 1411 training Loss: 0.5368172423556762 validation Loss: 0.5384919689070283  valid acc: 0.8333333333333334  train Acc: 0.8328571428571429\n",
      "epoch: 1412 training Loss: 0.5360681199932055 validation Loss: 0.5377123938437565  valid acc: 0.8333333333333334  train Acc: 0.8328571428571429\n",
      "epoch: 1413 training Loss: 0.5353205634681303 validation Loss: 0.5369360482009857  valid acc: 0.8333333333333334  train Acc: 0.8328571428571429\n",
      "epoch: 1414 training Loss: 0.5345745572633184 validation Loss: 0.5361628037771093  valid acc: 0.8333333333333334  train Acc: 0.8328571428571429\n",
      "epoch: 1415 training Loss: 0.5338300870848137 validation Loss: 0.5353925423703536  valid acc: 0.8333333333333334  train Acc: 0.8328571428571429\n",
      "epoch: 1416 training Loss: 0.533087366939264 validation Loss: 0.5346316869041035  valid acc: 0.8333333333333334  train Acc: 0.8328571428571429\n",
      "epoch: 1417 training Loss: 0.5323480245035775 validation Loss: 0.5338798046225657  valid acc: 0.8333333333333334  train Acc: 0.8328571428571429\n",
      "epoch: 1418 training Loss: 0.5316118956118322 validation Loss: 0.5331287037284751  valid acc: 0.84  train Acc: 0.8328571428571429\n",
      "epoch: 1419 training Loss: 0.5308777184203668 validation Loss: 0.532364289300107  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1420 training Loss: 0.5301473577212772 validation Loss: 0.5316023096176233  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1421 training Loss: 0.5294188079308231 validation Loss: 0.53084935058065  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1422 training Loss: 0.528692750667616 validation Loss: 0.5300987962470377  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1423 training Loss: 0.5279672164759538 validation Loss: 0.5293500483661399  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1424 training Loss: 0.527243235870838 validation Loss: 0.5286031047668649  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1425 training Loss: 0.526520801212756 validation Loss: 0.527857962741145  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1426 training Loss: 0.5257999049772952 validation Loss: 0.5271064987211115  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1427 training Loss: 0.525080539749459 validation Loss: 0.5263549243316448  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1428 training Loss: 0.5243626982184065 validation Loss: 0.5256052734964182  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1429 training Loss: 0.5236463731725727 validation Loss: 0.5248575405986833  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1430 training Loss: 0.5229315574951392 validation Loss: 0.5241117197369433  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1431 training Loss: 0.5222190508611549 validation Loss: 0.5233715720391389  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1432 training Loss: 0.5215088974746013 validation Loss: 0.5226351999878746  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1433 training Loss: 0.520800316944525 validation Loss: 0.5219032600445284  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1434 training Loss: 0.5200937531374049 validation Loss: 0.5211756175660572  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1435 training Loss: 0.5193896395892148 validation Loss: 0.5204492645712188  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1436 training Loss: 0.518686975353671 validation Loss: 0.5197279745702811  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1437 training Loss: 0.5179858797576058 validation Loss: 0.5190074292512802  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1438 training Loss: 0.5172799421882035 validation Loss: 0.5182886729496092  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1439 training Loss: 0.5165754961017001 validation Loss: 0.5175717064107643  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1440 training Loss: 0.5158727025553707 validation Loss: 0.5168640526535366  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1441 training Loss: 0.5151719115774562 validation Loss: 0.5161577247460248  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1442 training Loss: 0.5144726311654974 validation Loss: 0.5154527590298436  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1443 training Loss: 0.5137750040960486 validation Loss: 0.5147569013809  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1444 training Loss: 0.5130796655333622 validation Loss: 0.5140603944076582  valid acc: 0.84  train Acc: 0.8342857142857143\n",
      "epoch: 1445 training Loss: 0.5123868716886857 validation Loss: 0.5133590412673614  valid acc: 0.84  train Acc: 0.8357142857142857\n",
      "epoch: 1446 training Loss: 0.5116986144937765 validation Loss: 0.512659502901764  valid acc: 0.84  train Acc: 0.8357142857142857\n",
      "epoch: 1447 training Loss: 0.511011881869538 validation Loss: 0.5119597312935641  valid acc: 0.84  train Acc: 0.8357142857142857\n",
      "epoch: 1448 training Loss: 0.5103257462532474 validation Loss: 0.5112512965489229  valid acc: 0.84  train Acc: 0.8357142857142857\n",
      "epoch: 1449 training Loss: 0.5096344510356459 validation Loss: 0.5105454279902991  valid acc: 0.84  train Acc: 0.8371428571428572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1450 training Loss: 0.5089413655540854 validation Loss: 0.5098176552428608  valid acc: 0.84  train Acc: 0.8371428571428572\n",
      "epoch: 1451 training Loss: 0.5082396984230921 validation Loss: 0.509093596211833  valid acc: 0.84  train Acc: 0.8371428571428572\n",
      "epoch: 1452 training Loss: 0.5075397504191146 validation Loss: 0.5083730837408847  valid acc: 0.84  train Acc: 0.8371428571428572\n",
      "epoch: 1453 training Loss: 0.5068415017494616 validation Loss: 0.5076559649903599  valid acc: 0.84  train Acc: 0.8371428571428572\n",
      "epoch: 1454 training Loss: 0.5061449345247667 validation Loss: 0.5069421000894522  valid acc: 0.84  train Acc: 0.8371428571428572\n",
      "epoch: 1455 training Loss: 0.5054500324615158 validation Loss: 0.5062313609247695  valid acc: 0.84  train Acc: 0.8371428571428572\n",
      "epoch: 1456 training Loss: 0.5047567806322715 validation Loss: 0.5055236300501139  valid acc: 0.84  train Acc: 0.8385714285714285\n",
      "epoch: 1457 training Loss: 0.5040651652559623 validation Loss: 0.5048187997041746  valid acc: 0.84  train Acc: 0.8385714285714285\n",
      "epoch: 1458 training Loss: 0.5033751735218147 validation Loss: 0.5041167709244617  valid acc: 0.84  train Acc: 0.8385714285714285\n",
      "epoch: 1459 training Loss: 0.5026867934415377 validation Loss: 0.5034174527472038  valid acc: 0.84  train Acc: 0.8385714285714285\n",
      "epoch: 1460 training Loss: 0.5020000137252305 validation Loss: 0.5027207614841559  valid acc: 0.84  train Acc: 0.8385714285714285\n",
      "epoch: 1461 training Loss: 0.5013148236772061 validation Loss: 0.5020266200683194  valid acc: 0.84  train Acc: 0.84\n",
      "epoch: 1462 training Loss: 0.5006312131085348 validation Loss: 0.5013359416306008  valid acc: 0.84  train Acc: 0.84\n",
      "epoch: 1463 training Loss: 0.49994917226362134 validation Loss: 0.5006500497276604  valid acc: 0.84  train Acc: 0.8414285714285714\n",
      "epoch: 1464 training Loss: 0.4992686917585645 validation Loss: 0.4999664857927299  valid acc: 0.84  train Acc: 0.8428571428571429\n",
      "epoch: 1465 training Loss: 0.49858976252940074 validation Loss: 0.4992851943235672  valid acc: 0.84  train Acc: 0.8428571428571429\n",
      "epoch: 1466 training Loss: 0.4979123757886462 validation Loss: 0.4986061239425279  valid acc: 0.84  train Acc: 0.8428571428571429\n",
      "epoch: 1467 training Loss: 0.4972365229888013 validation Loss: 0.4979292270352187  valid acc: 0.84  train Acc: 0.8428571428571429\n",
      "epoch: 1468 training Loss: 0.4965621957916965 validation Loss: 0.497254459422028  valid acc: 0.84  train Acc: 0.8428571428571429\n",
      "epoch: 1469 training Loss: 0.4958893860427413 validation Loss: 0.4965817800593973  valid acc: 0.84  train Acc: 0.8428571428571429\n",
      "epoch: 1470 training Loss: 0.49521808574928683 validation Loss: 0.4959111507680116  valid acc: 0.84  train Acc: 0.8428571428571429\n",
      "epoch: 1471 training Loss: 0.4945482870624379 validation Loss: 0.49524253598538953  valid acc: 0.84  train Acc: 0.8428571428571429\n",
      "epoch: 1472 training Loss: 0.49388009603263544 validation Loss: 0.49457448892846895  valid acc: 0.84  train Acc: 0.8428571428571429\n",
      "epoch: 1473 training Loss: 0.4932110914196079 validation Loss: 0.4938859361929694  valid acc: 0.84  train Acc: 0.8428571428571429\n",
      "epoch: 1474 training Loss: 0.4925312488737403 validation Loss: 0.49320076006612124  valid acc: 0.84  train Acc: 0.8428571428571429\n",
      "epoch: 1475 training Loss: 0.49185312770885414 validation Loss: 0.49251881519222007  valid acc: 0.84  train Acc: 0.8428571428571429\n",
      "epoch: 1476 training Loss: 0.4911767094521155 validation Loss: 0.4918399687962199  valid acc: 0.84  train Acc: 0.8428571428571429\n",
      "epoch: 1477 training Loss: 0.4905019772238024 validation Loss: 0.4911640994753548  valid acc: 0.84  train Acc: 0.8428571428571429\n",
      "epoch: 1478 training Loss: 0.48982891548652335 validation Loss: 0.4904910961151798  valid acc: 0.84  train Acc: 0.8428571428571429\n",
      "epoch: 1479 training Loss: 0.4891576028095762 validation Loss: 0.48982158071333043  valid acc: 0.84  train Acc: 0.8428571428571429\n",
      "epoch: 1480 training Loss: 0.4884883476844264 validation Loss: 0.48914388067939835  valid acc: 0.84  train Acc: 0.8428571428571429\n",
      "epoch: 1481 training Loss: 0.48782071571836355 validation Loss: 0.488470602164709  valid acc: 0.84  train Acc: 0.8428571428571429\n",
      "epoch: 1482 training Loss: 0.4871549245679464 validation Loss: 0.48780116969404247  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1483 training Loss: 0.48649097541878633 validation Loss: 0.48713386676631104  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1484 training Loss: 0.4858225530350111 validation Loss: 0.4864433532437607  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1485 training Loss: 0.48514529705118314 validation Loss: 0.48575612194670176  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1486 training Loss: 0.48446990341243357 validation Loss: 0.48507446210650074  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1487 training Loss: 0.4837963511183844 validation Loss: 0.48439714629879405  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1488 training Loss: 0.48312462067295475 validation Loss: 0.48372279008411645  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1489 training Loss: 0.48245507472101873 validation Loss: 0.4830534424031749  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1490 training Loss: 0.4817889950009903 validation Loss: 0.48238777767501745  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1491 training Loss: 0.4811253422202079 validation Loss: 0.4817239261277729  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1492 training Loss: 0.48046397860721873 validation Loss: 0.4810547109670412  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1493 training Loss: 0.4798059333828071 validation Loss: 0.4803882318380706  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1494 training Loss: 0.47914957093099864 validation Loss: 0.479724401014747  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1495 training Loss: 0.4784935757216348 validation Loss: 0.4790581033103431  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1496 training Loss: 0.4778353060923359 validation Loss: 0.47839477012655846  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1497 training Loss: 0.4771787271420921 validation Loss: 0.47773429373424353  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1498 training Loss: 0.47652388432382425 validation Loss: 0.4770856938314045  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1499 training Loss: 0.47587239842075274 validation Loss: 0.47644091756128193  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1500 training Loss: 0.4752231206144821 validation Loss: 0.4757980662641516  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1501 training Loss: 0.4745754865842142 validation Loss: 0.4751571217523349  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1502 training Loss: 0.47392948663385187 validation Loss: 0.4745180664807266  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1503 training Loss: 0.47328511115614985 validation Loss: 0.47384971170022233  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1504 training Loss: 0.4726423506312766 validation Loss: 0.4731797238827384  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1505 training Loss: 0.47200119562549936 validation Loss: 0.4725118403001166  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1506 training Loss: 0.47136163678997167 validation Loss: 0.47184604361102306  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1507 training Loss: 0.4707236648596129 validation Loss: 0.471182316957805  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1508 training Loss: 0.47008764549654886 validation Loss: 0.47052266296804096  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1509 training Loss: 0.4694536218049117 validation Loss: 0.4698574572686156  valid acc: 0.84  train Acc: 0.8442857142857143\n",
      "epoch: 1510 training Loss: 0.4688150007771451 validation Loss: 0.4692043292581089  valid acc: 0.84  train Acc: 0.8457142857142858\n",
      "epoch: 1511 training Loss: 0.46817921787711597 validation Loss: 0.4685531413508548  valid acc: 0.84  train Acc: 0.8457142857142858\n",
      "epoch: 1512 training Loss: 0.46754506986935634 validation Loss: 0.46790388449871984  valid acc: 0.84  train Acc: 0.8457142857142858\n",
      "epoch: 1513 training Loss: 0.4669125469866419 validation Loss: 0.46725654949476997  valid acc: 0.84  train Acc: 0.8457142857142858\n",
      "epoch: 1514 training Loss: 0.4662816395548972 validation Loss: 0.46661112699591795  valid acc: 0.84  train Acc: 0.8457142857142858\n",
      "epoch: 1515 training Loss: 0.46565233799141276 validation Loss: 0.4659676075434469  valid acc: 0.84  train Acc: 0.8457142857142858\n",
      "epoch: 1516 training Loss: 0.4650246328031733 validation Loss: 0.4653259815815992  valid acc: 0.84  train Acc: 0.8457142857142858\n",
      "epoch: 1517 training Loss: 0.46439851458528875 validation Loss: 0.46468623947441057  valid acc: 0.84  train Acc: 0.8457142857142858\n",
      "epoch: 1518 training Loss: 0.4637693399826625 validation Loss: 0.4640219551947369  valid acc: 0.84  train Acc: 0.8457142857142858\n",
      "epoch: 1519 training Loss: 0.46313166063130895 validation Loss: 0.4633702746078242  valid acc: 0.84  train Acc: 0.8457142857142858\n",
      "epoch: 1520 training Loss: 0.46249310841180596 validation Loss: 0.46271453523129275  valid acc: 0.84  train Acc: 0.8457142857142858\n",
      "epoch: 1521 training Loss: 0.4618515626874483 validation Loss: 0.46206268228771935  valid acc: 0.84  train Acc: 0.8457142857142858\n",
      "epoch: 1522 training Loss: 0.46120645842393443 validation Loss: 0.46138789600348656  valid acc: 0.84  train Acc: 0.8457142857142858\n",
      "epoch: 1523 training Loss: 0.46054943839580614 validation Loss: 0.4607174722191838  valid acc: 0.8466666666666667  train Acc: 0.8457142857142858\n",
      "epoch: 1524 training Loss: 0.4598946149166626 validation Loss: 0.46005120709459607  valid acc: 0.8466666666666667  train Acc: 0.8457142857142858\n",
      "epoch: 1525 training Loss: 0.4592419569051179 validation Loss: 0.45938891520691677  valid acc: 0.8466666666666667  train Acc: 0.8457142857142858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1526 training Loss: 0.45859143598971885 validation Loss: 0.458730427664252  valid acc: 0.8466666666666667  train Acc: 0.8457142857142858\n",
      "epoch: 1527 training Loss: 0.4579430260785244 validation Loss: 0.4580799553072696  valid acc: 0.8466666666666667  train Acc: 0.8457142857142858\n",
      "epoch: 1528 training Loss: 0.45729670300245745 validation Loss: 0.45743671844941125  valid acc: 0.8466666666666667  train Acc: 0.8457142857142858\n",
      "epoch: 1529 training Loss: 0.4566526246522544 validation Loss: 0.45679924625211094  valid acc: 0.8466666666666667  train Acc: 0.8457142857142858\n",
      "epoch: 1530 training Loss: 0.45601192961172776 validation Loss: 0.45616486517539284  valid acc: 0.8466666666666667  train Acc: 0.8457142857142858\n",
      "epoch: 1531 training Loss: 0.45537322635002553 validation Loss: 0.45553348106800245  valid acc: 0.8466666666666667  train Acc: 0.8457142857142858\n",
      "epoch: 1532 training Loss: 0.45473649762347934 validation Loss: 0.45490500723462823  valid acc: 0.8466666666666667  train Acc: 0.8457142857142858\n",
      "epoch: 1533 training Loss: 0.45410194068169135 validation Loss: 0.4542823888919469  valid acc: 0.8466666666666667  train Acc: 0.8457142857142858\n",
      "epoch: 1534 training Loss: 0.45347023380545004 validation Loss: 0.4536637715493699  valid acc: 0.8466666666666667  train Acc: 0.8457142857142858\n",
      "epoch: 1535 training Loss: 0.4528404391779796 validation Loss: 0.4530476136925921  valid acc: 0.8466666666666667  train Acc: 0.8471428571428572\n",
      "epoch: 1536 training Loss: 0.4522125420925678 validation Loss: 0.4524344265525474  valid acc: 0.8466666666666667  train Acc: 0.8471428571428572\n",
      "epoch: 1537 training Loss: 0.45158652814481653 validation Loss: 0.45182667665763887  valid acc: 0.8466666666666667  train Acc: 0.8471428571428572\n",
      "epoch: 1538 training Loss: 0.45096257604950923 validation Loss: 0.45121342714092655  valid acc: 0.8466666666666667  train Acc: 0.8471428571428572\n",
      "epoch: 1539 training Loss: 0.4503424152192852 validation Loss: 0.45060620571928744  valid acc: 0.8466666666666667  train Acc: 0.8471428571428572\n",
      "epoch: 1540 training Loss: 0.44972408349953064 validation Loss: 0.4500018549896066  valid acc: 0.8466666666666667  train Acc: 0.8471428571428572\n",
      "epoch: 1541 training Loss: 0.44910756543412533 validation Loss: 0.44940028929063697  valid acc: 0.8466666666666667  train Acc: 0.8471428571428572\n",
      "epoch: 1542 training Loss: 0.4484928560098109 validation Loss: 0.44880305124327097  valid acc: 0.8466666666666667  train Acc: 0.8471428571428572\n",
      "epoch: 1543 training Loss: 0.44788092034625226 validation Loss: 0.44820821458577986  valid acc: 0.8466666666666667  train Acc: 0.8485714285714285\n",
      "epoch: 1544 training Loss: 0.44727074064323996 validation Loss: 0.44761573434653257  valid acc: 0.8466666666666667  train Acc: 0.85\n",
      "epoch: 1545 training Loss: 0.4466623038738232 validation Loss: 0.4470255683665807  valid acc: 0.8466666666666667  train Acc: 0.85\n",
      "epoch: 1546 training Loss: 0.4460555972583268 validation Loss: 0.44643767704745185  valid acc: 0.8466666666666667  train Acc: 0.85\n",
      "epoch: 1547 training Loss: 0.44545060824818833 validation Loss: 0.4458520231230847  valid acc: 0.8466666666666667  train Acc: 0.85\n",
      "epoch: 1548 training Loss: 0.4448473245118711 validation Loss: 0.4452685714535329  valid acc: 0.8466666666666667  train Acc: 0.85\n",
      "epoch: 1549 training Loss: 0.44424634772793264 validation Loss: 0.44467611813188096  valid acc: 0.8466666666666667  train Acc: 0.8514285714285714\n",
      "epoch: 1550 training Loss: 0.44364883019385243 validation Loss: 0.4440864625294864  valid acc: 0.8466666666666667  train Acc: 0.8514285714285714\n",
      "epoch: 1551 training Loss: 0.4430529842850509 validation Loss: 0.4434995138486469  valid acc: 0.8466666666666667  train Acc: 0.8514285714285714\n",
      "epoch: 1552 training Loss: 0.44245879611179656 validation Loss: 0.44291518869428825  valid acc: 0.8466666666666667  train Acc: 0.8514285714285714\n",
      "epoch: 1553 training Loss: 0.4418662523209395 validation Loss: 0.4423334103523458  valid acc: 0.8466666666666667  train Acc: 0.8514285714285714\n",
      "epoch: 1554 training Loss: 0.44127534002332247 validation Loss: 0.4417541081425577  valid acc: 0.8466666666666667  train Acc: 0.8514285714285714\n",
      "epoch: 1555 training Loss: 0.44068608886323735 validation Loss: 0.44117828936970016  valid acc: 0.8466666666666667  train Acc: 0.8514285714285714\n",
      "epoch: 1556 training Loss: 0.44009890216702013 validation Loss: 0.4406046994106756  valid acc: 0.8466666666666667  train Acc: 0.8514285714285714\n",
      "epoch: 1557 training Loss: 0.4395137948799278 validation Loss: 0.44003632846931034  valid acc: 0.8466666666666667  train Acc: 0.8514285714285714\n",
      "epoch: 1558 training Loss: 0.43893174376847355 validation Loss: 0.4394726267353574  valid acc: 0.8466666666666667  train Acc: 0.8514285714285714\n",
      "epoch: 1559 training Loss: 0.43835240039243517 validation Loss: 0.4389106343881565  valid acc: 0.8466666666666667  train Acc: 0.8514285714285714\n",
      "epoch: 1560 training Loss: 0.4377745749352414 validation Loss: 0.43835035069037037  valid acc: 0.8466666666666667  train Acc: 0.8514285714285714\n",
      "epoch: 1561 training Loss: 0.43719825712438526 validation Loss: 0.43779177367245303  valid acc: 0.8466666666666667  train Acc: 0.8514285714285714\n",
      "epoch: 1562 training Loss: 0.43662343683725097 validation Loss: 0.43723490026656975  valid acc: 0.8466666666666667  train Acc: 0.8514285714285714\n",
      "epoch: 1563 training Loss: 0.4360501040953025 validation Loss: 0.43667972642779274  valid acc: 0.8466666666666667  train Acc: 0.8514285714285714\n",
      "epoch: 1564 training Loss: 0.43547824905894766 validation Loss: 0.4361262472437204  valid acc: 0.8466666666666667  train Acc: 0.8514285714285714\n",
      "epoch: 1565 training Loss: 0.43490786202296605 validation Loss: 0.43557445703357794  valid acc: 0.8466666666666667  train Acc: 0.8514285714285714\n",
      "epoch: 1566 training Loss: 0.4343389334124092 validation Loss: 0.4350243494377592  valid acc: 0.8466666666666667  train Acc: 0.8514285714285714\n",
      "epoch: 1567 training Loss: 0.433771453778897 validation Loss: 0.43447591749868936  valid acc: 0.8466666666666667  train Acc: 0.8514285714285714\n",
      "epoch: 1568 training Loss: 0.43320541379724864 validation Loss: 0.43392915373381524  valid acc: 0.8466666666666667  train Acc: 0.8514285714285714\n",
      "epoch: 1569 training Loss: 0.43264080426239643 validation Loss: 0.4333840502014516  valid acc: 0.8466666666666667  train Acc: 0.8528571428571429\n",
      "epoch: 1570 training Loss: 0.43207761608653966 validation Loss: 0.43284059856015006  valid acc: 0.8466666666666667  train Acc: 0.8528571428571429\n",
      "epoch: 1571 training Loss: 0.4315158402965051 validation Loss: 0.43229879012220246  valid acc: 0.8466666666666667  train Acc: 0.8528571428571429\n",
      "epoch: 1572 training Loss: 0.43095107369224545 validation Loss: 0.43173861758405224  valid acc: 0.8466666666666667  train Acc: 0.8528571428571429\n",
      "epoch: 1573 training Loss: 0.4303797871625344 validation Loss: 0.43118132331848424  valid acc: 0.8466666666666667  train Acc: 0.8528571428571429\n",
      "epoch: 1574 training Loss: 0.4298101122838553 validation Loss: 0.43062679405124676  valid acc: 0.8466666666666667  train Acc: 0.8528571428571429\n",
      "epoch: 1575 training Loss: 0.42924203269108213 validation Loss: 0.4300749263932275  valid acc: 0.8466666666666667  train Acc: 0.8528571428571429\n",
      "epoch: 1576 training Loss: 0.4286755329425468 validation Loss: 0.4295256258404831  valid acc: 0.8466666666666667  train Acc: 0.8528571428571429\n",
      "epoch: 1577 training Loss: 0.42811059838896653 validation Loss: 0.4289788058818516  valid acc: 0.8466666666666667  train Acc: 0.8528571428571429\n",
      "epoch: 1578 training Loss: 0.4275472150651335 validation Loss: 0.42843438720170857  valid acc: 0.8466666666666667  train Acc: 0.8528571428571429\n",
      "epoch: 1579 training Loss: 0.42698536960023725 validation Loss: 0.4278922969670168  valid acc: 0.8466666666666667  train Acc: 0.8528571428571429\n",
      "epoch: 1580 training Loss: 0.42642504914344864 validation Loss: 0.4273524681891683  valid acc: 0.8466666666666667  train Acc: 0.8528571428571429\n",
      "epoch: 1581 training Loss: 0.4258662413020111 validation Loss: 0.4268148391522912  valid acc: 0.8466666666666667  train Acc: 0.8528571428571429\n",
      "epoch: 1582 training Loss: 0.4253089340895925 validation Loss: 0.4262793529007049  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1583 training Loss: 0.42475311588305975 validation Loss: 0.42574595677907306  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1584 training Loss: 0.42419877538617656 validation Loss: 0.42521503840146163  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1585 training Loss: 0.42364590159899923 validation Loss: 0.42468659198765046  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1586 training Loss: 0.4230944837919664 validation Loss: 0.4241601149161311  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1587 training Loss: 0.4225445114838642 validation Loss: 0.42363556908136374  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1588 training Loss: 0.42199631256817416 validation Loss: 0.42311457116889706  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1589 training Loss: 0.42145108674465437 validation Loss: 0.42260610451753905  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1590 training Loss: 0.42090873262694556 validation Loss: 0.42209874707580447  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1591 training Loss: 0.42036781111503585 validation Loss: 0.421593076211809  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1592 training Loss: 0.4198286882845963 validation Loss: 0.4211001847645182  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1593 training Loss: 0.4192920648815006 validation Loss: 0.4206093896573312  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1594 training Loss: 0.41875691596513237 validation Loss: 0.4201196863103495  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1595 training Loss: 0.41822322905480025 validation Loss: 0.419630664654696  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1596 training Loss: 0.41769099242747976 validation Loss: 0.41914238736181453  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1597 training Loss: 0.4171601949884893 validation Loss: 0.4186549102222084  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1598 training Loss: 0.4166308261664858 validation Loss: 0.41816828279433477  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1599 training Loss: 0.416103108514303 validation Loss: 0.4176719553007096  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1600 training Loss: 0.4155785194719992 validation Loss: 0.4171770404354384  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1601 training Loss: 0.41505528879140197 validation Loss: 0.416683528855491  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1602 training Loss: 0.41453340819195983 validation Loss: 0.4161914112558122  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1603 training Loss: 0.4140133095193778 validation Loss: 0.41570244869573697  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1604 training Loss: 0.4134950084950966 validation Loss: 0.41521471063695053  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1605 training Loss: 0.41297802330338756 validation Loss: 0.4147282020067471  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1606 training Loss: 0.41246234590512465 validation Loss: 0.4142429264013071  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1607 training Loss: 0.41194796836973396 validation Loss: 0.4137588862269848  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1608 training Loss: 0.4114354775730906 validation Loss: 0.4132643011672455  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1609 training Loss: 0.4109252466832708 validation Loss: 0.4127542544778766  valid acc: 0.8533333333333334  train Acc: 0.8528571428571429\n",
      "epoch: 1610 training Loss: 0.41040493477572537 validation Loss: 0.4122472550998176  valid acc: 0.86  train Acc: 0.8528571428571429\n",
      "epoch: 1611 training Loss: 0.40988610855166957 validation Loss: 0.4117452209972576  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1612 training Loss: 0.4093687498965827 validation Loss: 0.4112458430787134  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1613 training Loss: 0.40885318434981743 validation Loss: 0.41076019377148343  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1614 training Loss: 0.4083413017347542 validation Loss: 0.41027634857566136  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1615 training Loss: 0.40783085332195435 validation Loss: 0.40979426773427224  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1616 training Loss: 0.4073218291409827 validation Loss: 0.40931391440312587  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1617 training Loss: 0.40681421939721313 validation Loss: 0.4088352543653816  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1618 training Loss: 0.40630801445794745 validation Loss: 0.4083582557757617  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1619 training Loss: 0.405803676477893 validation Loss: 0.40788526585263224  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1620 training Loss: 0.4053017075599473 validation Loss: 0.407413688612318  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1621 training Loss: 0.40480109541262216 validation Loss: 0.4069435156722238  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1622 training Loss: 0.404301831338743 validation Loss: 0.4064747384731344  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1623 training Loss: 0.4038039067360442 validation Loss: 0.4060073483102243  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1624 training Loss: 0.4033073130956434 validation Loss: 0.4055413363607545  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1625 training Loss: 0.4028120420005854 validation Loss: 0.40507669370879335  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1626 training Loss: 0.40231808512445405 validation Loss: 0.40461341136725887  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1627 training Loss: 0.40182543423003947 validation Loss: 0.40415148029755493  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1628 training Loss: 0.40133408116805785 validation Loss: 0.40369089142704423  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1629 training Loss: 0.40084401787591634 validation Loss: 0.4032316356645791  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1630 training Loss: 0.40035523637652065 validation Loss: 0.4027739113979868  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1631 training Loss: 0.3998677287771209 validation Loss: 0.4023182316614261  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1632 training Loss: 0.3993814872681916 validation Loss: 0.401863847391771  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1633 training Loss: 0.3988965041223447 validation Loss: 0.40140739273822307  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1634 training Loss: 0.39841277169327227 validation Loss: 0.40093139855725357  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1635 training Loss: 0.39793028241471695 validation Loss: 0.40045688454638273  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1636 training Loss: 0.39744902879946853 validation Loss: 0.3999838404144096  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1637 training Loss: 0.3969690034383843 validation Loss: 0.3995122559412019  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1638 training Loss: 0.39649019899943355 validation Loss: 0.3990421209824419  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1639 training Loss: 0.39601260822676265 validation Loss: 0.3985734254737038  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1640 training Loss: 0.39553622393978083 validation Loss: 0.3981061594339327  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1641 training Loss: 0.3950610390322665 validation Loss: 0.3976403129683911  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1642 training Loss: 0.39458704647149 validation Loss: 0.3971758762711298  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1643 training Loss: 0.3941142392973565 validation Loss: 0.39671283962703596  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1644 training Loss: 0.3936426106215635 validation Loss: 0.3962511934135035  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1645 training Loss: 0.39317215362677627 validation Loss: 0.395790928101769  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1646 training Loss: 0.39270375469153035 validation Loss: 0.3953435902254706  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1647 training Loss: 0.3922378082137558 validation Loss: 0.39489694376551854  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1648 training Loss: 0.39177307129772665 validation Loss: 0.3944510408988288  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1649 training Loss: 0.391309535265905 validation Loss: 0.3940059278817176  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1650 training Loss: 0.3908471918061358 validation Loss: 0.39356164563791485  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1651 training Loss: 0.3903860329131601 validation Loss: 0.39311823029177606  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1652 training Loss: 0.3899260508414698 validation Loss: 0.39267571365114873  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1653 training Loss: 0.3894672380672816 validation Loss: 0.39223412364413063  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1654 training Loss: 0.3890095872578445 validation Loss: 0.3917934847137091  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1655 training Loss: 0.3885530912466427 validation Loss: 0.39135381817400716  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1656 training Loss: 0.3880982661812204 validation Loss: 0.39091800414155226  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1657 training Loss: 0.3876450977059043 validation Loss: 0.39048299967592254  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1658 training Loss: 0.38719305295849443 validation Loss: 0.39004883595074574  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1659 training Loss: 0.3867421249190895 validation Loss: 0.3896125057089494  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1660 training Loss: 0.38629230674914455 validation Loss: 0.38917277699010555  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1661 training Loss: 0.38584359176856897 validation Loss: 0.38873399824421173  valid acc: 0.8666666666666667  train Acc: 0.8542857142857143\n",
      "epoch: 1662 training Loss: 0.38539597343713866 validation Loss: 0.3882961885500493  valid acc: 0.8666666666666667  train Acc: 0.8571428571428571\n",
      "epoch: 1663 training Loss: 0.3849494453393803 validation Loss: 0.38785936446033614  valid acc: 0.8666666666666667  train Acc: 0.8571428571428571\n",
      "epoch: 1664 training Loss: 0.3845040011722506 validation Loss: 0.3874235402634352  valid acc: 0.8666666666666667  train Acc: 0.8571428571428571\n",
      "epoch: 1665 training Loss: 0.38405963473506455 validation Loss: 0.3869887282187607  valid acc: 0.8666666666666667  train Acc: 0.8571428571428571\n",
      "epoch: 1666 training Loss: 0.3836163399212353 validation Loss: 0.38655493876842645  valid acc: 0.8666666666666667  train Acc: 0.8571428571428571\n",
      "epoch: 1667 training Loss: 0.3831741107114759 validation Loss: 0.3861221807274533  valid acc: 0.8666666666666667  train Acc: 0.8585714285714285\n",
      "epoch: 1668 training Loss: 0.38273050271564735 validation Loss: 0.3856665277543573  valid acc: 0.8666666666666667  train Acc: 0.8585714285714285\n",
      "epoch: 1669 training Loss: 0.38228043900928455 validation Loss: 0.38521340976121393  valid acc: 0.8666666666666667  train Acc: 0.8585714285714285\n",
      "epoch: 1670 training Loss: 0.3818320934517192 validation Loss: 0.3847519692558566  valid acc: 0.8666666666666667  train Acc: 0.8585714285714285\n",
      "epoch: 1671 training Loss: 0.3813850149158824 validation Loss: 0.38429604502462217  valid acc: 0.8666666666666667  train Acc: 0.8585714285714285\n",
      "epoch: 1672 training Loss: 0.3809402768953956 validation Loss: 0.3838428148858471  valid acc: 0.8666666666666667  train Acc: 0.8585714285714285\n",
      "epoch: 1673 training Loss: 0.3804971201337033 validation Loss: 0.38339412793574  valid acc: 0.8666666666666667  train Acc: 0.8585714285714285\n",
      "epoch: 1674 training Loss: 0.3800555782998039 validation Loss: 0.38294770602952705  valid acc: 0.8666666666666667  train Acc: 0.8585714285714285\n",
      "epoch: 1675 training Loss: 0.37961530264271637 validation Loss: 0.3825053984960593  valid acc: 0.8666666666666667  train Acc: 0.8585714285714285\n",
      "epoch: 1676 training Loss: 0.37917691008778076 validation Loss: 0.3820662338928618  valid acc: 0.8733333333333333  train Acc: 0.8585714285714285\n",
      "epoch: 1677 training Loss: 0.37873625468511285 validation Loss: 0.3816056527568685  valid acc: 0.8733333333333333  train Acc: 0.8585714285714285\n",
      "epoch: 1678 training Loss: 0.37828916153013203 validation Loss: 0.3811501344826622  valid acc: 0.8733333333333333  train Acc: 0.8585714285714285\n",
      "epoch: 1679 training Loss: 0.3778440999278255 validation Loss: 0.3806992253084394  valid acc: 0.8733333333333333  train Acc: 0.8585714285714285\n",
      "epoch: 1680 training Loss: 0.3774012481844219 validation Loss: 0.38025315401582316  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1681 training Loss: 0.3769606655434529 validation Loss: 0.3798093239795629  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1682 training Loss: 0.37652129385342537 validation Loss: 0.3793676505741695  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1683 training Loss: 0.376083120885123 validation Loss: 0.37892695992977027  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1684 training Loss: 0.3756461350303179 validation Loss: 0.37848838362438  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1685 training Loss: 0.3752104756397016 validation Loss: 0.37805570004187206  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1686 training Loss: 0.3747768115647884 validation Loss: 0.3776247248349253  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1687 training Loss: 0.3743452557998151 validation Loss: 0.37720708509869827  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1688 training Loss: 0.37391585930002047 validation Loss: 0.3767903434618469  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1689 training Loss: 0.3734876072895286 validation Loss: 0.3763745369440207  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1690 training Loss: 0.3730604911460328 validation Loss: 0.37595969759843845  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1691 training Loss: 0.37263450259162223 validation Loss: 0.37554585303932203  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1692 training Loss: 0.37220963364461146 validation Loss: 0.3751330269172656  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1693 training Loss: 0.3717858765802512 validation Loss: 0.3747212393471557  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1694 training Loss: 0.37136322389859155 validation Loss: 0.3743105072929572  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1695 training Loss: 0.3709416682981103 validation Loss: 0.37390084491336606  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1696 training Loss: 0.3705212026539965 validation Loss: 0.37349226387202983  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1697 training Loss: 0.3701018200001937 validation Loss: 0.37308477361573306  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1698 training Loss: 0.3696835135144887 validation Loss: 0.3726783816236678  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1699 training Loss: 0.36926627650606747 validation Loss: 0.37227309363063354  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1700 training Loss: 0.36885010240507826 validation Loss: 0.37186891382676  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1701 training Loss: 0.36843498475382536 validation Loss: 0.3714658450361095  valid acc: 0.8733333333333333  train Acc: 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1702 training Loss: 0.36802091719929797 validation Loss: 0.3710638888762979  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1703 training Loss: 0.3676078934867889 validation Loss: 0.37066304590106675  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1704 training Loss: 0.3671959074544106 validation Loss: 0.37026331572755855  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1705 training Loss: 0.36678495302834807 validation Loss: 0.36986469714987286  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1706 training Loss: 0.36637502421872414 validation Loss: 0.3694671882403275  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1707 training Loss: 0.36596611511597116 validation Loss: 0.369070786439707  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1708 training Loss: 0.36555829907052617 validation Loss: 0.3686764996021831  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1709 training Loss: 0.365151556915797 validation Loss: 0.36828314023989084  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1710 training Loss: 0.36474581646932286 validation Loss: 0.3678907238395946  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1711 training Loss: 0.3643410717922775 validation Loss: 0.3674992634153757  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1712 training Loss: 0.3639373170730911 validation Loss: 0.36710876978297513  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1713 training Loss: 0.363534706810886 validation Loss: 0.3667209454235129  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1714 training Loss: 0.36313392965355795 validation Loss: 0.366335832915519  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1715 training Loss: 0.36273455685819445 validation Loss: 0.36595142376899553  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1716 training Loss: 0.36233613357789846 validation Loss: 0.3655677492663464  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1717 training Loss: 0.36193865437822315 validation Loss: 0.3651848367232872  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1718 training Loss: 0.3615421139877714 validation Loss: 0.3648027099123033  valid acc: 0.8733333333333333  train Acc: 0.86\n",
      "epoch: 1719 training Loss: 0.3611465072740682 validation Loss: 0.36442138944294344  valid acc: 0.8733333333333333  train Acc: 0.8614285714285714\n",
      "epoch: 1720 training Loss: 0.3607518292242464 validation Loss: 0.3640408931030635  valid acc: 0.8733333333333333  train Acc: 0.8614285714285714\n",
      "epoch: 1721 training Loss: 0.3603580749295696 validation Loss: 0.3636612361647993  valid acc: 0.8733333333333333  train Acc: 0.8614285714285714\n",
      "epoch: 1722 training Loss: 0.3599652395730115 validation Loss: 0.36328243165872953  valid acc: 0.8733333333333333  train Acc: 0.8614285714285714\n",
      "epoch: 1723 training Loss: 0.35957331841927076 validation Loss: 0.36290449061938573  valid acc: 0.8733333333333333  train Acc: 0.8614285714285714\n",
      "epoch: 1724 training Loss: 0.35918230680672636 validation Loss: 0.36252742230498125  valid acc: 0.8733333333333333  train Acc: 0.8614285714285714\n",
      "epoch: 1725 training Loss: 0.35879282608294893 validation Loss: 0.36216356949028644  valid acc: 0.8733333333333333  train Acc: 0.8614285714285714\n",
      "epoch: 1726 training Loss: 0.35840599928298567 validation Loss: 0.36179985612506194  valid acc: 0.8733333333333333  train Acc: 0.8614285714285714\n",
      "epoch: 1727 training Loss: 0.3580201267816071 validation Loss: 0.36143635780267935  valid acc: 0.8733333333333333  train Acc: 0.8614285714285714\n",
      "epoch: 1728 training Loss: 0.3576352825157737 validation Loss: 0.3610742383480628  valid acc: 0.8733333333333333  train Acc: 0.8614285714285714\n",
      "epoch: 1729 training Loss: 0.3572516889358165 validation Loss: 0.36071233161799854  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1730 training Loss: 0.35686902881759264 validation Loss: 0.36035070437012623  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1731 training Loss: 0.3564872951384796 validation Loss: 0.35998941601340084  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1732 training Loss: 0.35610648132363903 validation Loss: 0.35962851936316326  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1733 training Loss: 0.35572658116596967 validation Loss: 0.35926806132300354  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1734 training Loss: 0.3553475887622205 validation Loss: 0.35890808349957753  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1735 training Loss: 0.35496969285294716 validation Loss: 0.35856132294671555  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1736 training Loss: 0.3545945228299375 validation Loss: 0.3582143710604861  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1737 training Loss: 0.35422033045626455 validation Loss: 0.35786732589310527  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1738 training Loss: 0.35384710517449297 validation Loss: 0.35752027516494944  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1739 training Loss: 0.35347483733887675 validation Loss: 0.3571732972988634  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1740 training Loss: 0.3531035859078966 validation Loss: 0.3568279875793005  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1741 training Loss: 0.35273373253610274 validation Loss: 0.35648269579378405  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1742 training Loss: 0.3523650084812148 validation Loss: 0.3561386893757797  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1743 training Loss: 0.35199740300764576 validation Loss: 0.35579470171311556  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1744 training Loss: 0.35163071970166876 validation Loss: 0.35545080794261313  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1745 training Loss: 0.3512649502766894 validation Loss: 0.35510707516179213  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1746 training Loss: 0.3509000870135106 validation Loss: 0.35476356325262204  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1747 training Loss: 0.35053612266424217 validation Loss: 0.3544203256253874  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1748 training Loss: 0.3501730503751817 validation Loss: 0.35407740988934355  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1749 training Loss: 0.349810863624803 validation Loss: 0.35373485845652636  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1750 training Loss: 0.34944955617378404 validation Loss: 0.3533927090847257  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1751 training Loss: 0.3490891220246343 validation Loss: 0.35305099536524  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1752 training Loss: 0.34872955538898304 validation Loss: 0.3527097471606286  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1753 training Loss: 0.3483710065872265 validation Loss: 0.3523818648068606  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1754 training Loss: 0.34801438453675976 validation Loss: 0.35205373422870984  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1755 training Loss: 0.34765870195505966 validation Loss: 0.35172544957082613  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1756 training Loss: 0.347303948334656 validation Loss: 0.35139709508851735  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1757 training Loss: 0.34695011401051323 validation Loss: 0.3510687461497182  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1758 training Loss: 0.34659719001449796 validation Loss: 0.3507404701425143  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1759 training Loss: 0.3462451679587729 validation Loss: 0.3504123272954244  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1760 training Loss: 0.3458940399421976 validation Loss: 0.3500843714175039  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1761 training Loss: 0.34554379847503625 validation Loss: 0.34975579423890335  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1762 training Loss: 0.3451944364182401 validation Loss: 0.3494253291180336  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1763 training Loss: 0.344845946934343 validation Loss: 0.3490951921722493  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1764 training Loss: 0.3444983234476156 validation Loss: 0.3487654190689262  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1765 training Loss: 0.3441515596116101 validation Loss: 0.34843604159263314  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1766 training Loss: 0.34380564928261104 validation Loss: 0.3481070880555116  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1767 training Loss: 0.34346058649781097 validation Loss: 0.34777858366469333  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1768 training Loss: 0.34311636545727425 validation Loss: 0.3474505508510725  valid acc: 0.8733333333333333  train Acc: 0.8628571428571429\n",
      "epoch: 1769 training Loss: 0.34277298050894167 validation Loss: 0.3471230095633615  valid acc: 0.8733333333333333  train Acc: 0.8642857142857143\n",
      "epoch: 1770 training Loss: 0.3424304261360818 validation Loss: 0.3467959775309876  valid acc: 0.8733333333333333  train Acc: 0.8642857142857143\n",
      "epoch: 1771 training Loss: 0.3420886969467173 validation Loss: 0.34646947049905286  valid acc: 0.8733333333333333  train Acc: 0.8642857142857143\n",
      "epoch: 1772 training Loss: 0.3417477876646484 validation Loss: 0.34614350243826547  valid acc: 0.8733333333333333  train Acc: 0.8642857142857143\n",
      "epoch: 1773 training Loss: 0.34140769312177227 validation Loss: 0.3458180857324624  valid acc: 0.8733333333333333  train Acc: 0.8642857142857143\n",
      "epoch: 1774 training Loss: 0.3410684082514589 validation Loss: 0.3454932313460851  valid acc: 0.8733333333333333  train Acc: 0.8642857142857143\n",
      "epoch: 1775 training Loss: 0.34072992808279173 validation Loss: 0.34516894897372585  valid acc: 0.8733333333333333  train Acc: 0.8642857142857143\n",
      "epoch: 1776 training Loss: 0.34039224773551746 validation Loss: 0.3448452471736514  valid acc: 0.8733333333333333  train Acc: 0.8642857142857143\n",
      "epoch: 1777 training Loss: 0.3400553624155837 validation Loss: 0.34452213348701016  valid acc: 0.8733333333333333  train Acc: 0.8642857142857143\n",
      "epoch: 1778 training Loss: 0.33971926741116304 validation Loss: 0.3441996145442536  valid acc: 0.8733333333333333  train Acc: 0.8642857142857143\n",
      "epoch: 1779 training Loss: 0.3393839580890861 validation Loss: 0.3438776961601421  valid acc: 0.8733333333333333  train Acc: 0.8642857142857143\n",
      "epoch: 1780 training Loss: 0.33904942989161646 validation Loss: 0.34355638341856204  valid acc: 0.8733333333333333  train Acc: 0.8642857142857143\n",
      "epoch: 1781 training Loss: 0.3387156783335164 validation Loss: 0.34323568074825034  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1782 training Loss: 0.3383826989993609 validation Loss: 0.34291559199040705  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1783 training Loss: 0.3380504875410646 validation Loss: 0.3425961204590717  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1784 training Loss: 0.3377191209854376 validation Loss: 0.3422784145702528  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1785 training Loss: 0.3373887931984679 validation Loss: 0.3419611919327251  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1786 training Loss: 0.33706016433177005 validation Loss: 0.3416584017756118  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1787 training Loss: 0.3367332871748422 validation Loss: 0.34135528599981974  valid acc: 0.88  train Acc: 0.8642857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1788 training Loss: 0.3364072379607204 validation Loss: 0.3410519381257717  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1789 training Loss: 0.33608200765882273 validation Loss: 0.34074844181478936  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1790 training Loss: 0.3357575879569339 validation Loss: 0.3404448718838423  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1791 training Loss: 0.33543397113773143 validation Loss: 0.3401412952217017  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1792 training Loss: 0.33511114997990377 validation Loss: 0.3398377716146433  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1793 training Loss: 0.33478911767880254 validation Loss: 0.3395343544895161  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1794 training Loss: 0.3344678677826225 validation Loss: 0.3392310915815798  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1795 training Loss: 0.33414739414093025 validation Loss: 0.3389280255340502  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1796 training Loss: 0.3338276908630227 validation Loss: 0.3386251944358106  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1797 training Loss: 0.3335087522841168 validation Loss: 0.33832263230325915  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1798 training Loss: 0.3331905729377846 validation Loss: 0.33802036951177683  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1799 training Loss: 0.3328731475333757 validation Loss: 0.33771843318183914  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1800 training Loss: 0.3325564709374303 validation Loss: 0.3374168475243491  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1801 training Loss: 0.3322405381582867 validation Loss: 0.33711563414935397  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1802 training Loss: 0.33192534433325616 validation Loss: 0.3368148123419173  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1803 training Loss: 0.3316108847178622 validation Loss: 0.33651439930855565  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1804 training Loss: 0.3312971546767466 validation Loss: 0.33621441039732086  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1805 training Loss: 0.330984149675924 validation Loss: 0.3359148592942973  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1806 training Loss: 0.33067186527613296 validation Loss: 0.33561575819901246  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1807 training Loss: 0.33036029712708076 validation Loss: 0.33531711798099795  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1808 training Loss: 0.33004944096241956 validation Loss: 0.33501894831951384  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1809 training Loss: 0.3297392925953276 validation Loss: 0.33472125782823886  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1810 training Loss: 0.3294298479145877 validation Loss: 0.33442405416653903  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1811 training Loss: 0.3291211028810836 validation Loss: 0.33412734413876133  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1812 training Loss: 0.32881306901507107 validation Loss: 0.3338182420541463  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1813 training Loss: 0.3285056988973469 validation Loss: 0.3335234723807899  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1814 training Loss: 0.3281990046432498 validation Loss: 0.33322911725660304  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1815 training Loss: 0.32789299544813655 validation Loss: 0.33293519116177367  valid acc: 0.88  train Acc: 0.8642857142857143\n",
      "epoch: 1816 training Loss: 0.3275876675122298 validation Loss: 0.33263425405348224  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1817 training Loss: 0.3272830171052417 validation Loss: 0.33232992682938023  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1818 training Loss: 0.32697904056197596 validation Loss: 0.33202613038974393  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1819 training Loss: 0.32667573427856617 validation Loss: 0.3317228745359864  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1820 training Loss: 0.3263730947092323 validation Loss: 0.33142016795438384  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1821 training Loss: 0.32607111836345737 validation Loss: 0.3311180183324594  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1822 training Loss: 0.3257698018035083 validation Loss: 0.33081643246285725  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1823 training Loss: 0.32546914164224 validation Loss: 0.3305154163360562  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1824 training Loss: 0.3251691345411338 validation Loss: 0.33021497522312887  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1825 training Loss: 0.32486977720852944 validation Loss: 0.3299151137496253  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1826 training Loss: 0.3245710663980209 validation Loss: 0.3296158359615396  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1827 training Loss: 0.32427299890698974 validation Loss: 0.3293171453842212  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1828 training Loss: 0.3239755715752536 validation Loss: 0.3290190450749946  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1829 training Loss: 0.3236787812838173 validation Loss: 0.32872153767017087  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1830 training Loss: 0.3233831745007402 validation Loss: 0.32844099009391536  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1831 training Loss: 0.32309012304837526 validation Loss: 0.328159943121078  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1832 training Loss: 0.3227977689100236 validation Loss: 0.32787850134225316  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1833 training Loss: 0.32250610425932313 validation Loss: 0.3275967584629736  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1834 training Loss: 0.3222159416145213 validation Loss: 0.3273297034625097  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1835 training Loss: 0.3219268756929767 validation Loss: 0.3270615558462851  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1836 training Loss: 0.32163859070927037 validation Loss: 0.3267924707123637  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1837 training Loss: 0.32135131622694185 validation Loss: 0.32650924357200384  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1838 training Loss: 0.32106482463801844 validation Loss: 0.3262263143539196  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1839 training Loss: 0.3207790730624605 validation Loss: 0.32595887501572274  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1840 training Loss: 0.32049437591391 validation Loss: 0.3256772449104222  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1841 training Loss: 0.3202102326908898 validation Loss: 0.3253959302879764  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1842 training Loss: 0.3199268133868174 validation Loss: 0.3251283728380418  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1843 training Loss: 0.3196441354195597 validation Loss: 0.32484671188845693  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1844 training Loss: 0.31936194523518446 validation Loss: 0.32456550264384454  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1845 training Loss: 0.3190806153478375 validation Loss: 0.3242982261506155  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1846 training Loss: 0.3187997550665803 validation Loss: 0.3240168994076267  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1847 training Loss: 0.3185196755984458 validation Loss: 0.323749646972032  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1848 training Loss: 0.31824013517583793 validation Loss: 0.32346839340001  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1849 training Loss: 0.3179613534112339 validation Loss: 0.32320133410678104  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1850 training Loss: 0.3176830452574581 validation Loss: 0.3229203082769893  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1851 training Loss: 0.31740560636115117 validation Loss: 0.3226535791888757  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1852 training Loss: 0.3171285749581559 validation Loss: 0.32238652757783615  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1853 training Loss: 0.3168522823687333 validation Loss: 0.3221055851313205  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1854 training Loss: 0.3165766529793793 validation Loss: 0.3218391261727591  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1855 training Loss: 0.31630153641633835 validation Loss: 0.32157245370857596  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1856 training Loss: 0.31602706505275396 validation Loss: 0.321291933929451  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1857 training Loss: 0.3157533529687101 validation Loss: 0.32102247652336513  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1858 training Loss: 0.3154801084737566 validation Loss: 0.3207487113758951  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1859 training Loss: 0.315207492237069 validation Loss: 0.32047416643182247  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1860 training Loss: 0.3149354990031079 validation Loss: 0.32019959018979127  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1861 training Loss: 0.31466417666580315 validation Loss: 0.3199272404967536  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1862 training Loss: 0.3143937824348937 validation Loss: 0.31964114275801897  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1863 training Loss: 0.3141239753027816 validation Loss: 0.31937030423069745  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1864 training Loss: 0.31385472934591535 validation Loss: 0.3190993640986372  valid acc: 0.88  train Acc: 0.8657142857142858\n",
      "epoch: 1865 training Loss: 0.313586083231324 validation Loss: 0.3188283789479538  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1866 training Loss: 0.3133180323703482 validation Loss: 0.3185573994286282  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1867 training Loss: 0.31305057239899053 validation Loss: 0.3182864708844356  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1868 training Loss: 0.3127836991462897 validation Loss: 0.3180156339166618  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1869 training Loss: 0.31251740860869326 validation Loss: 0.3177449248882581  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1870 training Loss: 0.3122516969292053 validation Loss: 0.31747437637449066  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1871 training Loss: 0.31198656038034195 validation Loss: 0.3172040175655843  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1872 training Loss: 0.31172199535012735 validation Loss: 0.3169338746263369  valid acc: 0.88  train Acc: 0.8671428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1873 training Loss: 0.3114579983305208 validation Loss: 0.316663971017203  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1874 training Loss: 0.31119456590779215 validation Loss: 0.3163943277808988  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1875 training Loss: 0.3109317784183145 validation Loss: 0.31612668273200045  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1876 training Loss: 0.31066971505264285 validation Loss: 0.3158643162350286  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1877 training Loss: 0.310408349677324 validation Loss: 0.3156054823420319  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1878 training Loss: 0.31014782642078664 validation Loss: 0.3153467558920867  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1879 training Loss: 0.3098878500299824 validation Loss: 0.3150888645507449  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1880 training Loss: 0.3096284170158437 validation Loss: 0.3148310997803777  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1881 training Loss: 0.3093695240472873 validation Loss: 0.31457349788024636  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1882 training Loss: 0.30911116792868654 validation Loss: 0.31431609127142646  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1883 training Loss: 0.30885334558161676 validation Loss: 0.31405890890987676  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1884 training Loss: 0.30859605403000523 validation Loss: 0.31380197665551995  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1885 training Loss: 0.30833929038800084 validation Loss: 0.31354531760189625  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1886 training Loss: 0.30808305185001417 validation Loss: 0.3132889523705136  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1887 training Loss: 0.3078273356824985 validation Loss: 0.3130328993736066  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1888 training Loss: 0.30757215564339935 validation Loss: 0.31277856470777654  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1889 training Loss: 0.3073176661547552 validation Loss: 0.3125244602504584  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1890 training Loss: 0.30706369074555034 validation Loss: 0.31227061063756884  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1891 training Loss: 0.3068102267884224 validation Loss: 0.3120170378587093  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1892 training Loss: 0.30655805340181475 validation Loss: 0.31177716525292365  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1893 training Loss: 0.30630794799206884 validation Loss: 0.3115390658670653  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1894 training Loss: 0.3060585657229609 validation Loss: 0.3113002097927612  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1895 training Loss: 0.3058097549571744 validation Loss: 0.3110607156876743  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1896 training Loss: 0.3055615078269945 validation Loss: 0.31082068986986466  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1897 training Loss: 0.30531381728318197 validation Loss: 0.3105802275901541  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1898 training Loss: 0.3050666769490032 validation Loss: 0.31033941418008565  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1899 training Loss: 0.3048200810034369 validation Loss: 0.3100983260858706  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1900 training Loss: 0.30457402408754297 validation Loss: 0.3098570317982753  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1901 training Loss: 0.30432850122923316 validation Loss: 0.3096155926878617  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1902 training Loss: 0.3040835077826674 validation Loss: 0.30937406375439414  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1903 training Loss: 0.3038390393792901 validation Loss: 0.30913249429860123  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1904 training Loss: 0.3035950918881343 validation Loss: 0.3088909285238492  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1905 training Loss: 0.3033516613835182 validation Loss: 0.3086494060746676  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1906 training Loss: 0.30310874411864136 validation Loss: 0.30840796251846764  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1907 training Loss: 0.30286633650390243 validation Loss: 0.3081666297762314  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1908 training Loss: 0.30262454380817005 validation Loss: 0.30792715377903884  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1909 training Loss: 0.30238337005040095 validation Loss: 0.3076877027959683  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1910 training Loss: 0.3021426960605022 validation Loss: 0.30744831426767616  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1911 training Loss: 0.3019025184904668 validation Loss: 0.3072090217745206  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1912 training Loss: 0.30166283412805556 validation Loss: 0.30696985544440364  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1913 training Loss: 0.30142363988105353 validation Loss: 0.30673084231721237  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1914 training Loss: 0.30118493276422187 validation Loss: 0.306492006670371  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1915 training Loss: 0.30094670988841804 validation Loss: 0.30625337030958005  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1916 training Loss: 0.30070896845146455 validation Loss: 0.30601495282841285  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1917 training Loss: 0.30047170573043397 validation Loss: 0.3057767718400684  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1918 training Loss: 0.30023491907508254 validation Loss: 0.3055388431842473  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1919 training Loss: 0.29999860590222094 validation Loss: 0.30530118111180715  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1920 training Loss: 0.2997627636908513 validation Loss: 0.3050637984495835  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1921 training Loss: 0.2995273899779359 validation Loss: 0.3048267067475067  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1922 training Loss: 0.2992924823546859 validation Loss: 0.3045899164099261  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1923 training Loss: 0.299058038463285 validation Loss: 0.30435343681284516  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1924 training Loss: 0.2988240559939744 validation Loss: 0.3041172764085934  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1925 training Loss: 0.2985905326824436 validation Loss: 0.3038814428192969  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1926 training Loss: 0.2983574663074801 validation Loss: 0.30364594292035996  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1927 training Loss: 0.2981248546888386 validation Loss: 0.30341078291504525  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1928 training Loss: 0.2978926956853009 validation Loss: 0.30317596840111577  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1929 training Loss: 0.29766098719289913 validation Loss: 0.30294150443040413  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1930 training Loss: 0.2974297271432823 validation Loss: 0.30270739556207504  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1931 training Loss: 0.29719980638559246 validation Loss: 0.30248892831556845  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1932 training Loss: 0.29697155948349974 validation Loss: 0.3022696798258092  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1933 training Loss: 0.29674382179634895 validation Loss: 0.30204976526577243  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1934 training Loss: 0.2965165863006579 validation Loss: 0.301829287841401  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1935 training Loss: 0.2962898467163935 validation Loss: 0.3016083400264085  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1936 training Loss: 0.29606359737320315 validation Loss: 0.3013870046759836  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1937 training Loss: 0.29583783310346484 validation Loss: 0.30116535602963923  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1938 training Loss: 0.29561254915663204 validation Loss: 0.300943460612977  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1939 training Loss: 0.29538774113049254 validation Loss: 0.3007213780475737  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1940 training Loss: 0.29516340491587056 validation Loss: 0.3004991617775941  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1941 training Loss: 0.29493953665202394 validation Loss: 0.30027685972110496  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1942 training Loss: 0.29471613269055247 validation Loss: 0.3000545148534417  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1943 training Loss: 0.2944931895660919 validation Loss: 0.2998321657293701  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1944 training Loss: 0.2942707039724186 validation Loss: 0.2996098469501965  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1945 training Loss: 0.29404867274287977 validation Loss: 0.2993875895814311  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1946 training Loss: 0.2938270928342827 validation Loss: 0.2991654215260856  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1947 training Loss: 0.2936059613135589 validation Loss: 0.2989433678582048  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1948 training Loss: 0.29338527534665626 validation Loss: 0.2987214511207851  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1949 training Loss: 0.29316509743736485 validation Loss: 0.29850107236924434  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1950 training Loss: 0.2929454682103107 validation Loss: 0.2982807568809881  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1951 training Loss: 0.2927262760859666 validation Loss: 0.2980605324506383  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1952 training Loss: 0.2925075184255816 validation Loss: 0.2978404240405562  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1953 training Loss: 0.29228919267835446 validation Loss: 0.2976204540791513  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1954 training Loss: 0.29207129637261675 validation Loss: 0.29740064272731453  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1955 training Loss: 0.29185423614147327 validation Loss: 0.2971944955443427  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1956 training Loss: 0.2916375029136129 validation Loss: 0.2969874934544528  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1957 training Loss: 0.2914212638974882 validation Loss: 0.29677975996026057  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1958 training Loss: 0.29120551182863447 validation Loss: 0.2965578314070013  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1959 training Loss: 0.2909903109451412 validation Loss: 0.2963500206832413  valid acc: 0.88  train Acc: 0.8671428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1960 training Loss: 0.2907754681846775 validation Loss: 0.29614166541070336  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1961 training Loss: 0.2905610973078167 validation Loss: 0.2959328565066079  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1962 training Loss: 0.2903471927901137 validation Loss: 0.29572367527036375  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1963 training Loss: 0.2901337495981647 validation Loss: 0.2955141943921905  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1964 training Loss: 0.2899207631049711 validation Loss: 0.29530447885997946  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1965 training Loss: 0.2897082290221481 validation Loss: 0.2950945867736584  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1966 training Loss: 0.28949614334551493 validation Loss: 0.2948845700757185  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1967 training Loss: 0.2892845023113207 validation Loss: 0.2946744752059263  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1968 training Loss: 0.28907330236092915 validation Loss: 0.2944643436876211  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1969 training Loss: 0.2888625401122341 validation Loss: 0.2942542126523771  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1970 training Loss: 0.2886523127066127 validation Loss: 0.294047821617033  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1971 training Loss: 0.2884427856729859 validation Loss: 0.2938412184455575  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1972 training Loss: 0.28823368545839906 validation Loss: 0.2936344557753036  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1973 training Loss: 0.28802500869101677 validation Loss: 0.29342758076526965  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1974 training Loss: 0.2878167521908342 validation Loss: 0.29322063567543816  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1975 training Loss: 0.2876089129420948 validation Loss: 0.29301365838541005  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1976 training Loss: 0.2874014880709061 validation Loss: 0.2928066828584269  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1977 training Loss: 0.2871944748270011 validation Loss: 0.2925997395563307  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1978 training Loss: 0.2869878705688114 validation Loss: 0.2923928558104913  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1979 training Loss: 0.2867816727511883 validation Loss: 0.29218605615326015  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1980 training Loss: 0.2865758789152457 validation Loss: 0.29197936261405955  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1981 training Loss: 0.2863704866799067 validation Loss: 0.2917727949838192  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1982 training Loss: 0.2861654937348199 validation Loss: 0.29156637105109584  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1983 training Loss: 0.2859608978343811 validation Loss: 0.29136010681287583  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1984 training Loss: 0.28575669679264964 validation Loss: 0.2911540166627546  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1985 training Loss: 0.28555288847898996 validation Loss: 0.29094811355890604  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1986 training Loss: 0.28534947081430595 validation Loss: 0.29074240917400657  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1987 training Loss: 0.28514644176775816 validation Loss: 0.2905369140290507  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1988 training Loss: 0.2849437993538807 validation Loss: 0.2903316376127899  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1989 training Loss: 0.28474154163002735 validation Loss: 0.2901265884883462  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1990 training Loss: 0.2845396666940913 validation Loss: 0.28992177438838207  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1991 training Loss: 0.28433817268245554 validation Loss: 0.289717202300065  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1992 training Loss: 0.2841370577681361 validation Loss: 0.28951287854093105  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1993 training Loss: 0.28393632015909026 validation Loss: 0.28930880882663124  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1994 training Loss: 0.28373595809666546 validation Loss: 0.2891049983314417  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1995 training Loss: 0.2835359698541703 validation Loss: 0.28890145174232074  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1996 training Loss: 0.2833363537355501 validation Loss: 0.28869817330721226  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1997 training Loss: 0.2831371080741564 validation Loss: 0.2884951668782208  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1998 training Loss: 0.2829382312315976 validation Loss: 0.2882924359502122  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 1999 training Loss: 0.2827397215966628 validation Loss: 0.2880899836953366  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2000 training Loss: 0.28254157758431053 validation Loss: 0.28788781299391575  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2001 training Loss: 0.28234379763471734 validation Loss: 0.28768592646208685  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2002 training Loss: 0.2821463802123789 validation Loss: 0.2874843264765568  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2003 training Loss: 0.2819493238052607 validation Loss: 0.28728301519677557  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2004 training Loss: 0.2817526269239936 validation Loss: 0.28708199458481043  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2005 training Loss: 0.2815562881011104 validation Loss: 0.28688126642316747  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2006 training Loss: 0.28136030589032124 validation Loss: 0.28668083233078306  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2007 training Loss: 0.2811646788658244 validation Loss: 0.2864806937773801  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2008 training Loss: 0.2809694056216506 validation Loss: 0.286280852096367  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2009 training Loss: 0.2807744847710383 validation Loss: 0.28608130849643487  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2010 training Loss: 0.2805799149458376 validation Loss: 0.28588206407199085  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2011 training Loss: 0.2803856947959429 validation Loss: 0.2856831198125557  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2012 training Loss: 0.28019182298875006 validation Loss: 0.2854844766112309  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2013 training Loss: 0.2799982982086382 validation Loss: 0.2852861352723395  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2014 training Loss: 0.27980511915647543 validation Loss: 0.2850880965183244  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2015 training Loss: 0.2796122845491445 validation Loss: 0.284890360995985  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2016 training Loss: 0.2794197931190907 validation Loss: 0.2846929292821203  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2017 training Loss: 0.27922764361388835 validation Loss: 0.2844958018886424  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2018 training Loss: 0.2790358347958258 validation Loss: 0.2842989792672146  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2019 training Loss: 0.27884436544150815 validation Loss: 0.28410246181346444  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2020 training Loss: 0.2786532343414763 validation Loss: 0.2839062498708156  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2021 training Loss: 0.2784624402998424 validation Loss: 0.2837103437339776  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2022 training Loss: 0.27827198213393917 validation Loss: 0.28351474365212914  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2023 training Loss: 0.27808185867398455 validation Loss: 0.2833265107346545  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2024 training Loss: 0.2778920687627594 validation Loss: 0.2831387392888083  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2025 training Loss: 0.27770261125529805 validation Loss: 0.2829512991291602  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2026 training Loss: 0.2775134850185912 validation Loss: 0.28276418985624974  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2027 training Loss: 0.27732468893130074 validation Loss: 0.28257741104638534  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2028 training Loss: 0.2771362300941942 validation Loss: 0.28239186935875876  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2029 training Loss: 0.27694811985591916 validation Loss: 0.282206550437825  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2030 training Loss: 0.2767603363448513 validation Loss: 0.2820214660640695  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2031 training Loss: 0.27657287843989653 validation Loss: 0.28183662664105397  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2032 training Loss: 0.27638574503920554 validation Loss: 0.2816520413445584  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2033 training Loss: 0.2761989350582086 validation Loss: 0.28146771825568206  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2034 training Loss: 0.27601244742800957 validation Loss: 0.28128366447961145  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2035 training Loss: 0.2758262810940678 validation Loss: 0.2810998862515822  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2036 training Loss: 0.2756404350151101 validation Loss: 0.28091638903140753  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2037 training Loss: 0.27545490816222623 validation Loss: 0.2807331775877958  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2038 training Loss: 0.275269699518114 validation Loss: 0.2805502560735551  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2039 training Loss: 0.2750848080764419 validation Loss: 0.28036762809266425  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2040 training Loss: 0.2749002328413086 validation Loss: 0.28018529676008436  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2041 training Loss: 0.27471597282677984 validation Loss: 0.280003264755096  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2042 training Loss: 0.2745320270564872 validation Loss: 0.27982153436885904  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2043 training Loss: 0.27434839456327964 validation Loss: 0.2796401075468181  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2044 training Loss: 0.27416507438891513 validation Loss: 0.27945898592651325  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2045 training Loss: 0.27398206558378685 validation Loss: 0.279278170871291  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2046 training Loss: 0.2737993672066781 validation Loss: 0.27909766350035903  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2047 training Loss: 0.27361697832453974 validation Loss: 0.2789174647155831  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2048 training Loss: 0.27343489801228715 validation Loss: 0.27873757522537457  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2049 training Loss: 0.273253125352614 validation Loss: 0.278557995565989  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2050 training Loss: 0.27307165943581896 validation Loss: 0.2783787261205124  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2051 training Loss: 0.27289049935964443 validation Loss: 0.2781997671357876  valid acc: 0.88  train Acc: 0.8671428571428571\n",
      "epoch: 2052 training Loss: 0.2727096442291257 validation Loss: 0.27802111873750446  valid acc: 0.88  train Acc: 0.8685714285714285\n",
      "epoch: 2053 training Loss: 0.2725290931564479 validation Loss: 0.2778427809436519  valid acc: 0.88  train Acc: 0.8685714285714285\n",
      "epoch: 2054 training Loss: 0.2723488452608112 validation Loss: 0.27766475367651033  valid acc: 0.88  train Acc: 0.8685714285714285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2055 training Loss: 0.27216889966830243 validation Loss: 0.27748703677334324  valid acc: 0.88  train Acc: 0.8685714285714285\n",
      "epoch: 2056 training Loss: 0.27198925551177267 validation Loss: 0.2773096299959274  valid acc: 0.88  train Acc: 0.8685714285714285\n",
      "epoch: 2057 training Loss: 0.2718099119307195 validation Loss: 0.27713253303905044  valid acc: 0.88  train Acc: 0.8685714285714285\n",
      "epoch: 2058 training Loss: 0.27163086807117526 validation Loss: 0.27695574553808483  valid acc: 0.88  train Acc: 0.8685714285714285\n",
      "epoch: 2059 training Loss: 0.2714527618272322 validation Loss: 0.27679182161804  valid acc: 0.88  train Acc: 0.8685714285714285\n",
      "epoch: 2060 training Loss: 0.2712769669572296 validation Loss: 0.2766271292896234  valid acc: 0.88  train Acc: 0.8685714285714285\n",
      "epoch: 2061 training Loss: 0.2711015141214794 validation Loss: 0.27646177230565433  valid acc: 0.88  train Acc: 0.8685714285714285\n",
      "epoch: 2062 training Loss: 0.27092639847061595 validation Loss: 0.27629584376766786  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2063 training Loss: 0.2707516157605606 validation Loss: 0.27612942720496475  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2064 training Loss: 0.27057716224030126 validation Loss: 0.27596259755003266  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2065 training Loss: 0.2704032719251524 validation Loss: 0.2758080303210754  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2066 training Loss: 0.2702295677497749 validation Loss: 0.2756394948798596  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2067 training Loss: 0.27005625437230485 validation Loss: 0.2754834998043512  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2068 training Loss: 0.26988334355743393 validation Loss: 0.2753136857006741  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2069 training Loss: 0.26971067533016024 validation Loss: 0.2751566405809296  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2070 training Loss: 0.26953845295104023 validation Loss: 0.27498589124340805  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2071 training Loss: 0.2693664941974936 validation Loss: 0.2748280995219973  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2072 training Loss: 0.26919486715509994 validation Loss: 0.27465669090338274  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2073 training Loss: 0.26902367817177136 validation Loss: 0.2744983961495429  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2074 training Loss: 0.2688526335904554 validation Loss: 0.27433936229215905  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2075 training Loss: 0.26868214368060905 validation Loss: 0.27416685019625636  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2076 training Loss: 0.26851185306834946 validation Loss: 0.2740077048158574  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2077 training Loss: 0.2683418371853632 validation Loss: 0.2738479832629093  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2078 training Loss: 0.2681722893951024 validation Loss: 0.27367487216242653  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2079 training Loss: 0.2680030322086895 validation Loss: 0.2735153204215617  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2080 training Loss: 0.26783401536728946 validation Loss: 0.2733553093495694  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2081 training Loss: 0.2676653398981264 validation Loss: 0.2731949120293679  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2082 training Loss: 0.2674970470977591 validation Loss: 0.2730212204279018  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2083 training Loss: 0.2673291146144885 validation Loss: 0.27286132098439947  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2084 training Loss: 0.26716140402912253 validation Loss: 0.27270109818536725  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2085 training Loss: 0.26699402330704863 validation Loss: 0.27254061097336146  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2086 training Loss: 0.26682696916589166 validation Loss: 0.2723799121035849  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2087 training Loss: 0.26666023857121585 validation Loss: 0.27221904878893793  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2088 training Loss: 0.26649382869811405 validation Loss: 0.2720580632794621  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2089 training Loss: 0.2663277369000765 validation Loss: 0.27189699338244366  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2090 training Loss: 0.2661619606836845 validation Loss: 0.271735872928932  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2091 training Loss: 0.2659964976879693 validation Loss: 0.27157473219194667  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2092 training Loss: 0.2658313456675111 validation Loss: 0.2714129309782771  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2093 training Loss: 0.26566650247853896 validation Loss: 0.2712507988381876  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2094 training Loss: 0.2655019660674444 validation Loss: 0.27108871968193066  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2095 training Loss: 0.2653377344612361 validation Loss: 0.27092671317323264  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2096 training Loss: 0.26517380575956145 validation Loss: 0.2707647968898174  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2097 training Loss: 0.2650101781279943 validation Loss: 0.2706029865442567  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2098 training Loss: 0.26484684979234846 validation Loss: 0.27044129618143364  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2099 training Loss: 0.26468381903382443 validation Loss: 0.27027973835506475  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2100 training Loss: 0.26452108418483644 validation Loss: 0.27011832428547555  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2101 training Loss: 0.2643586436253953 validation Loss: 0.2699570640006007  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2102 training Loss: 0.264196495779948 validation Loss: 0.26979596646198056  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2103 training Loss: 0.26403463911459557 validation Loss: 0.26963503967733904  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2104 training Loss: 0.2638730721346229 validation Loss: 0.26947429080116553  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2105 training Loss: 0.2637118375675132 validation Loss: 0.26932714674945896  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2106 training Loss: 0.26355246214784056 validation Loss: 0.26917908267239427  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2107 training Loss: 0.26339344269846565 validation Loss: 0.2690302106748275  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2108 training Loss: 0.263234772620343 validation Loss: 0.26888063149832203  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2109 training Loss: 0.2630764460350328 validation Loss: 0.2687304356611068  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2110 training Loss: 0.2629184576608184 validation Loss: 0.2685797044897977  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2111 training Loss: 0.26276080271279245 validation Loss: 0.2684285110516747  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2112 training Loss: 0.26260358605658407 validation Loss: 0.26826338243569103  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2113 training Loss: 0.26244659339394666 validation Loss: 0.2681125769497223  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2114 training Loss: 0.26228987741329424 validation Loss: 0.26796136950628785  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2115 training Loss: 0.2621334818417936 validation Loss: 0.26780982009184484  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2116 training Loss: 0.26197740308538325 validation Loss: 0.2676579825988333  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2117 training Loss: 0.261821637803084 validation Loss: 0.2675059054515014  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2118 training Loss: 0.2616661828708309 validation Loss: 0.2673536321685603  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2119 training Loss: 0.26151103535190307 validation Loss: 0.2672012018686918  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2120 training Loss: 0.2613561924726568 validation Loss: 0.2670486497244404  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2121 training Loss: 0.2612016516025259 validation Loss: 0.2668960073695453  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2122 training Loss: 0.26104741023746036 validation Loss: 0.2667433032643266  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2123 training Loss: 0.2608934659861415 validation Loss: 0.2665905630233193  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2124 training Loss: 0.2607398165584417 validation Loss: 0.26643780970895875  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2125 training Loss: 0.260586459755704 validation Loss: 0.2662850640947652  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2126 training Loss: 0.2604333934625007 validation Loss: 0.2661323449011392  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2127 training Loss: 0.26028061563959676 validation Loss: 0.2659796690065819  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2128 training Loss: 0.26012812431789867 validation Loss: 0.26582705163687026  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2129 training Loss: 0.25997591759321265 validation Loss: 0.26567450653447316  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2130 training Loss: 0.2598239936216684 validation Loss: 0.26552204611025576  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2131 training Loss: 0.2596723506156958 validation Loss: 0.2653696815793204  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2132 training Loss: 0.2595209868404597 validation Loss: 0.2652174230826381  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2133 training Loss: 0.25936990061067955 validation Loss: 0.2650652797959588  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2134 training Loss: 0.2592190902877726 validation Loss: 0.26491326002733245  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2135 training Loss: 0.2590685542772708 validation Loss: 0.2647613713044368  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2136 training Loss: 0.2589182910264712 validation Loss: 0.2646096204527846  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2137 training Loss: 0.2587683369148281 validation Loss: 0.26445948298128835  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2138 training Loss: 0.25861871565043015 validation Loss: 0.2643093876909392  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2139 training Loss: 0.25846936210625787 validation Loss: 0.26415934967379967  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2140 training Loss: 0.2583202747990938 validation Loss: 0.26400938255102074  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2141 training Loss: 0.25817145228958915 validation Loss: 0.2638594986224595  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2142 training Loss: 0.2580228931790084 validation Loss: 0.26370970900074914  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2143 training Loss: 0.2578745961063913 validation Loss: 0.2635600237314308  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2144 training Loss: 0.2577265597460596 validation Loss: 0.26341045190060014  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2145 training Loss: 0.2575787899749795 validation Loss: 0.2632627031919246  valid acc: 0.88  train Acc: 0.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2146 training Loss: 0.25743137138214883 validation Loss: 0.26311495962263703  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2147 training Loss: 0.2572842093215884 validation Loss: 0.26296723939872907  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2148 training Loss: 0.2571373024773447 validation Loss: 0.2628195589442017  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2149 training Loss: 0.25699064957576256 validation Loss: 0.2626719330821319  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2150 training Loss: 0.2568440730109478 validation Loss: 0.26251630529432957  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2151 training Loss: 0.25669662943537913 validation Loss: 0.2623614635634678  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2152 training Loss: 0.25654944793392703 validation Loss: 0.2622073456430591  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2153 training Loss: 0.25640252621471027 validation Loss: 0.2620538960022022  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2154 training Loss: 0.25625586219793756 validation Loss: 0.26190106508956873  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2155 training Loss: 0.2561094539793028 validation Loss: 0.2617488086802472  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2156 training Loss: 0.25596329980048355 validation Loss: 0.2615970872956854  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2157 training Loss: 0.2558173980253258 validation Loss: 0.26144586568820527  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2158 training Loss: 0.25567174712058227 validation Loss: 0.26129511238261977  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2159 training Loss: 0.2555263456402987 validation Loss: 0.26114479926840173  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2160 training Loss: 0.2553812600860913 validation Loss: 0.26100919815620777  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2161 training Loss: 0.25523775314272584 validation Loss: 0.26087284621221296  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2162 training Loss: 0.2550945532558688 validation Loss: 0.26073583274018736  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2163 training Loss: 0.2549516553224589 validation Loss: 0.2605982381144718  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2164 training Loss: 0.2548090547251122 validation Loss: 0.2604601346751297  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2165 training Loss: 0.25466674725429245 validation Loss: 0.260321587536706  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2166 training Loss: 0.2545247290451084 validation Loss: 0.26018265531807183  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2167 training Loss: 0.2543829965258615 validation Loss: 0.2600433908003843  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2168 training Loss: 0.2542415463760387 validation Loss: 0.25990384151971313  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2169 training Loss: 0.25410037549190756 validation Loss: 0.25976405030041466  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2170 training Loss: 0.2539594809582325 validation Loss: 0.25962405573485436  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2171 training Loss: 0.25381886002493037 validation Loss: 0.2594838926146285  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2172 training Loss: 0.25367851008771247 validation Loss: 0.25934359231799464  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2173 training Loss: 0.2535384286719538 validation Loss: 0.25920318315781005  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2174 training Loss: 0.25339861341917574 validation Loss: 0.25906269069389076  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2175 training Loss: 0.25325906207565296 validation Loss: 0.25892213801334224  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2176 training Loss: 0.25311892752651616 validation Loss: 0.25877145280258296  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2177 training Loss: 0.25297766708032765 validation Loss: 0.25862164940380644  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2178 training Loss: 0.25283669138069276 validation Loss: 0.25847265148284215  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2179 training Loss: 0.2526959964598124 validation Loss: 0.25832439099128623  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2180 training Loss: 0.2525555787057598 validation Loss: 0.25817680725494724  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2181 training Loss: 0.2524154348058915 validation Loss: 0.25802984616590907  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2182 training Loss: 0.2522755617009397 validation Loss: 0.25788345946579194  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2183 training Loss: 0.2521359565476724 validation Loss: 0.25773760410939084  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2184 training Loss: 0.25199661668842915 validation Loss: 0.2575922416992546  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2185 training Loss: 0.25185753962618224 validation Loss: 0.2574473379829531  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2186 training Loss: 0.2517187230040412 validation Loss: 0.25730286240580047  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2187 training Loss: 0.25158016458833715 validation Loss: 0.25715878771269235  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2188 training Loss: 0.25144186225459336 validation Loss: 0.2570150895934776  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2189 training Loss: 0.2513036812598778 validation Loss: 0.2568594180145494  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2190 training Loss: 0.2511623064150502 validation Loss: 0.25670521655333667  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2191 training Loss: 0.2510212361897153 validation Loss: 0.25655234878876937  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2192 training Loss: 0.2508804635024275 validation Loss: 0.256400693109989  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2193 training Loss: 0.25073998219126575 validation Loss: 0.25625014105264815  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2194 training Loss: 0.2505997868484581 validation Loss: 0.2561005958322165  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2195 training Loss: 0.2504598726872524 validation Loss: 0.2559519710491405  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2196 training Loss: 0.25032023543459325 validation Loss: 0.2558041895442372  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2197 training Loss: 0.2501808712444616 validation Loss: 0.2556571823856851  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2198 training Loss: 0.2500417766277714 validation Loss: 0.2555108879715002  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2199 training Loss: 0.24990294839554167 validation Loss: 0.25536525123353404  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2200 training Loss: 0.24976438361272077 validation Loss: 0.25522022293086  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2201 training Loss: 0.2496260795605662 validation Loss: 0.2550757590219804  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2202 training Loss: 0.24948803370590172 validation Loss: 0.2549318201066318  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2203 training Loss: 0.24935031149410078 validation Loss: 0.25480335862346937  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2204 training Loss: 0.24921369455323852 validation Loss: 0.25467420501793137  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2205 training Loss: 0.24907739925731112 validation Loss: 0.2545444392445136  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2206 training Loss: 0.24894141975947337 validation Loss: 0.2544141333240683  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2207 training Loss: 0.24880575068219732 validation Loss: 0.2542833521388437  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2208 training Loss: 0.24867038705023212 validation Loss: 0.25415215415043246  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2209 training Loss: 0.2485353242356516 validation Loss: 0.2540205920474242  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2210 training Loss: 0.24840055791264742 validation Loss: 0.25388871332911095  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2211 training Loss: 0.24826608402018718 validation Loss: 0.2537565608311416  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2212 training Loss: 0.2481318987310296 validation Loss: 0.25362417319857306  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2213 training Loss: 0.24799799842588535 validation Loss: 0.2534915853113259  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2214 training Loss: 0.24786437967175182 validation Loss: 0.25335882866663706  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2215 training Loss: 0.24773122187265645 validation Loss: 0.25324119568833525  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2216 training Loss: 0.247598207641968 validation Loss: 0.2531070046111141  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2217 training Loss: 0.2474654460088857 validation Loss: 0.2529728401071922  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2218 training Loss: 0.24733308001727117 validation Loss: 0.25285405634500135  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2219 training Loss: 0.24720102153946805 validation Loss: 0.2527187854701417  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2220 training Loss: 0.24706908259765262 validation Loss: 0.25258368433226974  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2221 training Loss: 0.24693766772855222 validation Loss: 0.25246416538224975  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2222 training Loss: 0.2468063362518013 validation Loss: 0.2523281980389486  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2223 training Loss: 0.24667538925703764 validation Loss: 0.25220797646947746  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2224 training Loss: 0.24654469683172464 validation Loss: 0.2520713490664579  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2225 training Loss: 0.24641428912162447 validation Loss: 0.25195060659724733  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2226 training Loss: 0.24628413680709083 validation Loss: 0.25181348667954223  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2227 training Loss: 0.24615433568625966 validation Loss: 0.2516923707845812  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2228 training Loss: 0.24602463169224822 validation Loss: 0.2515548947132131  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2229 training Loss: 0.24589550069884175 validation Loss: 0.25143352522047585  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2230 training Loss: 0.24576635868776775 validation Loss: 0.2513117401785187  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2231 training Loss: 0.24563757530022803 validation Loss: 0.2511738934315924  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2232 training Loss: 0.24550919510025676 validation Loss: 0.25105234705549  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2233 training Loss: 0.24538090211841992 validation Loss: 0.25093019636802455  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2234 training Loss: 0.2452529014330916 validation Loss: 0.2508075132210736  valid acc: 0.88  train Acc: 0.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2235 training Loss: 0.24512529546227807 validation Loss: 0.2506685920845204  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2236 training Loss: 0.2449978985341801 validation Loss: 0.2505462182114843  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2237 training Loss: 0.24487070500911956 validation Loss: 0.250423370571336  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2238 training Loss: 0.2447437897034133 validation Loss: 0.25030010825392274  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2239 training Loss: 0.24461714868712842 validation Loss: 0.2501764844803609  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2240 training Loss: 0.24449077830449945 validation Loss: 0.25005254719192965  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2241 training Loss: 0.24436469429377367 validation Loss: 0.24991243703409008  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2242 training Loss: 0.2442388735110941 validation Loss: 0.24978919306517014  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2243 training Loss: 0.24411325878492623 validation Loss: 0.24966563529605845  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2244 training Loss: 0.24398790449451593 validation Loss: 0.24954180725673286  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2245 training Loss: 0.24386280764399348 validation Loss: 0.24941774817126336  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2246 training Loss: 0.24373796541085466 validation Loss: 0.24929349339165696  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2247 training Loss: 0.24361337512526218 validation Loss: 0.24916907478848838  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2248 training Loss: 0.2434890342528044 validation Loss: 0.24904452110244327  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2249 training Loss: 0.2433649403800639 validation Loss: 0.24891985826055513  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2250 training Loss: 0.2432410912024715 validation Loss: 0.24879581848021706  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2251 training Loss: 0.243117484514025 validation Loss: 0.2486729142527753  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2252 training Loss: 0.24299411819853256 validation Loss: 0.24855003270564385  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2253 training Loss: 0.24287099022210498 validation Loss: 0.2484271893711453  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2254 training Loss: 0.2427480986266756 validation Loss: 0.248304398210901  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2255 training Loss: 0.2426254415243671 validation Loss: 0.24818167177590125  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2256 training Loss: 0.24250301709256067 validation Loss: 0.24805902135021332  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2257 training Loss: 0.2423808235695487 validation Loss: 0.24793645707998327  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2258 training Loss: 0.24225885925067497 validation Loss: 0.24781398808922167  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2259 training Loss: 0.24213712248488484 validation Loss: 0.24769162258371571  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2260 training Loss: 0.24201561167162125 validation Loss: 0.2475693679442788  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2261 training Loss: 0.24189432525801424 validation Loss: 0.24744723081042494  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2262 training Loss: 0.2417732617363227 validation Loss: 0.24732521715544767  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2263 training Loss: 0.24165241964159123 validation Loss: 0.24720333235378433  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2264 training Loss: 0.24153181797198442 validation Loss: 0.2470824607324645  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2265 training Loss: 0.24141158440483795 validation Loss: 0.2469616589191277  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2266 training Loss: 0.24129156650933947 validation Loss: 0.24684093730221568  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2267 training Loss: 0.2411717629793031 validation Loss: 0.24672030523372832  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2268 training Loss: 0.24105217254531672 validation Loss: 0.24659977113346296  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2269 training Loss: 0.24093279397243655 validation Loss: 0.24647934258265092  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2270 training Loss: 0.2408136260581342 validation Loss: 0.2463590264080678  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2271 training Loss: 0.24069466763045805 validation Loss: 0.24623882875758416  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2272 training Loss: 0.24057591754637486 validation Loss: 0.2461187551680241  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2273 training Loss: 0.24045737469026424 validation Loss: 0.24599881062611867  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2274 training Loss: 0.2403393002309259 validation Loss: 0.24589170918936057  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2275 training Loss: 0.24022170095356857 validation Loss: 0.24578363225090755  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2276 training Loss: 0.24010438298491157 validation Loss: 0.24566191663223066  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2277 training Loss: 0.2399873992472861 validation Loss: 0.24555332703631524  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2278 training Loss: 0.23987055851148953 validation Loss: 0.2454439498116759  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2279 training Loss: 0.23975400211490305 validation Loss: 0.24533469933536386  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2280 training Loss: 0.23963773508874428 validation Loss: 0.24522475027422777  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2281 training Loss: 0.23952172203039984 validation Loss: 0.24511418417133587  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2282 training Loss: 0.2394059586232211 validation Loss: 0.24500307462307233  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2283 training Loss: 0.23929044097026608 validation Loss: 0.24489148804867905  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2284 training Loss: 0.23917516552683327 validation Loss: 0.24477948438838884  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2285 training Loss: 0.23906012904541926 validation Loss: 0.2446671177360407  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2286 training Loss: 0.2389453285307259 validation Loss: 0.24455443691174167  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2287 training Loss: 0.23883076120280036 validation Loss: 0.2444414859798019  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2288 training Loss: 0.23871642446676397 validation Loss: 0.24432830471680536  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2289 training Loss: 0.2386023158878787 validation Loss: 0.24421492903433584  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2290 training Loss: 0.23848843317094637 validation Loss: 0.24410139136052167  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2291 training Loss: 0.23837477414322608 validation Loss: 0.2439877209842381  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2292 training Loss: 0.23826134170263993 validation Loss: 0.24387505356492098  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2293 training Loss: 0.23814832917000453 validation Loss: 0.243762217617128  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2294 training Loss: 0.23803553317556317 validation Loss: 0.24364924285660897  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2295 training Loss: 0.23792295182934745 validation Loss: 0.24353615613553037  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2296 training Loss: 0.2378105833315835 validation Loss: 0.24342298172495078  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2297 training Loss: 0.237698425962847 validation Loss: 0.24330974156942575  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2298 training Loss: 0.23758657278053735 validation Loss: 0.24320970786543764  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2299 training Loss: 0.23747505701430546 validation Loss: 0.24309523397824936  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2300 training Loss: 0.23736354780487204 validation Loss: 0.24298085751363704  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2301 training Loss: 0.23725248775277621 validation Loss: 0.24287989539865312  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2302 training Loss: 0.23714151245721155 validation Loss: 0.2427645609882663  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2303 training Loss: 0.23703079569579455 validation Loss: 0.24266280260658377  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2304 training Loss: 0.2369203623543388 validation Loss: 0.24254672864487625  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2305 training Loss: 0.23681005853084888 validation Loss: 0.24244436763835386  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2306 training Loss: 0.23670007750952868 validation Loss: 0.24232773263819202  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2307 training Loss: 0.23659025231760608 validation Loss: 0.24222492710544757  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2308 training Loss: 0.2364806405197266 validation Loss: 0.24210787709823567  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2309 training Loss: 0.23637135600580414 validation Loss: 0.24200475620838607  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2310 training Loss: 0.23626208665629903 validation Loss: 0.2419009145438441  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2311 training Loss: 0.23615331502422987 validation Loss: 0.24178289887366614  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2312 training Loss: 0.23604455929694362 validation Loss: 0.24167900104679008  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2313 training Loss: 0.23593598986515107 validation Loss: 0.24157449149637486  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2314 training Loss: 0.2358278694688026 validation Loss: 0.24145585021628146  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2315 training Loss: 0.23571978621432774 validation Loss: 0.24135147629432765  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2316 training Loss: 0.235611893549991 validation Loss: 0.2412465712577142  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2317 training Loss: 0.23550425374662673 validation Loss: 0.24112755682974987  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2318 training Loss: 0.2353969752232434 validation Loss: 0.24102292969522657  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2319 training Loss: 0.23528973944206857 validation Loss: 0.24091783118412327  valid acc: 0.88  train Acc: 0.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2320 training Loss: 0.2351827324665912 validation Loss: 0.2408123172285462  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2321 training Loss: 0.2350759512480664 validation Loss: 0.24070643841658948  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2322 training Loss: 0.2349693929594601 validation Loss: 0.24060024050932394  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2323 training Loss: 0.23486308477016224 validation Loss: 0.24048001433791152  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2324 training Loss: 0.2347570019833086 validation Loss: 0.24037446721782904  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2325 training Loss: 0.23465106253720616 validation Loss: 0.24026860546542644  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2326 training Loss: 0.234545338458587 validation Loss: 0.24016247028779983  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2327 training Loss: 0.2344398274231548 validation Loss: 0.2400560989700035  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2328 training Loss: 0.23433452724387585 validation Loss: 0.23994952525597876  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2329 training Loss: 0.2342294358537913 validation Loss: 0.23984277969277779  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2330 training Loss: 0.2341245512917036 validation Loss: 0.23973588994148115  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2331 training Loss: 0.23401956923189637 validation Loss: 0.2396135675508902  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2332 training Loss: 0.23391177766320587 validation Loss: 0.2394926677416359  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2333 training Loss: 0.23380427452872765 validation Loss: 0.239373051885831  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2334 training Loss: 0.23369705068094196 validation Loss: 0.23925459617524913  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2335 training Loss: 0.23359009814473441 validation Loss: 0.23913719001504405  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2336 training Loss: 0.23348340992257444 validation Loss: 0.2390207346001791  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2337 training Loss: 0.23337697983490735 validation Loss: 0.23890514165205634  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2338 training Loss: 0.23327080238916909 validation Loss: 0.23879033229590318  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2339 training Loss: 0.2331648726720858 validation Loss: 0.23867623606208743  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2340 training Loss: 0.23305918626093428 validation Loss: 0.23856278999674363  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2341 training Loss: 0.2329537391502567 validation Loss: 0.23844993786898827  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2342 training Loss: 0.2328485276911856 validation Loss: 0.2383376294636194  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2343 training Loss: 0.23274354854107207 validation Loss: 0.23822581994958844  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2344 training Loss: 0.2326387986215413 validation Loss: 0.23811446931572514  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2345 training Loss: 0.2325342750834541 validation Loss: 0.23800354186623504  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2346 training Loss: 0.23242997527753426 validation Loss: 0.23789300576937877  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2347 training Loss: 0.23232589672965534 validation Loss: 0.23778283265352396  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2348 training Loss: 0.23222203711996456 validation Loss: 0.23767299724543492  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2349 training Loss: 0.23211839426517528 validation Loss: 0.23756347704625477  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2350 training Loss: 0.23201496610348143 validation Loss: 0.23745425204115383  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2351 training Loss: 0.2319117506816482 validation Loss: 0.23734530443906782  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2352 training Loss: 0.23180874614391256 validation Loss: 0.23723661843934957  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2353 training Loss: 0.23170595072239553 validation Loss: 0.23712818002250666  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2354 training Loss: 0.23160336272878068 validation Loss: 0.23701997676250736  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2355 training Loss: 0.2315009805470565 validation Loss: 0.23691199765840698  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2356 training Loss: 0.23139880262715637 validation Loss: 0.236804232983295  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2357 training Loss: 0.23129682747936012 validation Loss: 0.23669667414876788  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2358 training Loss: 0.23119505366934154 validation Loss: 0.2365893135833314  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2359 training Loss: 0.23109347981377068 validation Loss: 0.2364821446232987  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2360 training Loss: 0.23099210457638938 validation Loss: 0.23637516141490467  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2361 training Loss: 0.23089092666449715 validation Loss: 0.2362683588264865  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2362 training Loss: 0.23078994482579188 validation Loss: 0.236161732369706  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2363 training Loss: 0.23068915784551897 validation Loss: 0.23605527812888846  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2364 training Loss: 0.23058856454389107 validation Loss: 0.23594899269765335  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2365 training Loss: 0.23048816377374493 validation Loss: 0.2358428731220938  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2366 training Loss: 0.23038795441840787 validation Loss: 0.23573691684984102  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2367 training Loss: 0.2302879353897501 validation Loss: 0.23563112168441402  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2368 training Loss: 0.23018810562640205 validation Loss: 0.2355254857443184  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2369 training Loss: 0.23008846409211978 validation Loss: 0.23542000742641325  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2370 training Loss: 0.2299890097742828 validation Loss: 0.23531468537311165  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2371 training Loss: 0.22988979502100867 validation Loss: 0.23521034612543176  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2372 training Loss: 0.22979083722919183 validation Loss: 0.23510607724378207  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2373 training Loss: 0.22969206179779364 validation Loss: 0.23500188658495452  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2374 training Loss: 0.22959346784565193 validation Loss: 0.2348977812808957  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2375 training Loss: 0.22949505450787 validation Loss: 0.23479376780652436  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2376 training Loss: 0.22939682093473107 validation Loss: 0.23468985204100762  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2377 training Loss: 0.22929876629073648 validation Loss: 0.2345860393231313  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2378 training Loss: 0.22920088975374978 validation Loss: 0.23448233450134148  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2379 training Loss: 0.22910319051423228 validation Loss: 0.23437874197897182  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2380 training Loss: 0.22900566777455605 validation Loss: 0.2342752657551288  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2381 training Loss: 0.22890832425654634 validation Loss: 0.23417282409913961  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2382 training Loss: 0.22881129766358957 validation Loss: 0.23407043508693534  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2383 training Loss: 0.22871444450440012 validation Loss: 0.2339681081757768  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2384 training Loss: 0.22861776398879685 validation Loss: 0.233865851925429  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2385 training Loss: 0.2285212553411128 validation Loss: 0.23376367408343687  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2386 training Loss: 0.2284248787585085 validation Loss: 0.233655788321205  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2387 training Loss: 0.22832735799817247 validation Loss: 0.23354853103051462  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2388 training Loss: 0.22823001549800925 validation Loss: 0.2334418555015472  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2389 training Loss: 0.2281328499471339 validation Loss: 0.2333357197045302  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2390 training Loss: 0.2280358601465712 validation Loss: 0.2332300858107747  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2391 training Loss: 0.2279390449894508 validation Loss: 0.2331249197639372  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2392 training Loss: 0.22784240344491666 validation Loss: 0.2330201908960187  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2393 training Loss: 0.22774593454504966 validation Loss: 0.23291587158325427  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2394 training Loss: 0.22764963737423305 validation Loss: 0.23281193693759794  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2395 training Loss: 0.22755351106050134 validation Loss: 0.23270836453000318  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2396 training Loss: 0.22745755476849638 validation Loss: 0.23260513414212278  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2397 training Loss: 0.22736179494252562 validation Loss: 0.23250295413900776  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2398 training Loss: 0.22726626300163735 validation Loss: 0.2324010075519941  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2399 training Loss: 0.22717089765902243 validation Loss: 0.23229928720186058  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2400 training Loss: 0.22707569822419654 validation Loss: 0.23219778657818088  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2401 training Loss: 0.22698067959280277 validation Loss: 0.2320973554225442  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2402 training Loss: 0.2268859065969399 validation Loss: 0.23199704582961828  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2403 training Loss: 0.22679129665240963 validation Loss: 0.23189686189080172  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2404 training Loss: 0.22669684909309767 validation Loss: 0.23179680723249704  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2405 training Loss: 0.22660256326103453 validation Loss: 0.2316968850628773  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2406 training Loss: 0.22650843850598112 validation Loss: 0.23159709821400232  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2407 training Loss: 0.22641447418506475 validation Loss: 0.231497449179743  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2408 training Loss: 0.2263206696624582 validation Loss: 0.23139794014992807  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2409 training Loss: 0.22622702430909394 validation Loss: 0.23129857304108892  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2410 training Loss: 0.22613353750240825 validation Loss: 0.23119934952413787  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2411 training Loss: 0.22604020862611138 validation Loss: 0.23110027104928557  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2412 training Loss: 0.22594703706997848 validation Loss: 0.23100133886847052  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2413 training Loss: 0.22585383248679242 validation Loss: 0.23089781383177807  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2414 training Loss: 0.22575967358125396 validation Loss: 0.23079489205017278  valid acc: 0.88  train Acc: 0.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2415 training Loss: 0.2256656807734453 validation Loss: 0.23069252969370657  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2416 training Loss: 0.22557185283216574 validation Loss: 0.23059068724529416  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2417 training Loss: 0.22547818862693564 validation Loss: 0.23048932906370387  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2418 training Loss: 0.2253846871109993 validation Loss: 0.2303893324740189  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2419 training Loss: 0.22529134730745792 validation Loss: 0.2302897791258954  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2420 training Loss: 0.22519816829794492 validation Loss: 0.23019061663843918  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2421 training Loss: 0.22510514921336774 validation Loss: 0.23009182148871649  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2422 training Loss: 0.22501228922632946 validation Loss: 0.2299933724268967  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2423 training Loss: 0.22491958754491634 validation Loss: 0.2298952502493881  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2424 training Loss: 0.22482704340759777 validation Loss: 0.22979743759504545  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2425 training Loss: 0.22473466131539566 validation Loss: 0.2297005882977713  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2426 training Loss: 0.22464256520834044 validation Loss: 0.2296039647575422  valid acc: 0.88  train Acc: 0.87\n",
      "epoch: 2427 training Loss: 0.2245506230712826 validation Loss: 0.22950755907642909  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2428 training Loss: 0.22445883427604582 validation Loss: 0.22941136408920376  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2429 training Loss: 0.22436719820446666 validation Loss: 0.22931537329125234  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2430 training Loss: 0.2242757142476689 validation Loss: 0.229219580773667  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2431 training Loss: 0.22418438180544142 validation Loss: 0.22912398116479163  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2432 training Loss: 0.22409320028570254 validation Loss: 0.22902856957757214  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2433 training Loss: 0.2240021691040341 validation Loss: 0.2289333415621275  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2434 training Loss: 0.22391128768327584 validation Loss: 0.22883829306301967  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2435 training Loss: 0.22382055545316787 validation Loss: 0.22874342038074966  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2436 training Loss: 0.22372997185003488 validation Loss: 0.2286487201370593  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2437 training Loss: 0.22363953631650482 validation Loss: 0.22855418924365778  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2438 training Loss: 0.22354924830125697 validation Loss: 0.2284598248740321  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2439 training Loss: 0.22345910725879403 validation Loss: 0.22836562443803402  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2440 training Loss: 0.2233691126492362 validation Loss: 0.22827158555896768  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2441 training Loss: 0.2232787446772665 validation Loss: 0.22817085121759892  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2442 training Loss: 0.22318690725824436 validation Loss: 0.22807095785253656  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2443 training Loss: 0.22309523416228602 validation Loss: 0.22797183638463273  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2444 training Loss: 0.22300372338056187 validation Loss: 0.22787342461813523  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2445 training Loss: 0.2229123731443244 validation Loss: 0.2277756665336966  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2446 training Loss: 0.22282118188253353 validation Loss: 0.2276785116567968  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2447 training Loss: 0.22273014818733908 validation Loss: 0.2275819144930656  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2448 training Loss: 0.2226392707859437 validation Loss: 0.22748583402303127  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2449 training Loss: 0.22254854851764957 validation Loss: 0.22739023324972313  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2450 training Loss: 0.22245798031511727 validation Loss: 0.22729507879333705  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2451 training Loss: 0.22236756518904835 validation Loss: 0.2272003405278511  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2452 training Loss: 0.22227730221565423 validation Loss: 0.22710599125507203  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2453 training Loss: 0.2221871905263891 validation Loss: 0.22701200641211042  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2454 training Loss: 0.2220972292995285 validation Loss: 0.22691836380873345  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2455 training Loss: 0.22200741775324875 validation Loss: 0.22682504339144444  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2456 training Loss: 0.22191775513993123 validation Loss: 0.22673202703148532  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2457 training Loss: 0.2218282407414635 validation Loss: 0.22663929833426796  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2458 training Loss: 0.22173887386535593 validation Loss: 0.22654684246801057  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2459 training Loss: 0.22164965384152197 validation Loss: 0.22645464600959547  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2460 training Loss: 0.22156058001960283 validation Loss: 0.2263626968058792  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2461 training Loss: 0.2214716517667363 validation Loss: 0.22627098384887095  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2462 training Loss: 0.2213828684656901 validation Loss: 0.22617949716336597  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2463 training Loss: 0.22129422951329344 validation Loss: 0.22608822770576695  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2464 training Loss: 0.22120573431911453 validation Loss: 0.2259971672729596  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2465 training Loss: 0.22111738230433967 validation Loss: 0.22590630842022752  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2466 training Loss: 0.22102917290081855 validation Loss: 0.22581564438729443  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2467 training Loss: 0.2209411055502472 validation Loss: 0.2257251690316779  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2468 training Loss: 0.22085317970346466 validation Loss: 0.22563487676862035  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2469 training Loss: 0.22076539481984417 validation Loss: 0.2255447625169403  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2470 training Loss: 0.2206784095200263 validation Loss: 0.2254695033178931  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2471 training Loss: 0.2205914801709419 validation Loss: 0.2253932300199142  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2472 training Loss: 0.22050474415734114 validation Loss: 0.22531604445028952  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2473 training Loss: 0.22041819680688954 validation Loss: 0.22523803898042524  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2474 training Loss: 0.22033183403492324 validation Loss: 0.22515929738703103  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2475 training Loss: 0.22024565224408685 validation Loss: 0.22507989564028938  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2476 training Loss: 0.22015970041573066 validation Loss: 0.2249850654249249  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2477 training Loss: 0.22007385759500048 validation Loss: 0.22490574140190725  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2478 training Loss: 0.21998816529240564 validation Loss: 0.22482583775562476  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2479 training Loss: 0.2199026456503751 validation Loss: 0.22474541587219315  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2480 training Loss: 0.21981729618722634 validation Loss: 0.22466453140874132  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2481 training Loss: 0.21973211465026846 validation Loss: 0.2245832348289384  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2482 training Loss: 0.2196470989800835 validation Loss: 0.22450157188997708  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2483 training Loss: 0.2195622472811491 validation Loss: 0.22441958408504917  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2484 training Loss: 0.21947755779763453 validation Loss: 0.224337309045088  valid acc: 0.88  train Acc: 0.8714285714285714\n",
      "epoch: 2485 training Loss: 0.21939302889342413 validation Loss: 0.22425478090329734  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2486 training Loss: 0.21930865903559152 validation Loss: 0.22417203062572677  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2487 training Loss: 0.21922401110154824 validation Loss: 0.22407573085723806  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2488 training Loss: 0.21913626986576032 validation Loss: 0.2239807003637796  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2489 training Loss: 0.2190487633922176 validation Loss: 0.22388681579957545  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2490 training Loss: 0.21896148404320717 validation Loss: 0.22379396660824416  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2491 training Loss: 0.21887442513621677 validation Loss: 0.22370205368045948  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2492 training Loss: 0.21878758078988939 validation Loss: 0.22361098815931924  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2493 training Loss: 0.21870094579706018 validation Loss: 0.22352069037582495  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2494 training Loss: 0.218614515519939 validation Loss: 0.22343108889919563  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2495 training Loss: 0.2185282858034182 validation Loss: 0.22334211968872716  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2496 training Loss: 0.21844225290322616 validation Loss: 0.22325372533560056  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2497 training Loss: 0.21835641342625386 validation Loss: 0.22316585438449713  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2498 training Loss: 0.21827076428087208 validation Loss: 0.22307846072613094  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2499 training Loss: 0.2181853026354584 validation Loss: 0.22299150305288754  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2500 training Loss: 0.21810002588367877 validation Loss: 0.2229049443706892  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2501 training Loss: 0.21801493161533506 validation Loss: 0.22281875156102005  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2502 training Loss: 0.2179300175918044 validation Loss: 0.2227328949877472  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2503 training Loss: 0.2178452817252757 validation Loss: 0.22264734814398734  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2504 training Loss: 0.21776072206113006 validation Loss: 0.22256208733480634  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2505 training Loss: 0.21767633676293022 validation Loss: 0.22247709139201025  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2506 training Loss: 0.21759212409958098 validation Loss: 0.22239049122272836  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2507 training Loss: 0.21750808243429873 validation Loss: 0.2223022250687031  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2508 training Loss: 0.21742421021509495 validation Loss: 0.22221417962894507  valid acc: 0.8866666666666667  train Acc: 0.8714285714285714\n",
      "epoch: 2509 training Loss: 0.2173405059665282 validation Loss: 0.22212636723629933  valid acc: 0.8933333333333333  train Acc: 0.8714285714285714\n",
      "epoch: 2510 training Loss: 0.21725696828252306 validation Loss: 0.2220389587856704  valid acc: 0.8933333333333333  train Acc: 0.8714285714285714\n",
      "epoch: 2511 training Loss: 0.21717359582009096 validation Loss: 0.22195173202562582  valid acc: 0.8933333333333333  train Acc: 0.8714285714285714\n",
      "epoch: 2512 training Loss: 0.21709038729381291 validation Loss: 0.22186467750415423  valid acc: 0.8933333333333333  train Acc: 0.8714285714285714\n",
      "epoch: 2513 training Loss: 0.21700734147097203 validation Loss: 0.22177778685450286  valid acc: 0.8933333333333333  train Acc: 0.8714285714285714\n",
      "epoch: 2514 training Loss: 0.21692445716723904 validation Loss: 0.22169105268257472  valid acc: 0.8933333333333333  train Acc: 0.8714285714285714\n",
      "epoch: 2515 training Loss: 0.2168417332428332 validation Loss: 0.2216044684655759  valid acc: 0.8933333333333333  train Acc: 0.8714285714285714\n",
      "epoch: 2516 training Loss: 0.21675916859909103 validation Loss: 0.22151802846079524  valid acc: 0.8933333333333333  train Acc: 0.8714285714285714\n",
      "epoch: 2517 training Loss: 0.21667676217538928 validation Loss: 0.22143172762350394  valid acc: 0.8933333333333333  train Acc: 0.8714285714285714\n",
      "epoch: 2518 training Loss: 0.21659451294637386 validation Loss: 0.22134556153307158  valid acc: 0.8933333333333333  train Acc: 0.8714285714285714\n",
      "epoch: 2519 training Loss: 0.21651241991945652 validation Loss: 0.2212595263264823  valid acc: 0.8933333333333333  train Acc: 0.8714285714285714\n",
      "epoch: 2520 training Loss: 0.2164304821325469 validation Loss: 0.22117361863851775  valid acc: 0.8933333333333333  train Acc: 0.8714285714285714\n",
      "epoch: 2521 training Loss: 0.2163486986519892 validation Loss: 0.22108783554794836  valid acc: 0.8933333333333333  train Acc: 0.8714285714285714\n",
      "epoch: 2522 training Loss: 0.2162670685706828 validation Loss: 0.22100217452913679  valid acc: 0.8933333333333333  train Acc: 0.8714285714285714\n",
      "epoch: 2523 training Loss: 0.21618559100636417 validation Loss: 0.220916633408521  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2524 training Loss: 0.21610426510003278 validation Loss: 0.22083121032549313  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2525 training Loss: 0.21602309001450634 validation Loss: 0.22074590369724173  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2526 training Loss: 0.21594206493309226 validation Loss: 0.22066071218716463  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2527 training Loss: 0.21586118905836266 validation Loss: 0.22057563467650104  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2528 training Loss: 0.2157804616110247 validation Loss: 0.22049067023886465  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2529 training Loss: 0.21569988182887626 validation Loss: 0.22040581811739107  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2530 training Loss: 0.2156194489658387 validation Loss: 0.220321077704241  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2531 training Loss: 0.21553916229106196 validation Loss: 0.2202364485222265  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2532 training Loss: 0.2154590210880935 validation Loss: 0.22015193020835047  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2533 training Loss: 0.21537902465410672 validation Loss: 0.2200675224990683  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2534 training Loss: 0.2152991722991843 validation Loss: 0.21998322521710245  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2535 training Loss: 0.21521946334565156 validation Loss: 0.21989903825965493  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2536 training Loss: 0.21513989712745538 validation Loss: 0.2198149615878785  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2537 training Loss: 0.21506047298958647 validation Loss: 0.21973099521748016  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2538 training Loss: 0.21498119028754042 validation Loss: 0.21964713921034654  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2539 training Loss: 0.21490204838681506 validation Loss: 0.21956339366708544  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2540 training Loss: 0.21482304666244156 validation Loss: 0.21947975872039374  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2541 training Loss: 0.2147441844985465 validation Loss: 0.2193963778950808  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2542 training Loss: 0.21466546128794295 validation Loss: 0.21931316831877756  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2543 training Loss: 0.21458687643174812 validation Loss: 0.21923006832320704  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2544 training Loss: 0.2145084293390259 validation Loss: 0.21914707813813913  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2545 training Loss: 0.21443011942645235 validation Loss: 0.2190641980014098  valid acc: 0.8933333333333333  train Acc: 0.8728571428571429\n",
      "epoch: 2546 training Loss: 0.214351946118003 validation Loss: 0.21898142815578225  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2547 training Loss: 0.2142739088446594 validation Loss: 0.2188987688461953  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2548 training Loss: 0.21419600704413508 validation Loss: 0.21881622031736042  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2549 training Loss: 0.21411827439151912 validation Loss: 0.21873456898930377  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2550 training Loss: 0.21404070952084883 validation Loss: 0.2186529713325925  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2551 training Loss: 0.21396327792362327 validation Loss: 0.21857143252146685  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2552 training Loss: 0.21388597904335582 validation Loss: 0.21848995726764245  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2553 training Loss: 0.21380881233150165 validation Loss: 0.21840854986221284  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2554 training Loss: 0.21373177724699782 validation Loss: 0.21832721421363072  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2555 training Loss: 0.21365487325585258 validation Loss: 0.21824595388213888  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2556 training Loss: 0.2135780998307779 validation Loss: 0.218164772110988  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2557 training Loss: 0.2135014564508587 validation Loss: 0.2180836718547431  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2558 training Loss: 0.21342494260125397 validation Loss: 0.21800265580495629  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2559 training Loss: 0.21334855777292602 validation Loss: 0.21792172641345295  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2560 training Loss: 0.21327230146239376 validation Loss: 0.21784088591345915  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2561 training Loss: 0.21319617317150735 validation Loss: 0.21776013633877236  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2562 training Loss: 0.2131201724072417 validation Loss: 0.21767947954116237  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2563 training Loss: 0.21304429868150662 validation Loss: 0.2175989172061675  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2564 training Loss: 0.2129685515109719 validation Loss: 0.21751845086744045  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2565 training Loss: 0.21289293041690519 validation Loss: 0.21743808191977748  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2566 training Loss: 0.2128174349250223 validation Loss: 0.21735781163095755  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2567 training Loss: 0.2127420645653477 validation Loss: 0.21727764115250234  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2568 training Loss: 0.21266681887208475 validation Loss: 0.21719757152945837  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2569 training Loss: 0.21259169738349454 validation Loss: 0.21711760370929478  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2570 training Loss: 0.21251669964178316 validation Loss: 0.21703773854999744  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2571 training Loss: 0.21244182519299537 validation Loss: 0.2169579768274369  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2572 training Loss: 0.2123670735869153 validation Loss: 0.2168783192420767  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2573 training Loss: 0.21229244437697337 validation Loss: 0.2167987664250842  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2574 training Loss: 0.212217937120158 validation Loss: 0.21671931894389948  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2575 training Loss: 0.2121435513769334 validation Loss: 0.21663997730731377  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2576 training Loss: 0.21206928671116104 validation Loss: 0.21656074197010095  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2577 training Loss: 0.2119951426900265 validation Loss: 0.2164816133372454  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2578 training Loss: 0.211921118883969 validation Loss: 0.2164025917678031  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2579 training Loss: 0.21184721486661606 validation Loss: 0.21632367757842885  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2580 training Loss: 0.21177343021472037 validation Loss: 0.2162448710466017  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2581 training Loss: 0.21169976450810066 validation Loss: 0.21616617241357555  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2582 training Loss: 0.21162621732958534 validation Loss: 0.21608758188708  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2583 training Loss: 0.2115527882649583 validation Loss: 0.2160090996437947  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2584 training Loss: 0.21147947690290828 validation Loss: 0.21593072583161738  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2585 training Loss: 0.2114062828349798 validation Loss: 0.21585246057174448  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2586 training Loss: 0.2113332056555268 validation Loss: 0.21577430396058114  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2587 training Loss: 0.21126024496166793 validation Loss: 0.2156962560714958  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2588 training Loss: 0.21118740035324435 validation Loss: 0.21561831695643396  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2589 training Loss: 0.2111148787314301 validation Loss: 0.21555608352042938  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2590 training Loss: 0.21104266171192315 validation Loss: 0.21547710370692547  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2591 training Loss: 0.21097060578402135 validation Loss: 0.21541466747768903  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2592 training Loss: 0.21089857003478227 validation Loss: 0.2153355001039795  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2593 training Loss: 0.21082705228777887 validation Loss: 0.2152722513215476  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2594 training Loss: 0.21075529038309176 validation Loss: 0.21520801828502015  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2595 training Loss: 0.21068378223000483 validation Loss: 0.21512718344847323  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2596 training Loss: 0.21061256194191597 validation Loss: 0.21506249325643031  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2597 training Loss: 0.21054122603448985 validation Loss: 0.2149969702979898  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2598 training Loss: 0.2104700965840455 validation Loss: 0.21491493632421585  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2599 training Loss: 0.2103992918743958 validation Loss: 0.21484922286984906  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2600 training Loss: 0.2103283662748615 validation Loss: 0.21478279025699484  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2601 training Loss: 0.210257587229201 validation Loss: 0.2147157093855853  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2602 training Loss: 0.21018710918640604 validation Loss: 0.21463222137822466  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2603 training Loss: 0.21011669327462706 validation Loss: 0.21456527499591602  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2604 training Loss: 0.210046314780389 validation Loss: 0.21449832878113967  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2605 training Loss: 0.20997615731605032 validation Loss: 0.2144308158320133  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2606 training Loss: 0.209906194700772 validation Loss: 0.21434691429850924  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2607 training Loss: 0.20983644426627351 validation Loss: 0.2142796802895462  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2608 training Loss: 0.20976666531829566 validation Loss: 0.2142119261770898  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2609 training Loss: 0.2096970216419212 validation Loss: 0.21414370478367553  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2610 training Loss: 0.20962751149583572 validation Loss: 0.2140750641485311  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2611 training Loss: 0.20955813329608608 validation Loss: 0.21400604796278894  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2612 training Loss: 0.2094889499460535 validation Loss: 0.2139207333897006  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2613 training Loss: 0.2094198877638138 validation Loss: 0.21385233349498464  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2614 training Loss: 0.20935087022294335 validation Loss: 0.21378355708508712  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2615 training Loss: 0.20928198116287758 validation Loss: 0.21371444411778667  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2616 training Loss: 0.2092132192863841 validation Loss: 0.21364503093709677  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2617 training Loss: 0.20914458339215872 validation Loss: 0.21357535060386462  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2618 training Loss: 0.20907607236101203 validation Loss: 0.21350543319653165  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2619 training Loss: 0.20900768514439075 validation Loss: 0.2134353060846249  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2620 training Loss: 0.20893942075482186 validation Loss: 0.21336499417734994  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2621 training Loss: 0.208871278257939 validation Loss: 0.21329452014947292  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2622 training Loss: 0.2088032567658133 validation Loss: 0.21322188049063462  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2623 training Loss: 0.20873535543135988 validation Loss: 0.2131484873127548  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2624 training Loss: 0.20866757344363301 validation Loss: 0.2130749952982907  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2625 training Loss: 0.20859991002385475 validation Loss: 0.21300142020209353  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2626 training Loss: 0.20853236442205178 validation Loss: 0.21292777637506177  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2627 training Loss: 0.20846493591419496 validation Loss: 0.2128540768931069  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2628 training Loss: 0.20839762379975743 validation Loss: 0.21278033367414972  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2629 training Loss: 0.20833042739962002 validation Loss: 0.2127065575842474  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2630 training Loss: 0.2082633460542657 validation Loss: 0.2126327585338556  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2631 training Loss: 0.20819637912221628 validation Loss: 0.2125589455651389  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2632 training Loss: 0.20812952597867138 validation Loss: 0.21248512693115815  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2633 training Loss: 0.20806278601431683 validation Loss: 0.21241131016769174  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2634 training Loss: 0.20799615863427642 validation Loss: 0.2123375021583752  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2635 training Loss: 0.20792964325718405 validation Loss: 0.2122637091937818  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2636 training Loss: 0.20786323931435854 validation Loss: 0.2121899370250111  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2637 training Loss: 0.20779694624906525 validation Loss: 0.212116190912297  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2638 training Loss: 0.20773076351585198 validation Loss: 0.21204248018906852  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2639 training Loss: 0.20766469057994885 validation Loss: 0.21196888297970076  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2640 training Loss: 0.20759872691672343 validation Loss: 0.2118953246683219  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2641 training Loss: 0.20753287201118292 validation Loss: 0.21182180895278382  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2642 training Loss: 0.20746712535751818 validation Loss: 0.21174833922348274  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2643 training Loss: 0.20740148645868423 validation Loss: 0.21167491859046833  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2644 training Loss: 0.207335954826012 validation Loss: 0.21160154990805227  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2645 training Loss: 0.20727052997884934 validation Loss: 0.21152823579714905  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2646 training Loss: 0.20720521144422605 validation Loss: 0.2114549786655634  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2647 training Loss: 0.20713999875654207 validation Loss: 0.21138178072641556  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2648 training Loss: 0.20707489145727542 validation Loss: 0.2113086440148781  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2649 training Loss: 0.2070098890947087 validation Loss: 0.21123557040338337  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2650 training Loss: 0.2069449912236716 validation Loss: 0.21116256161544444  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2651 training Loss: 0.20688019740529895 validation Loss: 0.21108961923821798  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2652 training Loss: 0.20681550720680175 validation Loss: 0.21101674473392823  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2653 training Loss: 0.20675103105960732 validation Loss: 0.21094558457103782  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2654 training Loss: 0.20668676714040307 validation Loss: 0.21087437500721135  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2655 training Loss: 0.20662260434607724 validation Loss: 0.2108031276093381  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2656 training Loss: 0.20655854223669784 validation Loss: 0.21073185290584562  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2657 training Loss: 0.2064945803835516 validation Loss: 0.21066056048159051  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2658 training Loss: 0.20643071836795687 validation Loss: 0.210589259064  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2659 training Loss: 0.2063669557802599 validation Loss: 0.21051795660126668  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2660 training Loss: 0.20630329221898416 validation Loss: 0.21044666033332646  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2661 training Loss: 0.20623972729010592 validation Loss: 0.2103753768562841  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2662 training Loss: 0.20617626060643549 validation Loss: 0.21030411218088996  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2663 training Loss: 0.2061128917870857 validation Loss: 0.21023287178561853  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2664 training Loss: 0.20604962045701403 validation Loss: 0.2101616606648461  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2665 training Loss: 0.2059864462466256 validation Loss: 0.2100904833725822  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2666 training Loss: 0.20592336879142764 validation Loss: 0.21001934406216724  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2667 training Loss: 0.20586038773172682 validation Loss: 0.20994824652230826  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2668 training Loss: 0.20579750271236333 validation Loss: 0.20987719420979492  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2669 training Loss: 0.20573471338247504 validation Loss: 0.20980619027920197  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2670 training Loss: 0.2056720193952881 validation Loss: 0.20973523760985857  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2671 training Loss: 0.20560942040792934 validation Loss: 0.2096643388303386  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2672 training Loss: 0.20554691608125827 validation Loss: 0.2095934963407016  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2673 training Loss: 0.20548450607971402 validation Loss: 0.2095227123326935  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2674 training Loss: 0.2054221900711776 validation Loss: 0.20945198880809685  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2675 training Loss: 0.20535996772684517 validation Loss: 0.20938132759540093  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2676 training Loss: 0.2052978387211125 validation Loss: 0.20931073036495093  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2677 training Loss: 0.20523580273146827 validation Loss: 0.2092401986427133  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2678 training Loss: 0.20517385943839542 validation Loss: 0.20916973382278856  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2679 training Loss: 0.20511200852527964 validation Loss: 0.20909933717878648  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2680 training Loss: 0.20505024967832397 validation Loss: 0.20902900987416964  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2681 training Loss: 0.20498858258646951 validation Loss: 0.2089587529716601  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2682 training Loss: 0.20492700694132057 validation Loss: 0.20888856744179668  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2683 training Loss: 0.20486552243707462 validation Loss: 0.20881845417072098  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2684 training Loss: 0.20480412877045617 validation Loss: 0.20874841396726382  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2685 training Loss: 0.20474282564065446 validation Loss: 0.20867844756939588  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2686 training Loss: 0.20468161274926444 validation Loss: 0.20860855565010222  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2687 training Loss: 0.20462048980023112 validation Loss: 0.2085387388227327  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2688 training Loss: 0.2045594564997962 validation Loss: 0.2084689976458775  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2689 training Loss: 0.20449851255644774 validation Loss: 0.2083993326278111  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2690 training Loss: 0.20443765768087213 validation Loss: 0.20832974423054354  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2691 training Loss: 0.20437689158590838 validation Loss: 0.2082602328735158  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2692 training Loss: 0.20431621398650426 validation Loss: 0.2081907989369725  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2693 training Loss: 0.20425562459967458 validation Loss: 0.2081214427650393  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2694 training Loss: 0.20419512314446128 validation Loss: 0.20805216466853443  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2695 training Loss: 0.20413470934189523 validation Loss: 0.2079829649275364  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2696 training Loss: 0.20407438291495936 validation Loss: 0.20791384379373198  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2697 training Loss: 0.20401414358855371 validation Loss: 0.2078448014925625  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2698 training Loss: 0.20395399108946144 validation Loss: 0.20777583822518825  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2699 training Loss: 0.20389392514631677 validation Loss: 0.20770695417028695  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2700 training Loss: 0.20383394548957345 validation Loss: 0.20763717383163158  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2701 training Loss: 0.20377405185147496 validation Loss: 0.20756652913559856  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2702 training Loss: 0.2037142439660256 validation Loss: 0.20749339811311235  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2703 training Loss: 0.20365452156896266 validation Loss: 0.20741820468915104  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2704 training Loss: 0.20359488439772971 validation Loss: 0.20734311740410824  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2705 training Loss: 0.20353533219145045 validation Loss: 0.207268136244388  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2706 training Loss: 0.20347586469090442 validation Loss: 0.20719326118339595  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2707 training Loss: 0.2034164816385021 validation Loss: 0.20711849218246153  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2708 training Loss: 0.2033571827782622 validation Loss: 0.2070438291916855  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2709 training Loss: 0.20329796785578894 validation Loss: 0.2069692721507178  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2710 training Loss: 0.20323883661825043 validation Loss: 0.2068948209894746  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2711 training Loss: 0.2031797888143575 validation Loss: 0.20682047562879693  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2712 training Loss: 0.20312082419434327 validation Loss: 0.2067462359810591  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2713 training Loss: 0.20306194250994353 validation Loss: 0.2066721019507289  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2714 training Loss: 0.20300314351437754 validation Loss: 0.20659807343488443  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2715 training Loss: 0.20294442696232945 validation Loss: 0.2065241503236922  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2716 training Loss: 0.20288579260993028 validation Loss: 0.20645033250084913  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2717 training Loss: 0.2028272402147405 validation Loss: 0.20637661984399108  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2718 training Loss: 0.2027687695357329 validation Loss: 0.20630301222507222  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2719 training Loss: 0.2027103803332762 validation Loss: 0.20622950951071603  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2720 training Loss: 0.20265207236911903 validation Loss: 0.2061561115625417  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2721 training Loss: 0.20259384540637426 validation Loss: 0.20608281823746713  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2722 training Loss: 0.2025356992095037 validation Loss: 0.20600962938799114  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2723 training Loss: 0.20247763354430356 validation Loss: 0.20593654486245597  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2724 training Loss: 0.2024196481778899 validation Loss: 0.20586356450529208  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2725 training Loss: 0.20236174287868444 validation Loss: 0.2057906881572468  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2726 training Loss: 0.2023039174164011 validation Loss: 0.20571791565559783  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2727 training Loss: 0.20224617156203278 validation Loss: 0.20564524683435306  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2728 training Loss: 0.20218850508783784 validation Loss: 0.20557268152443745  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2729 training Loss: 0.20213091776732772 validation Loss: 0.20550021955386868  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2730 training Loss: 0.20207340937525448 validation Loss: 0.20542786074792133  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2731 training Loss: 0.20201597968759874 validation Loss: 0.20535560492928198  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2732 training Loss: 0.2019586284815575 validation Loss: 0.20528345191819428  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2733 training Loss: 0.20190135553553287 validation Loss: 0.20521140153259643  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2734 training Loss: 0.20184416062912053 validation Loss: 0.20513945358824975  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2735 training Loss: 0.20178704354309868 validation Loss: 0.20506760789886103  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2736 training Loss: 0.20173000405941707 validation Loss: 0.20499586427619734  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2737 training Loss: 0.20167304196118643 validation Loss: 0.20492422253019538  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2738 training Loss: 0.20161615703266803 validation Loss: 0.20485268246906402  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2739 training Loss: 0.20155934905926343 validation Loss: 0.20478124389938235  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2740 training Loss: 0.20150261782750428 validation Loss: 0.20470990662619235  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2741 training Loss: 0.20144596312504276 validation Loss: 0.20463867045308676  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2742 training Loss: 0.20138938474064152 validation Loss: 0.20456861044167307  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2743 training Loss: 0.20133288246416478 validation Loss: 0.20449870259190944  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2744 training Loss: 0.20127645608656833 validation Loss: 0.20442889881265128  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2745 training Loss: 0.20122010539989088 validation Loss: 0.20435919886551535  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2746 training Loss: 0.20116383019724482 validation Loss: 0.20428960251158335  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2747 training Loss: 0.20110763027280737 validation Loss: 0.2042201095114565  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2748 training Loss: 0.20105150542181208 validation Loss: 0.20415071962530695  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2749 training Loss: 0.20099545544054023 validation Loss: 0.20408143261292716  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2750 training Loss: 0.20093948012631213 validation Loss: 0.20401224823377673  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2751 training Loss: 0.20088357927747919 validation Loss: 0.2039431662470272  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2752 training Loss: 0.20082775269341563 validation Loss: 0.20387418641160457  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2753 training Loss: 0.20077200017451047 validation Loss: 0.2038043903361149  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2754 training Loss: 0.2007163215221597 validation Loss: 0.203734495946433  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2755 training Loss: 0.20066071653875822 validation Loss: 0.20366470555636013  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2756 training Loss: 0.2006051850276926 validation Loss: 0.20359501891464002  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2757 training Loss: 0.20054973535597567 validation Loss: 0.20352608457805876  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2758 training Loss: 0.20049437611689086 validation Loss: 0.20345720850205942  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2759 training Loss: 0.20043908946526354 validation Loss: 0.20338839423108454  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2760 training Loss: 0.20038387520575718 validation Loss: 0.20331964497199537  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2761 training Loss: 0.2003287331446795 validation Loss: 0.20325096362466022  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2762 training Loss: 0.20027366308986683 validation Loss: 0.2031823528097619  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2763 training Loss: 0.2002186648505872 validation Loss: 0.2031138148940752  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2764 training Loss: 0.2001637382374576 validation Loss: 0.20304535201344606  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2765 training Loss: 0.20010888306237487 validation Loss: 0.20297696609368132  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2766 training Loss: 0.2000540991384552 validation Loss: 0.20290865886953918  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2767 training Loss: 0.19999938627998384 validation Loss: 0.20284043190199316  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2768 training Loss: 0.19994474430237064 validation Loss: 0.2027722865939283  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2769 training Loss: 0.19989017302211226 validation Loss: 0.20270422420440987  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2770 training Loss: 0.19983567225675927 validation Loss: 0.2026362458616583  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2771 training Loss: 0.19978124182488732 validation Loss: 0.20256835257484446  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2772 training Loss: 0.19972688154607154 validation Loss: 0.2025005452448161  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2773 training Loss: 0.19967259124086448 validation Loss: 0.20243282467385004  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2774 training Loss: 0.19961837073077568 validation Loss: 0.20236519157452193  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2775 training Loss: 0.19956421983825415 validation Loss: 0.20229764657777105  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2776 training Loss: 0.19951013838667198 validation Loss: 0.20223019024023586  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2777 training Loss: 0.19945612620030975 validation Loss: 0.20216282305092464  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2778 training Loss: 0.19940218310434293 validation Loss: 0.2020955454372842  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2779 training Loss: 0.19934834054661296 validation Loss: 0.20202982734416722  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2780 training Loss: 0.1992946573918406 validation Loss: 0.20196410194178152  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2781 training Loss: 0.1992410422416968 validation Loss: 0.2018983778849596  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2782 training Loss: 0.19918749490542317 validation Loss: 0.20183266303844788  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2783 training Loss: 0.19913401519666588 validation Loss: 0.20176696454796458  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2784 training Loss: 0.1990806029328831 validation Loss: 0.20170128890486752  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2785 training Loss: 0.1990272579348516 validation Loss: 0.2016356420050028  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2786 training Loss: 0.1989739800262552 validation Loss: 0.2015700292022546  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2787 training Loss: 0.19892076903334063 validation Loss: 0.20150445535727182  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2788 training Loss: 0.19886762478463124 validation Loss: 0.20143892488180265  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2789 training Loss: 0.1988145471106857 validation Loss: 0.20137344177903288  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2790 training Loss: 0.1987615358438967 validation Loss: 0.20130800968028714  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2791 training Loss: 0.1987085908183217 validation Loss: 0.20124263187842029  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2792 training Loss: 0.1986557118695401 validation Loss: 0.20117731135819697  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2793 training Loss: 0.19860289883453291 validation Loss: 0.2011120508239317  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2794 training Loss: 0.19855015155158137 validation Loss: 0.20104685272463543  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2795 training Loss: 0.19849746986018005 validation Loss: 0.200981719276895  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2796 training Loss: 0.19844485360096387 validation Loss: 0.20091665248568916  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2797 training Loss: 0.19839230261564508 validation Loss: 0.2008516541633281  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2798 training Loss: 0.19833981674695964 validation Loss: 0.20078672594668617  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2799 training Loss: 0.19828739583862107 validation Loss: 0.20072186931288152  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2800 training Loss: 0.19823503973528017 validation Loss: 0.2006570855935438  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2801 training Loss: 0.19818274828249052 validation Loss: 0.20059237598779708  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2802 training Loss: 0.19813052132667824 validation Loss: 0.2005277415740742  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2803 training Loss: 0.19807835871511514 validation Loss: 0.20046318332086838  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2804 training Loss: 0.19802626029589557 validation Loss: 0.20039870209651853  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2805 training Loss: 0.19797422591791547 validation Loss: 0.20033429867811456  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2806 training Loss: 0.19792225543085387 validation Loss: 0.2002699737596033  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2807 training Loss: 0.1978703486851563 validation Loss: 0.2002057279591671  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2808 training Loss: 0.1978185055320196 validation Loss: 0.20014156182594028  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2809 training Loss: 0.19776672582337848 validation Loss: 0.2000774758461239  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2810 training Loss: 0.19771500941189296 validation Loss: 0.20001347044855292  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2811 training Loss: 0.19766335615093697 validation Loss: 0.19994954600976494  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2812 training Loss: 0.19761176589458776 validation Loss: 0.19988570285861623  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2813 training Loss: 0.19756023849761617 validation Loss: 0.19982194128048536  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2814 training Loss: 0.19750877381547743 validation Loss: 0.19975826152110107  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2815 training Loss: 0.19745737170430264 validation Loss: 0.1996946637900301  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2816 training Loss: 0.19740603202089071 validation Loss: 0.19963114826385386  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2817 training Loss: 0.1973547546227007 validation Loss: 0.1995677150890621  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2818 training Loss: 0.19730353936784484 validation Loss: 0.19950436438469038  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2819 training Loss: 0.19725238611508128 validation Loss: 0.19944109624472317  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2820 training Loss: 0.19720129472380774 validation Loss: 0.19937791074028374  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2821 training Loss: 0.1971502650540554 validation Loss: 0.19931480792163106  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2822 training Loss: 0.1970992969664826 validation Loss: 0.1992517878199793  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2823 training Loss: 0.19704839032236904 validation Loss: 0.19918885044915774  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2824 training Loss: 0.19699754498361052 validation Loss: 0.1991259958071239  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2825 training Loss: 0.19694676081271317 validation Loss: 0.19906322387734396  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2826 training Loss: 0.19689603767278832 validation Loss: 0.19900053463005174  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2827 training Loss: 0.1968453754275475 validation Loss: 0.19893792802339785  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2828 training Loss: 0.19679477394129744 validation Loss: 0.19887540400449785  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2829 training Loss: 0.1967442330789352 validation Loss: 0.19881296251038938  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2830 training Loss: 0.1966937527059438 validation Loss: 0.1987506034689058  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2831 training Loss: 0.1966433326883871 validation Loss: 0.19868832679947399  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2832 training Loss: 0.19659297289290595 validation Loss: 0.19862613241384292  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2833 training Loss: 0.1965426731867133 validation Loss: 0.19856402021674938  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2834 training Loss: 0.1964924334375904 validation Loss: 0.19850199010652644  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2835 training Loss: 0.19644225351388225 validation Loss: 0.19844004197565912  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2836 training Loss: 0.1963921332844937 validation Loss: 0.19837817571129313  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2837 training Loss: 0.1963420726188854 validation Loss: 0.19831639119569952  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2838 training Loss: 0.19629207138706992 validation Loss: 0.19825468830670007  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2839 training Loss: 0.1962421294596077 validation Loss: 0.19819306691805635  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2840 training Loss: 0.1961922467076035 validation Loss: 0.19813152689982622  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2841 training Loss: 0.1961424230027023 validation Loss: 0.19807006811868988  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2842 training Loss: 0.19609265821708602 validation Loss: 0.1980086904382485  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2843 training Loss: 0.1960429522234696 validation Loss: 0.19794739371929845  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2844 training Loss: 0.19599330489509753 validation Loss: 0.1978861778200822  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2845 training Loss: 0.1959437161057404 validation Loss: 0.19782504259651792  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2846 training Loss: 0.19589418572969128 validation Loss: 0.19776398790241156  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2847 training Loss: 0.19584471364176237 validation Loss: 0.1977030135896499  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2848 training Loss: 0.19579529971728177 validation Loss: 0.19764211950837887  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2849 training Loss: 0.19574594383208968 validation Loss: 0.1975813055071667  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2850 training Loss: 0.19569664586253582 validation Loss: 0.1975205714331545  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2851 training Loss: 0.19564740568547556 validation Loss: 0.19745991713219396  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2852 training Loss: 0.1955982231782669 validation Loss: 0.19739934244897475  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2853 training Loss: 0.19554909821876737 validation Loss: 0.19733884722714098  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2854 training Loss: 0.195500030685331 validation Loss: 0.1972784313093994  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2855 training Loss: 0.19545102045680493 validation Loss: 0.1972180945376182  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2856 training Loss: 0.19540206741252644 validation Loss: 0.19715783675291843  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2857 training Loss: 0.19535317143232 validation Loss: 0.19709765779575852  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2858 training Loss: 0.19530433239649417 validation Loss: 0.19703755750601185  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2859 training Loss: 0.19525540394291113 validation Loss: 0.19696439627125145  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2860 training Loss: 0.19520494776868694 validation Loss: 0.1968930780339354  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2861 training Loss: 0.19515456719321328 validation Loss: 0.19682409580483598  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2862 training Loss: 0.19510426018171106 validation Loss: 0.19675596338288315  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2863 training Loss: 0.1950540249999293 validation Loss: 0.19668861405064925  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2864 training Loss: 0.19500386016402266 validation Loss: 0.19662198714286808  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2865 training Loss: 0.19495376439896428 validation Loss: 0.1965560274769467  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2866 training Loss: 0.1949037366040275 validation Loss: 0.19649068483948404  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2867 training Loss: 0.19485377582412378 validation Loss: 0.19642591352292488  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2868 training Loss: 0.19480388122599476 validation Loss: 0.19636167190714923  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2869 training Loss: 0.19475405207842617 validation Loss: 0.19629792208138422  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2870 training Loss: 0.19470428773579743 validation Loss: 0.19623462950233658  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2871 training Loss: 0.19465458762439702 validation Loss: 0.19617176268489672  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2872 training Loss: 0.19460495123103272 validation Loss: 0.1961092929221583  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2873 training Loss: 0.19455537809354423 validation Loss: 0.1960471940318484  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2874 training Loss: 0.19450586779289786 validation Loss: 0.195985442126568  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2875 training Loss: 0.19445641994659085 validation Loss: 0.19592401540551882  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2876 training Loss: 0.19440703420314737 validation Loss: 0.19586289396562814  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2877 training Loss: 0.19435771023751813 validation Loss: 0.19580205963020278  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2878 training Loss: 0.19430844774723308 validation Loss: 0.1957414957934298  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2879 training Loss: 0.19425924644917977 validation Loss: 0.19568118727921216  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2880 training Loss: 0.19421010607690176 validation Loss: 0.19562112021297895  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2881 training Loss: 0.19416102637833035 validation Loss: 0.19556128190524413  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2882 training Loss: 0.1941120071138773 validation Loss: 0.19550166074580946  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2883 training Loss: 0.19406304805482738 validation Loss: 0.19544224610761282  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2884 training Loss: 0.1940141489819825 validation Loss: 0.19538302825932366  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2885 training Loss: 0.1939653096845143 validation Loss: 0.19532399828587035  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2886 training Loss: 0.19391652995899147 validation Loss: 0.19526514801616562  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2887 training Loss: 0.19386780960855382 validation Loss: 0.19520646995736393  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2888 training Loss: 0.1938191484422073 validation Loss: 0.19514795723505002  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2889 training Loss: 0.1937705462742224 validation Loss: 0.19508960353881175  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2890 training Loss: 0.19372200292361816 validation Loss: 0.19503140307270675  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2891 training Loss: 0.19367351821371803 validation Loss: 0.1949733505101716  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2892 training Loss: 0.19362509197176678 validation Loss: 0.19491544095297164  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2893 training Loss: 0.1935767240285982 validation Loss: 0.19485766989382183  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2894 training Loss: 0.19352841421834627 validation Loss: 0.1948000331823448  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2895 training Loss: 0.1934801623781926 validation Loss: 0.1947425269940647  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2896 training Loss: 0.19343196834814513 validation Loss: 0.19468514780216076  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2897 training Loss: 0.1933838319708429 validation Loss: 0.194627892351732  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2898 training Loss: 0.19333575309138318 validation Loss: 0.1945707576363462  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2899 training Loss: 0.19328773155716814 validation Loss: 0.1945137408766667  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2900 training Loss: 0.19323976721776764 validation Loss: 0.19445683950097162  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2901 training Loss: 0.19319185992479618 validation Loss: 0.19440005112739353  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2902 training Loss: 0.1931440095318022 validation Loss: 0.19434337354772713  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2903 training Loss: 0.19309621589416776 validation Loss: 0.19428680471266266  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2904 training Loss: 0.1930484788690176 validation Loss: 0.19423034271831863  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2905 training Loss: 0.19300079831513595 validation Loss: 0.19417398579395784  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2906 training Loss: 0.1929531740928905 validation Loss: 0.19411773229077986  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2907 training Loss: 0.19290560606416282 validation Loss: 0.1940615806716953  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2908 training Loss: 0.19285809409228352 validation Loss: 0.19400552950199354  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2909 training Loss: 0.19281063804197324 validation Loss: 0.19394957744082503  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2910 training Loss: 0.192763237779287 validation Loss: 0.19389372323342524  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2911 training Loss: 0.19271589317156318 validation Loss: 0.19383796570401482  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2912 training Loss: 0.1926686040873753 validation Loss: 0.19378230374931624  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2913 training Loss: 0.1926213703964876 validation Loss: 0.1937267363326312  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2914 training Loss: 0.19257419196981296 validation Loss: 0.19367126247843086  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2915 training Loss: 0.1925270686793736 validation Loss: 0.1936158812674121  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2916 training Loss: 0.19248000039826438 validation Loss: 0.19356059183197938  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2917 training Loss: 0.19243298700061753 validation Loss: 0.19350539335211484  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2918 training Loss: 0.19238602836157 validation Loss: 0.19345028505160206  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2919 training Loss: 0.1923391243572329 validation Loss: 0.19339526619457256  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2920 training Loss: 0.19229227486466158 validation Loss: 0.19334033608234677  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2921 training Loss: 0.19224547976182813 validation Loss: 0.19328549405054307  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2922 training Loss: 0.19219873892759526 validation Loss: 0.19323073946643277  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2923 training Loss: 0.19215205224169113 validation Loss: 0.19317607172651766  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2924 training Loss: 0.19210541958468583 validation Loss: 0.1931214902543125  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2925 training Loss: 0.19205884083796865 validation Loss: 0.1930669944983136  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2926 training Loss: 0.19201231588372702 validation Loss: 0.19301258393013654  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2927 training Loss: 0.19196584460492594 validation Loss: 0.1929582580428107  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2928 training Loss: 0.19191942688528862 validation Loss: 0.1929040163492145  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2929 training Loss: 0.19187306260927797 validation Loss: 0.1928498583806402  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2930 training Loss: 0.19182675166207894 validation Loss: 0.19279578368547748  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2931 training Loss: 0.1917804939295818 validation Loss: 0.19274179182800474  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2932 training Loss: 0.1917342892983662 validation Loss: 0.19268788238727927  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2933 training Loss: 0.19168813765568546 validation Loss: 0.1926340549561178  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2934 training Loss: 0.19164203888945225 validation Loss: 0.19258030914015953  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2935 training Loss: 0.19159599288822432 validation Loss: 0.19252664455700408  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2936 training Loss: 0.1915499995411914 validation Loss: 0.19247306083541948  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2937 training Loss: 0.1915040587381621 validation Loss: 0.19241955761461255  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2938 training Loss: 0.19145817036955154 validation Loss: 0.1923661345435573  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2939 training Loss: 0.1914123343263699 validation Loss: 0.1923127912803758  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2940 training Loss: 0.1913665505002109 validation Loss: 0.192259527491768  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2941 training Loss: 0.19132081878324098 validation Loss: 0.19220634285248583  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2942 training Loss: 0.19127513906818894 validation Loss: 0.19215323704484719  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2943 training Loss: 0.19122951124833598 validation Loss: 0.1921002097582877  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2944 training Loss: 0.19118393521750598 validation Loss: 0.19204726068894673  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2945 training Loss: 0.19113841087005662 validation Loss: 0.19199438953928405  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2946 training Loss: 0.19109293810087016 validation Loss: 0.1919415960177254  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2947 training Loss: 0.19104751680534515 validation Loss: 0.19188887983833444  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2948 training Loss: 0.19100214687938816 validation Loss: 0.19183624072050862  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2949 training Loss: 0.19095682821940585 validation Loss: 0.1917833318893054  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2950 training Loss: 0.19091156072229748 validation Loss: 0.1917300517545104  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2951 training Loss: 0.19086634428544755 validation Loss: 0.1916768499679398  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2952 training Loss: 0.19082155500721812 validation Loss: 0.19163547966010272  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2953 training Loss: 0.19077695448220214 validation Loss: 0.19159302383331162  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2954 training Loss: 0.19073274579761873 validation Loss: 0.1915377691300495  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2955 training Loss: 0.19068833373672134 validation Loss: 0.1914945896357889  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2956 training Loss: 0.190643973766001 validation Loss: 0.19145049423109123  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2957 training Loss: 0.1905997040784041 validation Loss: 0.19140556539705836  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2958 training Loss: 0.19055576225593768 validation Loss: 0.1913479993915287  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2959 training Loss: 0.1905116558876256 validation Loss: 0.19130279462342595  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2960 training Loss: 0.19046760266310717 validation Loss: 0.1912568672320945  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2961 training Loss: 0.19042363170231702 validation Loss: 0.19121028347719787  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2962 training Loss: 0.19037974063687946 validation Loss: 0.19116310405120268  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2963 training Loss: 0.19033607649082407 validation Loss: 0.19110342864829866  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2964 training Loss: 0.19029235814091927 validation Loss: 0.19105639517585557  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2965 training Loss: 0.1902486636712115 validation Loss: 0.19100882113645754  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2966 training Loss: 0.1902050443001952 validation Loss: 0.19096075744208799  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2967 training Loss: 0.1901614983797817 validation Loss: 0.1909122507109319  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2968 training Loss: 0.19011802442852282 validation Loss: 0.19086334362571947  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2969 training Loss: 0.1900746211077356 validation Loss: 0.19081407526331215  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2970 training Loss: 0.19003128720142798 validation Loss: 0.19076448139761232  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2971 training Loss: 0.1899880724688839 validation Loss: 0.19070253070882057  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2972 training Loss: 0.18994489791541094 validation Loss: 0.19065356197732128  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2973 training Loss: 0.1899017407044236 validation Loss: 0.19060425947517098  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2974 training Loss: 0.18985865062761775 validation Loss: 0.1905546566486954  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2975 training Loss: 0.18981562669005897 validation Loss: 0.19050478411785077  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2976 training Loss: 0.18977266797725756 validation Loss: 0.1904546699148638  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2977 training Loss: 0.18972977364465213 validation Loss: 0.19040433970311643  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2978 training Loss: 0.1896869429087139 validation Loss: 0.19035381697782264  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2979 training Loss: 0.18964417503940897 validation Loss: 0.19030312324993962  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2980 training Loss: 0.18960146935379857 validation Loss: 0.19025227821465532  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2981 training Loss: 0.1895588252105929 validation Loss: 0.19020129990569803  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2982 training Loss: 0.18951624200550674 validation Loss: 0.19015020483662176  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2983 training Loss: 0.18947371916728672 validation Loss: 0.19009900813013644  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2984 training Loss: 0.18943125615430517 validation Loss: 0.19004772363646785  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2985 training Loss: 0.18938885245162956 validation Loss: 0.1899963640416585  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2986 training Loss: 0.1893465075684932 validation Loss: 0.18994494096664793  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2987 training Loss: 0.18930422103610495 validation Loss: 0.18989346505790422  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2988 training Loss: 0.1892619924057446 validation Loss: 0.18984194607031774  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2989 training Loss: 0.18921982124710135 validation Loss: 0.1897903929430091  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2990 training Loss: 0.1891777071468166 validation Loss: 0.18973881386865224  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2991 training Loss: 0.18913564970720262 validation Loss: 0.18968721635686153  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2992 training Loss: 0.18909348107039436 validation Loss: 0.18962765819981617  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2993 training Loss: 0.1890502945163171 validation Loss: 0.18956865429653896  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2994 training Loss: 0.1890071755170034 validation Loss: 0.189510163166755  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2995 training Loss: 0.18896412302029952 validation Loss: 0.18945214698281007  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2996 training Loss: 0.18892113607545827 validation Loss: 0.18939457123959946  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2997 training Loss: 0.1888782138184179 validation Loss: 0.18933740445536554  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2998 training Loss: 0.18883535545944796 validation Loss: 0.18928061790032102  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 2999 training Loss: 0.18879256027276947 validation Loss: 0.18922418535038144  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 3000 training Loss: 0.1887498275878251 validation Loss: 0.18916808286357173  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 3001 training Loss: 0.1887071567819255 validation Loss: 0.18911228857692716  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 3002 training Loss: 0.18866454727404752 validation Loss: 0.1890567825219313  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 3003 training Loss: 0.1886219985195941 validation Loss: 0.1890015464567316  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 3004 training Loss: 0.18857951000595993 validation Loss: 0.18894656371355215  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 3005 training Loss: 0.18853708124877017 validation Loss: 0.18889181905987823  valid acc: 0.8933333333333333  train Acc: 0.8742857142857143\n",
      "epoch: 3006 training Loss: 0.18849471178868363 validation Loss: 0.18883729857212922  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3007 training Loss: 0.18845240118866993 validation Loss: 0.18878298952066228  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3008 training Loss: 0.18841014903168213 validation Loss: 0.1887288802650609  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3009 training Loss: 0.18836795491866387 validation Loss: 0.18867496015876337  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3010 training Loss: 0.18832581846683583 validation Loss: 0.18862121946217703  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3011 training Loss: 0.18828374181127228 validation Loss: 0.18856806393970454  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3012 training Loss: 0.18824173454645676 validation Loss: 0.18851541810516403  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3013 training Loss: 0.18819979949840493 validation Loss: 0.1884628758236699  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3014 training Loss: 0.18815792049037255 validation Loss: 0.18841043460919957  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3015 training Loss: 0.18811609720889708 validation Loss: 0.18835809217446836  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3016 training Loss: 0.18807432934865345 validation Loss: 0.1883058464138907  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3017 training Loss: 0.18803261661211187 validation Loss: 0.18825369538801848  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3018 training Loss: 0.18799095870921495 validation Loss: 0.18820163730932804  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3019 training Loss: 0.187949355357072 validation Loss: 0.18814967052923573  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3020 training Loss: 0.18790779254188744 validation Loss: 0.18807213048726149  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3021 training Loss: 0.18786241457805106 validation Loss: 0.18799646933526556  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3022 training Loss: 0.18781721506085733 validation Loss: 0.18792251921650702  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3023 training Loss: 0.18777218172341878 validation Loss: 0.18785012785387586  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3024 training Loss: 0.18772730392769144 validation Loss: 0.18777915704518733  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3025 training Loss: 0.18768257241825495 validation Loss: 0.18770948131563975  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3026 training Loss: 0.1876379791152671 validation Loss: 0.18764098670916995  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3027 training Loss: 0.1875935213584272 validation Loss: 0.18757437221302023  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3028 training Loss: 0.18754927755673992 validation Loss: 0.18750869154297753  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3029 training Loss: 0.18750515266986084 validation Loss: 0.18744386327509854  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3030 training Loss: 0.18746114196357241 validation Loss: 0.18737981340482046  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3031 training Loss: 0.18741724334063486 validation Loss: 0.1873166895595361  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3032 training Loss: 0.18737346715439684 validation Loss: 0.1872541985415458  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3033 training Loss: 0.1873297930143708 validation Loss: 0.1871922863161282  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3034 training Loss: 0.18728621792083683 validation Loss: 0.18713090377310768  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3035 training Loss: 0.1872427391509988 validation Loss: 0.18707000628270654  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3036 training Loss: 0.1871993542244537 validation Loss: 0.18700955329258115  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3037 training Loss: 0.18715606087367223 validation Loss: 0.18694950796198678  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3038 training Loss: 0.18711285701871078 validation Loss: 0.1868898368294461  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3039 training Loss: 0.1870697407454961 validation Loss: 0.1868305095106885  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3040 training Loss: 0.18702671028713544 validation Loss: 0.18677149842396384  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3041 training Loss: 0.18698376400778577 validation Loss: 0.18671277854013493  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3042 training Loss: 0.18694090038869648 validation Loss: 0.18665432715521868  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3043 training Loss: 0.18689811801609643 validation Loss: 0.1865961236832822  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3044 training Loss: 0.18685541557065075 validation Loss: 0.18653814946780684  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3045 training Loss: 0.1868127918182555 validation Loss: 0.1864803876098227  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3046 training Loss: 0.18677024560197464 validation Loss: 0.1864228228112795  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3047 training Loss: 0.1867277758349548 validation Loss: 0.18636544123226775  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3048 training Loss: 0.1866853814941777 validation Loss: 0.18630823036084024  valid acc: 0.9  train Acc: 0.8742857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3049 training Loss: 0.186643061614933 validation Loss: 0.18625117889429882  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3050 training Loss: 0.18660081528591255 validation Loss: 0.1861942766309191  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3051 training Loss: 0.18655864164483968 validation Loss: 0.18613751437118406  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3052 training Loss: 0.1865165693223973 validation Loss: 0.18608141594914485  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3053 training Loss: 0.18647459711262238 validation Loss: 0.18602540887166  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3054 training Loss: 0.1864326952504897 validation Loss: 0.18596948902986707  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3055 training Loss: 0.18639086302065586 validation Loss: 0.18591365278440847  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3056 training Loss: 0.18634909974046385 validation Loss: 0.1858578969215309  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3057 training Loss: 0.18630740475784308 validation Loss: 0.18580221861303559  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3058 training Loss: 0.18626577744937034 validation Loss: 0.18574661537974715  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3059 training Loss: 0.18622421721847476 validation Loss: 0.18569108505819623  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3060 training Loss: 0.18618272349377477 validation Loss: 0.18563562577023637  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3061 training Loss: 0.18614129572753357 validation Loss: 0.18558023589534392  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3062 training Loss: 0.1860999333942247 validation Loss: 0.18552491404536728  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3063 training Loss: 0.1860586359891961 validation Loss: 0.18546965904151477  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3064 training Loss: 0.18601740302742656 validation Loss: 0.18541446989338714  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3065 training Loss: 0.18597623404236516 validation Loss: 0.1853593457798794  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3066 training Loss: 0.1859351285848487 validation Loss: 0.18530428603178917  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3067 training Loss: 0.18589408622208997 validation Loss: 0.18524929011598426  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3068 training Loss: 0.18585310653673157 validation Loss: 0.1851943576209964  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3069 training Loss: 0.18581218912596162 validation Loss: 0.18513948824391566  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3070 training Loss: 0.18577133360068485 validation Loss: 0.1850846817784742  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3071 training Loss: 0.18573053958474686 validation Loss: 0.18502993810421606  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3072 training Loss: 0.18568980671420673 validation Loss: 0.18497525717665916  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3073 training Loss: 0.18564913463665522 validation Loss: 0.18492063901836325  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3074 training Loss: 0.1856085230105751 validation Loss: 0.18486608371082472  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3075 training Loss: 0.18556797150474064 validation Loss: 0.18481159138712716  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3076 training Loss: 0.18552747979765433 validation Loss: 0.1847571622252813  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3077 training Loss: 0.18548704757701737 validation Loss: 0.18470279644219414  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3078 training Loss: 0.1854466745392329 validation Loss: 0.18464849428821245  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3079 training Loss: 0.18540636038893848 validation Loss: 0.18459425604219082  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3080 training Loss: 0.18536610483856725 validation Loss: 0.18454008200703692  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3081 training Loss: 0.185325907607935 validation Loss: 0.18448597250569368  valid acc: 0.9  train Acc: 0.8742857142857143\n",
      "epoch: 3082 training Loss: 0.18528576842385236 validation Loss: 0.18443192787751805  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3083 training Loss: 0.18524568701975933 validation Loss: 0.18437794847502276  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3084 training Loss: 0.18520566313538203 validation Loss: 0.18432403466094846  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3085 training Loss: 0.1851656965164097 validation Loss: 0.18427018680563612  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3086 training Loss: 0.18512578691419032 validation Loss: 0.1842164052846741  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3087 training Loss: 0.18508593408544471 validation Loss: 0.18416269047679396  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3088 training Loss: 0.18504613779199694 validation Loss: 0.18410904276199386  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3089 training Loss: 0.18500639780052047 validation Loss: 0.18405546251986807  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3090 training Loss: 0.1849667138822997 validation Loss: 0.18400195012812412  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3091 training Loss: 0.18492708581300446 validation Loss: 0.18394850596127138  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3092 training Loss: 0.18488751337247847 validation Loss: 0.18389513038946328  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3093 training Loss: 0.18484799634453924 validation Loss: 0.1838418237774813  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3094 training Loss: 0.18480855151848444 validation Loss: 0.1837889526834668  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3095 training Loss: 0.1847691877963435 validation Loss: 0.1837361242611875  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3096 training Loss: 0.18472987843350297 validation Loss: 0.18368334126215158  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3097 training Loss: 0.18469062323620172 validation Loss: 0.18363060622136  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3098 training Loss: 0.18465142201363813 validation Loss: 0.18357792147455104  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3099 training Loss: 0.18461227457780666 validation Loss: 0.18352528917402178  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3100 training Loss: 0.18457318074334855 validation Loss: 0.18347271130314532  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3101 training Loss: 0.1845341403274162 validation Loss: 0.18342018968969334  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3102 training Loss: 0.18449515314954823 validation Loss: 0.1833677260180627  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3103 training Loss: 0.1844562190315554 validation Loss: 0.18331532184049795  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3104 training Loss: 0.18441733779741534 validation Loss: 0.18326297858739468  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3105 training Loss: 0.18437850927317526 validation Loss: 0.1832106975767584  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3106 training Loss: 0.1843397332868623 validation Loss: 0.18315848002289128  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3107 training Loss: 0.18430100966840005 validation Loss: 0.18310632704437027  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3108 training Loss: 0.18426233824953125 validation Loss: 0.18305423967137563  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3109 training Loss: 0.18422371886374597 validation Loss: 0.18300221885242512  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3110 training Loss: 0.18418515134621413 validation Loss: 0.18295026546056162  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3111 training Loss: 0.18414663553372312 validation Loss: 0.18289838029904082  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3112 training Loss: 0.18410817126461917 validation Loss: 0.18284656410656092  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3113 training Loss: 0.1840697583787524 validation Loss: 0.18279481756207142  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3114 training Loss: 0.184031396717425 validation Loss: 0.1827431412891962  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3115 training Loss: 0.18399308612334345 validation Loss: 0.18269153586030343  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3116 training Loss: 0.18395482644057276 validation Loss: 0.18264000180025175  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3117 training Loss: 0.1839166175144936 validation Loss: 0.18258853958983745  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3118 training Loss: 0.18387845919176218 validation Loss: 0.18253714966897086  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3119 training Loss: 0.18384035132027235 validation Loss: 0.18248583243960098  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3120 training Loss: 0.18380229374911924 validation Loss: 0.1824345882684111  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3121 training Loss: 0.18376428632856598 validation Loss: 0.18238341748930362  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3122 training Loss: 0.18372632891001112 validation Loss: 0.1823323204056918  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3123 training Loss: 0.1836884213459589 validation Loss: 0.1822812972926129  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3124 training Loss: 0.18365056348998995 validation Loss: 0.18223034839867974  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3125 training Loss: 0.1836127551967345 validation Loss: 0.1821794739478816  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3126 training Loss: 0.18357499632184646 validation Loss: 0.1821286741412478  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3127 training Loss: 0.18353728672197892 validation Loss: 0.18207794915838552  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3128 training Loss: 0.18349962625476093 validation Loss: 0.18202729915890076  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3129 training Loss: 0.1834620294053219 validation Loss: 0.18197727219057816  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3130 training Loss: 0.1834245226178313 validation Loss: 0.18192728652836987  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3131 training Loss: 0.1833870642605963 validation Loss: 0.18187734490778193  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3132 training Loss: 0.1833496637404234 validation Loss: 0.1818276895072904  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3133 training Loss: 0.18331232564957958 validation Loss: 0.18177806515697498  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3134 training Loss: 0.18327503538445292 validation Loss: 0.18172847553481128  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3135 training Loss: 0.1832377928063438 validation Loss: 0.18167892400469307  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3136 training Loss: 0.18320059777820505 validation Loss: 0.1816294136424488  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3137 training Loss: 0.18316345016449548 validation Loss: 0.1815799472596929  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3138 training Loss: 0.18312634983105502 validation Loss: 0.1815305274256896  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3139 training Loss: 0.1830892966449982 validation Loss: 0.1814811564873957  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3140 training Loss: 0.1830522904746228 validation Loss: 0.18143183658783338  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3141 training Loss: 0.18301533118933125 validation Loss: 0.18138256968293254  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3142 training Loss: 0.18297841865956327 validation Loss: 0.18133335755697028  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3143 training Loss: 0.1829415527567374 validation Loss: 0.18128420183672497  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3144 training Loss: 0.1829047333532008 validation Loss: 0.1812351040044509  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3145 training Loss: 0.18286796032218505 validation Loss: 0.18118606540977367  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3146 training Loss: 0.18283123353776826 validation Loss: 0.1811370872805961  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3147 training Loss: 0.1827945528748413 validation Loss: 0.1810881707330959  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3148 training Loss: 0.18275791820907863 validation Loss: 0.18103931678089297  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3149 training Loss: 0.18272132941691213 validation Loss: 0.18099052634345497  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3150 training Loss: 0.18268478637550825 validation Loss: 0.1809418002538041  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3151 training Loss: 0.18264828896274743 validation Loss: 0.1808931392655851  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3152 training Loss: 0.1826118370572058 validation Loss: 0.1808445440595463  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3153 training Loss: 0.18257543053813854 validation Loss: 0.1807960152494837  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3154 training Loss: 0.1825390692854652 validation Loss: 0.1807475533876928  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3155 training Loss: 0.18250275317975598 validation Loss: 0.18069915896996896  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3156 training Loss: 0.18246648210221927 validation Loss: 0.18065083244019495  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3157 training Loss: 0.18243025593469067 validation Loss: 0.18060257419454875  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3158 training Loss: 0.18239407455962217 validation Loss: 0.18055438458536463  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3159 training Loss: 0.18235793786007254 validation Loss: 0.18050626392467578  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3160 training Loss: 0.1823218457196986 validation Loss: 0.1804582124874652  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3161 training Loss: 0.1822857980227464 validation Loss: 0.18041023051464905  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3162 training Loss: 0.1822497946540436 validation Loss: 0.18036231821581525  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3163 training Loss: 0.182213835498992 validation Loss: 0.18031447577173684  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3164 training Loss: 0.18217792044356043 validation Loss: 0.18026670333668066  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3165 training Loss: 0.1821420493742782 validation Loss: 0.1802190010405252  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3166 training Loss: 0.18210622217822864 validation Loss: 0.18017136899070743  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3167 training Loss: 0.18207043874304324 validation Loss: 0.18012380727400942  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3168 training Loss: 0.18203469895689578 validation Loss: 0.18007631595820034  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3169 training Loss: 0.18199900270849667 validation Loss: 0.18002889509354467  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3170 training Loss: 0.18196334988708782 validation Loss: 0.1799815447141878  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3171 training Loss: 0.18192774038243742 validation Loss: 0.17993426483943048  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3172 training Loss: 0.18189217408483493 validation Loss: 0.17988705547489872  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3173 training Loss: 0.1818566508850865 validation Loss: 0.17983991661362062  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3174 training Loss: 0.18182117067451004 validation Loss: 0.17979284823701622  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3175 training Loss: 0.18178573334493098 validation Loss: 0.17974585031580775  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3176 training Loss: 0.18175033878867777 validation Loss: 0.17969892281085711  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3177 training Loss: 0.18171498689857762 validation Loss: 0.1796520656739364  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3178 training Loss: 0.18167967756795247 validation Loss: 0.1796052788484371  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3179 training Loss: 0.1816444106906149 validation Loss: 0.17955856227002323  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3180 training Loss: 0.1816091861608641 validation Loss: 0.17951191586723236  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3181 training Loss: 0.18157400387348221 validation Loss: 0.17946533956202912  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3182 training Loss: 0.18153886372373043 validation Loss: 0.179418833270316  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3183 training Loss: 0.18150376560734552 validation Loss: 0.17937239690240248  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3184 training Loss: 0.1814687162805154 validation Loss: 0.17932638168620244  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3185 training Loss: 0.18143371647249384 validation Loss: 0.17928040876743193  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3186 training Loss: 0.18139875836409894 validation Loss: 0.17923448041107307  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3187 training Loss: 0.1813638418504751 validation Loss: 0.1791885986781525  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3188 training Loss: 0.18132896682760433 validation Loss: 0.1791427654427671  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3189 training Loss: 0.18129413319224352 validation Loss: 0.1790969824076946  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3190 training Loss: 0.18125893928543793 validation Loss: 0.17903879281442622  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3191 training Loss: 0.18122209182668145 validation Loss: 0.17898152272843765  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3192 training Loss: 0.18118530559440524 validation Loss: 0.17892510256217298  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3193 training Loss: 0.1811485807499373 validation Loss: 0.1788697081393272  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3194 training Loss: 0.18111194711471046 validation Loss: 0.1788150267276381  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3195 training Loss: 0.18107536908098018 validation Loss: 0.17876100626768207  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3196 training Loss: 0.1810388454544562 validation Loss: 0.17870759910492726  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3197 training Loss: 0.18100237519740842 validation Loss: 0.17865476160158503  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3198 training Loss: 0.18096595740436697 validation Loss: 0.17860245378431544  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3199 training Loss: 0.1809295912816914 validation Loss: 0.178550639024244  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3200 training Loss: 0.18089327613038764 validation Loss: 0.1784992837461329  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3201 training Loss: 0.18085701133165114 validation Loss: 0.178448357163883  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3202 training Loss: 0.18082079633469877 validation Loss: 0.1783978310398419  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3203 training Loss: 0.18078463064652345 validation Loss: 0.1783476794656563  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3204 training Loss: 0.180748513823263 validation Loss: 0.17829787866263672  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3205 training Loss: 0.1807124454629261 validation Loss: 0.1782484067998126  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3206 training Loss: 0.18067642519925803 validation Loss: 0.1781992438280335  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3207 training Loss: 0.1806404526965647 validation Loss: 0.1781503713286406  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3208 training Loss: 0.1806045276453429 validation Loss: 0.1781017723753733  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3209 training Loss: 0.1805686497585877 validation Loss: 0.17805343140830857  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3210 training Loss: 0.18053281876867025 validation Loss: 0.17800533411874417  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3211 training Loss: 0.18049703442469547 validation Loss: 0.17795746734404277  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3212 training Loss: 0.18046129649026338 validation Loss: 0.17790981897154598  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3213 training Loss: 0.1804256047415707 validation Loss: 0.17786237785075099  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3214 training Loss: 0.1803899589657993 validation Loss: 0.1778151337130174  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3215 training Loss: 0.18035435895974572 validation Loss: 0.17776807709814169  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3216 training Loss: 0.18031880452865498 validation Loss: 0.17772119928719424  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3217 training Loss: 0.18028329548522556 validation Loss: 0.17767449224107182  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3218 training Loss: 0.18024783164876032 validation Loss: 0.1776279485442675  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3219 training Loss: 0.1802124128444393 validation Loss: 0.17758156135340325  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3220 training Loss: 0.18017703890269657 validation Loss: 0.1775353243501144  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3221 training Loss: 0.18014170965868448 validation Loss: 0.1774892316979087  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3222 training Loss: 0.18010642495181262 validation Loss: 0.17744327800265777  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3223 training Loss: 0.18007118462534924 validation Loss: 0.17739745827640999  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3224 training Loss: 0.18003598852607608 validation Loss: 0.17735176790423854  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3225 training Loss: 0.18000083650398924 validation Loss: 0.17730620261386656  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3226 training Loss: 0.17996572841203773 validation Loss: 0.17726075844783143  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3227 training Loss: 0.17993066410589548 validation Loss: 0.17721543173797244  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3228 training Loss: 0.17989564344376116 validation Loss: 0.17717021908204492  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3229 training Loss: 0.17986066628618225 validation Loss: 0.17712511732227934  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3230 training Loss: 0.17982573249589912 validation Loss: 0.17708012352572158  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3231 training Loss: 0.17979084193770728 validation Loss: 0.17703523496620382  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3232 training Loss: 0.17975599447833435 validation Loss: 0.17699044910780828  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3233 training Loss: 0.1797211899863302 validation Loss: 0.17694576358969713  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3234 training Loss: 0.1796864283319683 validation Loss: 0.17690117621219612  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3235 training Loss: 0.17965170938715716 validation Loss: 0.17685668492402334  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3236 training Loss: 0.1796170330253597 validation Loss: 0.17681228781056998  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3237 training Loss: 0.17958239912152024 validation Loss: 0.17676798308314323  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3238 training Loss: 0.17954780755199828 validation Loss: 0.17672376906909087  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3239 training Loss: 0.1795132581945073 validation Loss: 0.1766796442027355  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3240 training Loss: 0.17947875092805898 validation Loss: 0.17663560701704817  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3241 training Loss: 0.17944428563291187 validation Loss: 0.1765916561360028  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3242 training Loss: 0.17940986219052338 validation Loss: 0.17654779026755202  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3243 training Loss: 0.17937548048350618 validation Loss: 0.1765040081971746  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3244 training Loss: 0.1793411403955866 validation Loss: 0.17646030878194613  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3245 training Loss: 0.17930684181156686 validation Loss: 0.17641669094508983  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3246 training Loss: 0.1792725846172889 validation Loss: 0.17637315367096779  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3247 training Loss: 0.17923836869960105 validation Loss: 0.17632969600047563  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3248 training Loss: 0.17920419394632653 validation Loss: 0.1762863170268084  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3249 training Loss: 0.17917006024623391 validation Loss: 0.17624301589156563  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3250 training Loss: 0.1791359674890088 validation Loss: 0.1761997917811688  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3251 training Loss: 0.17910191556522806 validation Loss: 0.17615664392356492  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3252 training Loss: 0.17906790436633438 validation Loss: 0.17611357158519225  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3253 training Loss: 0.17903393378461274 validation Loss: 0.17607057406818838  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3254 training Loss: 0.17900000371316813 validation Loss: 0.17602765070781787  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3255 training Loss: 0.17896611404590418 validation Loss: 0.17598480087010496  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3256 training Loss: 0.1789322646775028 validation Loss: 0.17594202394965153  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3257 training Loss: 0.17889845550340513 validation Loss: 0.17589931936762657  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3258 training Loss: 0.1788646864197928 validation Loss: 0.1758566865699141  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3259 training Loss: 0.1788309573235709 validation Loss: 0.17581412502540403  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3260 training Loss: 0.1787972681123508 validation Loss: 0.17577163422441702  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3261 training Loss: 0.17876361868443447 validation Loss: 0.17572921367725108  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3262 training Loss: 0.17873000893879903 validation Loss: 0.1756868629128398  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3263 training Loss: 0.17869643877508237 validation Loss: 0.1756445814775146  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3264 training Loss: 0.17866290809356872 validation Loss: 0.17560236893386086  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3265 training Loss: 0.17862941679517588 validation Loss: 0.1755602248596613  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3266 training Loss: 0.17859596478144182 validation Loss: 0.1755181488469202  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3267 training Loss: 0.1785625519545126 validation Loss: 0.1754761405009603  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3268 training Loss: 0.17852917821713055 validation Loss: 0.17543419943958796  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3269 training Loss: 0.17849584347262296 validation Loss: 0.175392325292321  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3270 training Loss: 0.17846254762489114 validation Loss: 0.17535051769967316  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3271 training Loss: 0.1784292905783999 validation Loss: 0.17530877631249173  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3272 training Loss: 0.17839607223816753 validation Loss: 0.17526710079134455  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3273 training Loss: 0.17836289250975604 validation Loss: 0.17522549080595  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3274 training Loss: 0.17832975129926223 validation Loss: 0.17518394603465065  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3275 training Loss: 0.178296648513308 validation Loss: 0.17514246616392234  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3276 training Loss: 0.17826358405903248 validation Loss: 0.17510105088792055  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3277 training Loss: 0.1782305578440831 validation Loss: 0.1750596999080574  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3278 training Loss: 0.17819756977660803 validation Loss: 0.17501841293260922  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3279 training Loss: 0.17816461976524803 validation Loss: 0.17497718967635195  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3280 training Loss: 0.1781317077191294 validation Loss: 0.17493602986022025  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3281 training Loss: 0.17809883354785633 validation Loss: 0.17489493321099212  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3282 training Loss: 0.17806629268261195 validation Loss: 0.17486615846122802  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3283 training Loss: 0.17803375616677738 validation Loss: 0.17482407541311681  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3284 training Loss: 0.17800111508735722 validation Loss: 0.17479442894468863  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3285 training Loss: 0.17796885662476863 validation Loss: 0.17475152416672116  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3286 training Loss: 0.17793617188376923 validation Loss: 0.17470884295852557  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3287 training Loss: 0.17790410768570375 validation Loss: 0.17467867834177778  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3288 training Loss: 0.17787151908081383 validation Loss: 0.17463529044356205  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3289 training Loss: 0.17783944311013256 validation Loss: 0.17460450630429175  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3290 training Loss: 0.17780703371220755 validation Loss: 0.17456053378099098  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3291 training Loss: 0.1777749863950881 validation Loss: 0.17452924013259036  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3292 training Loss: 0.17774271212607215 validation Loss: 0.17448478645178606  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3293 training Loss: 0.17771073114295724 validation Loss: 0.17445307646551633  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3294 training Loss: 0.177678551419168 validation Loss: 0.17440822932009942  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3295 training Loss: 0.17764667197162698 validation Loss: 0.1743761818887945  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3296 training Loss: 0.17761454922140374 validation Loss: 0.17433101560064515  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3297 training Loss: 0.17758280427791018 validation Loss: 0.1742986975200685  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3298 training Loss: 0.17755070354431884 validation Loss: 0.1742532750834822  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3299 training Loss: 0.17751912406323636 validation Loss: 0.17422074290547987  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3300 training Loss: 0.17748705690261118 validation Loss: 0.17418755624231627  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3301 training Loss: 0.1774555903205834 validation Loss: 0.17414130780973258  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3302 training Loss: 0.1774236761676799 validation Loss: 0.17410806137105486  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3303 training Loss: 0.17739212997244436 validation Loss: 0.17406175551769873  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3304 training Loss: 0.17736047247209646 validation Loss: 0.17402847353801545  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3305 training Loss: 0.17732882082071255 validation Loss: 0.17398213341799998  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3306 training Loss: 0.1772974434603579 validation Loss: 0.17394883643791242  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3307 training Loss: 0.17726572399298224 validation Loss: 0.1739149717840132  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3308 training Loss: 0.17723452987336144 validation Loss: 0.17386807593454429  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3309 training Loss: 0.17720297482922814 validation Loss: 0.17383430681120343  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3310 training Loss: 0.17717159223806844 validation Loss: 0.17378750068212118  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3311 training Loss: 0.17714039331991804 validation Loss: 0.1737538275858606  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3312 training Loss: 0.1771089126530838 validation Loss: 0.17371963530657056  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3313 training Loss: 0.17707787176985354 validation Loss: 0.1736724246397944  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3314 training Loss: 0.17704659967309264 validation Loss: 0.17363841451781103  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3315 training Loss: 0.17701529832993193 validation Loss: 0.1735913759201588  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3316 training Loss: 0.1769844488249569 validation Loss: 0.17355753526088444  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3317 training Loss: 0.1769531969479475 validation Loss: 0.1735232076601838  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3318 training Loss: 0.17692213603708382 validation Loss: 0.1734758652276192  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3319 training Loss: 0.17689130552569035 validation Loss: 0.17344177708686426  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3320 training Loss: 0.17686019275827033 validation Loss: 0.17340723255130985  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3321 training Loss: 0.1768293024819217 validation Loss: 0.17335968152916545  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3322 training Loss: 0.176798554902935 validation Loss: 0.17332543109198484  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3323 training Loss: 0.17676757767202048 validation Loss: 0.17329074816542842  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3324 training Loss: 0.1767367938088577 validation Loss: 0.17324306317850596  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3325 training Loss: 0.17670618828079987 validation Loss: 0.173208717011807  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3326 training Loss: 0.17667534345974198 validation Loss: 0.17317395708693337  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3327 training Loss: 0.17664460653226524 validation Loss: 0.173126196672736  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3328 training Loss: 0.17661419799737466 validation Loss: 0.17309180682757339  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3329 training Loss: 0.17658348280590067 validation Loss: 0.17305701792361536  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3330 training Loss: 0.1765528192308912 validation Loss: 0.1730218643418009  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3331 training Loss: 0.1765224965907269 validation Loss: 0.17297372599191946  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3332 training Loss: 0.17649200202296847 validation Loss: 0.1729390304723592  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3333 training Loss: 0.17646146367111964 validation Loss: 0.17290397429010512  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3334 training Loss: 0.1764310423093157 validation Loss: 0.1728559263107175  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3335 training Loss: 0.1764008803593428 validation Loss: 0.17282133497128982  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3336 training Loss: 0.17637046522256836 validation Loss: 0.1727863862054473  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3337 training Loss: 0.1763400990708915 validation Loss: 0.17275111113181474  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3338 training Loss: 0.17630991407409147 validation Loss: 0.17270285202166372  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3339 training Loss: 0.17627983088026478 validation Loss: 0.17266810318081832  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3340 training Loss: 0.17624958449079944 validation Loss: 0.17263302383605644  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3341 training Loss: 0.17621938562201547 validation Loss: 0.17259764299976077  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3342 training Loss: 0.17618933390630184 validation Loss: 0.1725492804797771  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3343 training Loss: 0.17615942563043682 validation Loss: 0.17251447005450643  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3344 training Loss: 0.1761293435644817 validation Loss: 0.17247934861654163  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3345 training Loss: 0.1760993077624879 validation Loss: 0.17244394365206223  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3346 training Loss: 0.17606931752956315 validation Loss: 0.1724082805045522  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3347 training Loss: 0.17603962681687582 validation Loss: 0.1723596456436466  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3348 training Loss: 0.17600973910304454 validation Loss: 0.172324630342362  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3349 training Loss: 0.17597986194287757 validation Loss: 0.17228933797882368  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3350 training Loss: 0.1759500293831505 validation Loss: 0.17225379338953725  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3351 training Loss: 0.1759202509542286 validation Loss: 0.17220526910759654  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3352 training Loss: 0.17589074574450386 validation Loss: 0.17217038249360925  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3353 training Loss: 0.17586102410922658 validation Loss: 0.1721352236136366  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3354 training Loss: 0.17583134619124044 validation Loss: 0.17209981692321988  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3355 training Loss: 0.17580171141642106 validation Loss: 0.17206418497291964  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3356 training Loss: 0.175772119259329 validation Loss: 0.17202834855652377  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3357 training Loss: 0.17574271653076062 validation Loss: 0.1719795432757797  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3358 training Loss: 0.1757132765227704 validation Loss: 0.17194445012664086  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3359 training Loss: 0.175683791592854 validation Loss: 0.17190912217800944  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3360 training Loss: 0.175654348651459 validation Loss: 0.17187358096408678  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3361 training Loss: 0.1756249472164813 validation Loss: 0.1718378463422873  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3362 training Loss: 0.17559558684427673 validation Loss: 0.17180193662387758  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3363 training Loss: 0.175566349487866 validation Loss: 0.17175305865034152  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3364 training Loss: 0.17553717319831147 validation Loss: 0.17171794429610632  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3365 training Loss: 0.1755079170164743 validation Loss: 0.17168261827941206  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3366 training Loss: 0.17547870142222396 validation Loss: 0.17164710032976163  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3367 training Loss: 0.17544952599331323 validation Loss: 0.17161140864117633  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3368 training Loss: 0.17542039034001847 validation Loss: 0.1715755599918478  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3369 training Loss: 0.17539129410094742 validation Loss: 0.17153956985460822  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3370 training Loss: 0.17536223693945951 validation Loss: 0.1715034524988909  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3371 training Loss: 0.17533336079111003 validation Loss: 0.17145437331726432  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3372 training Loss: 0.17530438283169322 validation Loss: 0.17141913654341204  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3373 training Loss: 0.17527542620521658 validation Loss: 0.17138372576640568  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3374 training Loss: 0.17524650835726302 validation Loss: 0.17134815778605922  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3375 training Loss: 0.17521762894630874 validation Loss: 0.17131244809551635  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3376 training Loss: 0.1751887876549935 validation Loss: 0.1712766109831497  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3377 training Loss: 0.17515998418706774 validation Loss: 0.17124065962660523  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3378 training Loss: 0.17513121826478323 validation Loss: 0.1712046061795733  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3379 training Loss: 0.17510248962665945 validation Loss: 0.17116846185183124  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3380 training Loss: 0.17507379802557027 validation Loss: 0.17113223698306476  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3381 training Loss: 0.17504523021804855 validation Loss: 0.17108305198605975  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3382 training Loss: 0.17501662159999576 validation Loss: 0.17104779043712687  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3383 training Loss: 0.17498802677149888 validation Loss: 0.17101239170505925  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3384 training Loss: 0.17495946880879992 validation Loss: 0.17097686972980344  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3385 training Loss: 0.1749309474435726 validation Loss: 0.1709412373677951  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3386 training Loss: 0.17490246242458088 validation Loss: 0.17090550647646652  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3387 training Loss: 0.17487401351556495 validation Loss: 0.17086968799221453  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3388 training Loss: 0.17484560049343192 validation Loss: 0.17083379200232054  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3389 training Loss: 0.17481722314670536 validation Loss: 0.1707978278112793  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3390 training Loss: 0.1747888812741948 validation Loss: 0.17076180400196175  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3391 training Loss: 0.17476057468385237 validation Loss: 0.17072572849200596  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3392 training Loss: 0.17473230319178848 validation Loss: 0.17068960858580207  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3393 training Loss: 0.17470406662142327 validation Loss: 0.17065345102240984  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3394 training Loss: 0.17467586480275274 validation Loss: 0.1706172620197233  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3395 training Loss: 0.1746476975717141 validation Loss: 0.1705810473151721  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3396 training Loss: 0.17461956476963436 validation Loss: 0.17054481220323026  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3397 training Loss: 0.17459146624275076 validation Loss: 0.17050856156997757  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3398 training Loss: 0.17456340184179225 validation Loss: 0.17047229992494736  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3399 training Loss: 0.1745353714216134 validation Loss: 0.17043603143046976  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3400 training Loss: 0.17450737484087323 validation Loss: 0.17039975992870712  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3401 training Loss: 0.1744794119617521 validation Loss: 0.170363488966564  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3402 training Loss: 0.174451482649702 validation Loss: 0.17032722181863624  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3403 training Loss: 0.1744235867732249 validation Loss: 0.17029096150835504  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3404 training Loss: 0.17439572420367563 validation Loss: 0.1702547108274677  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3405 training Loss: 0.17436789481508583 validation Loss: 0.17021847235398488  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3406 training Loss: 0.17434009848400625 validation Loss: 0.170182248468717  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3407 training Loss: 0.17431233508936453 validation Loss: 0.17014604137050962  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3408 training Loss: 0.17428460451233702 validation Loss: 0.17010985309028157  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3409 training Loss: 0.17425690663623233 validation Loss: 0.17007368550395943  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3410 training Loss: 0.17422924134638526 validation Loss: 0.170037540344396  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3411 training Loss: 0.17420160853006023 validation Loss: 0.1700014192123532  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3412 training Loss: 0.17417400807636207 validation Loss: 0.16996532358662306  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3413 training Loss: 0.17414643987615455 validation Loss: 0.16992925483335503  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3414 training Loss: 0.1741189038219845 validation Loss: 0.1698932142146523  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3415 training Loss: 0.174091399808012 validation Loss: 0.16985720289649506  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3416 training Loss: 0.1740639277299452 validation Loss: 0.16982122195604393  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3417 training Loss: 0.17403648748497944 validation Loss: 0.16978527238837196  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3418 training Loss: 0.17400907897174056 validation Loss: 0.16974935511267186  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3419 training Loss: 0.17398170209023184 validation Loss: 0.1697134709779779  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3420 training Loss: 0.1739543567417834 validation Loss: 0.16967762076844242  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3421 training Loss: 0.17392704282900553 validation Loss: 0.16964180520820177  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3422 training Loss: 0.17389976025574397 validation Loss: 0.16960602496586333  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3423 training Loss: 0.1738725089270377 validation Loss: 0.1695702806586445  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3424 training Loss: 0.1738452887490793 validation Loss: 0.1695345961703671  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3425 training Loss: 0.1738180996291768 validation Loss: 0.16949902571718078  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3426 training Loss: 0.17379094147571778 validation Loss: 0.16946349242836584  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3427 training Loss: 0.17376381419813505 validation Loss: 0.1694279967515929  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3428 training Loss: 0.17373671770687424 validation Loss: 0.1693925390995365  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3429 training Loss: 0.17370965191336227 validation Loss: 0.16935711985242682  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3430 training Loss: 0.17368261672997792 validation Loss: 0.16932173936040698  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3431 training Loss: 0.17365561207002322 validation Loss: 0.169286397945708  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3432 training Loss: 0.17362863784769642 validation Loss: 0.16925109590465892  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3433 training Loss: 0.17360169397806566 validation Loss: 0.16921583350954217  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3434 training Loss: 0.17357478037704424 validation Loss: 0.16918061101030885  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3435 training Loss: 0.1735478969613668 validation Loss: 0.16914542863616272  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3436 training Loss: 0.17352104364856616 validation Loss: 0.16911028659702476  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3437 training Loss: 0.17349422035695128 validation Loss: 0.16907518508488648  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3438 training Loss: 0.17346742700558643 validation Loss: 0.16904012427506218  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3439 training Loss: 0.17344066351427065 validation Loss: 0.16900510432734564  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3440 training Loss: 0.1734139298035182 validation Loss: 0.1689701253870814  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3441 training Loss: 0.1733872257945399 validation Loss: 0.1689351875861554  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3442 training Loss: 0.17336055140922527 validation Loss: 0.16890029104391197  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3443 training Loss: 0.17333390657012476 validation Loss: 0.1688654358680026  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3444 training Loss: 0.1733072912004334 validation Loss: 0.1688306221551722  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3445 training Loss: 0.17328070522397473 validation Loss: 0.16879584999198707  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3446 training Loss: 0.17325414856518506 validation Loss: 0.16876111945551012  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3447 training Loss: 0.17322762114909887 validation Loss: 0.16872643061392575  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3448 training Loss: 0.17320114834089018 validation Loss: 0.16869229141823164  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3449 training Loss: 0.17317472946277623 validation Loss: 0.16865816264670652  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3450 training Loss: 0.1731483393515435 validation Loss: 0.1686240466110055  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3451 training Loss: 0.17312197793317236 validation Loss: 0.16858994544327865  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3452 training Loss: 0.1730956451345561 validation Loss: 0.1685558611100008  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3453 training Loss: 0.17306934088343312 validation Loss: 0.1685217954247247  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3454 training Loss: 0.17304306510832773 validation Loss: 0.16848775005984223  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3455 training Loss: 0.17301681773849825 validation Loss: 0.16845372655743054  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3456 training Loss: 0.17299059870389177 validation Loss: 0.16841972633925462  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3457 training Loss: 0.1729644079351041 validation Loss: 0.1683857507159917  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3458 training Loss: 0.17293824536334423 validation Loss: 0.16835180089573942  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3459 training Loss: 0.17291211092040326 validation Loss: 0.16831787799186212  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3460 training Loss: 0.17288600453862638 validation Loss: 0.16828398303022798  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3461 training Loss: 0.17285992615088802 validation Loss: 0.1682501169558849  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3462 training Loss: 0.1728338756905696 validation Loss: 0.16821628063921712  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3463 training Loss: 0.17280785309153937 validation Loss: 0.16818247488162602  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3464 training Loss: 0.17278185828813453 validation Loss: 0.16814870042076932  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3465 training Loss: 0.17275589121514448 validation Loss: 0.16811495793539535  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3466 training Loss: 0.17272995180779616 validation Loss: 0.1680812480498025  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3467 training Loss: 0.17270404332981465 validation Loss: 0.16804779883215928  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3468 training Loss: 0.1726781657788383 validation Loss: 0.16801436674665837  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3469 training Loss: 0.1726523156879182 validation Loss: 0.16798095362671694  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3470 training Loss: 0.17262649299284952 validation Loss: 0.16794756115898152  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3471 training Loss: 0.17260069763001623 validation Loss: 0.16791419089470522  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3472 training Loss: 0.17257492953634795 validation Loss: 0.16788084426023872  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3473 training Loss: 0.1725491886492828 validation Loss: 0.16784752256670482  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3474 training Loss: 0.17252347490673506 validation Loss: 0.16781422701892104  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3475 training Loss: 0.1724977882470671 validation Loss: 0.16778095872362728  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3476 training Loss: 0.17247212860906494 validation Loss: 0.1677477186970733  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3477 training Loss: 0.17244649593191672 validation Loss: 0.16771450787201686  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3478 training Loss: 0.17242089015519418 validation Loss: 0.16768132710417688  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3479 training Loss: 0.17239531121883603 validation Loss: 0.16764817717818464  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3480 training Loss: 0.17236975906313334 validation Loss: 0.16761505881307354  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3481 training Loss: 0.17234423362871681 validation Loss: 0.16758197266734048  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3482 training Loss: 0.17231873485654522 validation Loss: 0.16754891934361596  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3483 training Loss: 0.17229326268789508 validation Loss: 0.16751589939296935  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3484 training Loss: 0.17226781706435174 validation Loss: 0.16748291331888204  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3485 training Loss: 0.1722423979278009 validation Loss: 0.16744996158091033  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3486 training Loss: 0.17221700522042138 validation Loss: 0.1674170445980658  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3487 training Loss: 0.17219163888467806 validation Loss: 0.16738416275193205  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3488 training Loss: 0.17216629886331594 validation Loss: 0.16735131638954096  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3489 training Loss: 0.17214098509935433 validation Loss: 0.16731850582602548  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3490 training Loss: 0.17211569753608183 validation Loss: 0.1672857313470676  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3491 training Loss: 0.17209044816311855 validation Loss: 0.16725329726246438  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3492 training Loss: 0.17206523934952403 validation Loss: 0.16722088100077936  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3493 training Loss: 0.17204005643187217 validation Loss: 0.16718848411318446  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3494 training Loss: 0.1720148993539341 validation Loss: 0.1671561080282795  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3495 training Loss: 0.17198976805990673 validation Loss: 0.16712375406150345  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3496 training Loss: 0.17196466249438372 validation Loss: 0.1670914234238191  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3497 training Loss: 0.17193958260232936 validation Loss: 0.16705911722972497  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3498 training Loss: 0.17191452832905696 validation Loss: 0.1670268365046496  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3499 training Loss: 0.17188949962020952 validation Loss: 0.16699458219177318  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3500 training Loss: 0.17186449642174292 validation Loss: 0.1669623551583233  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3501 training Loss: 0.17183951867991165 validation Loss: 0.16693015620138324  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3502 training Loss: 0.17181456634125572 validation Loss: 0.16689798605325332  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3503 training Loss: 0.17178963935259012 validation Loss: 0.16686584538639734  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3504 training Loss: 0.17176473766099434 validation Loss: 0.16683373481800817  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3505 training Loss: 0.17173986121380416 validation Loss: 0.16680165491422042  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3506 training Loss: 0.17171500995860398 validation Loss: 0.16676960619399925  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3507 training Loss: 0.17169018384321966 validation Loss: 0.16673758913272926  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3508 training Loss: 0.17166538281571292 validation Loss: 0.16670560416552685  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3509 training Loss: 0.17164060682437562 validation Loss: 0.16667365169029755  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3510 training Loss: 0.17161585581772484 validation Loss: 0.16664173207055877  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3511 training Loss: 0.17159112974449864 validation Loss: 0.1666098456380453  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3512 training Loss: 0.17156642855365178 validation Loss: 0.16657799269511397  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3513 training Loss: 0.17154175219435233 validation Loss: 0.16654617351696482  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3514 training Loss: 0.17151710061597808 validation Loss: 0.166514388353691  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3515 training Loss: 0.17149247376811347 validation Loss: 0.16648263743217218  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3516 training Loss: 0.17146787160054683 validation Loss: 0.16645092095782213  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3517 training Loss: 0.17144329406326766 validation Loss: 0.16641923911620343  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3518 training Loss: 0.1714187411064641 validation Loss: 0.16638759207451825  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3519 training Loss: 0.17139421268052074 validation Loss: 0.16635597998298493  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3520 training Loss: 0.17136970873601626 validation Loss: 0.16632440297611017  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3521 training Loss: 0.17134522922372136 validation Loss: 0.1662928611738638  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3522 training Loss: 0.171320774094597 validation Loss: 0.16626135468276398  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3523 training Loss: 0.17129634329979232 validation Loss: 0.16622988359688018  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3524 training Loss: 0.17127193679064298 validation Loss: 0.16619844799875955  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3525 training Loss: 0.1712475545186694 validation Loss: 0.16616704796028314  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3526 training Loss: 0.1712231964355752 validation Loss: 0.1661356835434574  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3527 training Loss: 0.1711985844433318 validation Loss: 0.16608776332437145  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3528 training Loss: 0.1711701708637941 validation Loss: 0.16604112226967468  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3529 training Loss: 0.17114188727175897 validation Loss: 0.16599565108323736  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3530 training Loss: 0.17111372500324779 validation Loss: 0.16595124969303512  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3531 training Loss: 0.17108567639746045 validation Loss: 0.16590782644934238  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3532 training Loss: 0.17105773466202773 validation Loss: 0.16586529739827952  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3533 training Loss: 0.1710298937574933 validation Loss: 0.165823585622807  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3534 training Loss: 0.1710021482982107 validation Loss: 0.16578262064418942  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3535 training Loss: 0.1709744934672583 validation Loss: 0.16574233787777182  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3536 training Loss: 0.17094692494332958 validation Loss: 0.1657026781376097  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3537 training Loss: 0.17091943883785832 validation Loss: 0.1656635871851187  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3538 training Loss: 0.17089203164089428 validation Loss: 0.1656250153174447  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3539 training Loss: 0.17086470017446453 validation Loss: 0.16558691699172592  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3540 training Loss: 0.17083744155233993 validation Loss: 0.165549250481836  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3541 training Loss: 0.1708102531452874 validation Loss: 0.16551197756455724  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3542 training Loss: 0.17078313255101965 validation Loss: 0.1654750632324547  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3543 training Loss: 0.17075607756817382 validation Loss: 0.16543847543100404  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3544 training Loss: 0.17072908617374413 validation Loss: 0.16540218481777455  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3545 training Loss: 0.17070215650347959 validation Loss: 0.16536616454169117  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3546 training Loss: 0.17067528683482885 validation Loss: 0.16533039004059336  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3547 training Loss: 0.17064847557207433 validation Loss: 0.16529483885548374  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3548 training Loss: 0.1706217212333497 validation Loss: 0.16525949046001456  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3549 training Loss: 0.17059502243927957 validation Loss: 0.16522432610389723  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3550 training Loss: 0.17056837790301713 validation Loss: 0.16518932866904515  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3551 training Loss: 0.17054178642148912 validation Loss: 0.16515448253736803  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3552 training Loss: 0.17051524686768235 validation Loss: 0.16511977346923826  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3553 training Loss: 0.17048875818383274 validation Loss: 0.16508518849173595  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3554 training Loss: 0.17046231937539516 validation Loss: 0.1650507157958603  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3555 training Loss: 0.17043592950569134 validation Loss: 0.16501634464196807  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3556 training Loss: 0.1704095876911464 validation Loss: 0.16498206527276277  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3557 training Loss: 0.17038329309703784 validation Loss: 0.16494786883321927  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3558 training Loss: 0.17035705069852328 validation Loss: 0.1649138427043858  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3559 training Loss: 0.1703308672133063 validation Loss: 0.1648798771577224  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3560 training Loss: 0.1703047284025892 validation Loss: 0.16484596621157963  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3561 training Loss: 0.1702786336073291 validation Loss: 0.16481210447858358  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3562 training Loss: 0.17025258220051123 validation Loss: 0.16477828711632847  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3563 training Loss: 0.17022657358501395 validation Loss: 0.1647445097819732  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3564 training Loss: 0.17020060719165997 validation Loss: 0.16471076859043315  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3565 training Loss: 0.170174682477434 validation Loss: 0.16467706007588254  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3566 training Loss: 0.17014879892384702 validation Loss: 0.16464338115630697  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3567 training Loss: 0.170122956035433 validation Loss: 0.1646097291008657  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3568 training Loss: 0.17009715333836326 validation Loss: 0.16457610149984317  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3569 training Loss: 0.17007139458004072 validation Loss: 0.16454260812531007  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3570 training Loss: 0.17004569225299876 validation Loss: 0.16450912693852293  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3571 training Loss: 0.17002002853920156 validation Loss: 0.16447565702658434  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3572 training Loss: 0.16999440305012664 validation Loss: 0.16444219764499401  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3573 training Loss: 0.16996881541261155 validation Loss: 0.16440874820204182  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3574 training Loss: 0.16994326526804016 validation Loss: 0.16437530824446053  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3575 training Loss: 0.16991775227157924 validation Loss: 0.1643418774442403  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3576 training Loss: 0.16989227609146132 validation Loss: 0.1643084555865148  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3577 training Loss: 0.1698668364083105 validation Loss: 0.1642750425584355  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3578 training Loss: 0.1698414329145081 validation Loss: 0.1642416383389555  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3579 training Loss: 0.16981606531359525 validation Loss: 0.16420824298945483  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3580 training Loss: 0.16979073331970973 validation Loss: 0.16417485664513848  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3581 training Loss: 0.16976543665705496 validation Loss: 0.16414147950714916  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3582 training Loss: 0.1697401750593989 validation Loss: 0.16410811183533755  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3583 training Loss: 0.16971494826960065 validation Loss: 0.16407475394163862  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3584 training Loss: 0.1696897560391634 validation Loss: 0.1640414061840076  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3585 training Loss: 0.16966459812781176 validation Loss: 0.16400806896086967  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3586 training Loss: 0.16963947430309223 validation Loss: 0.16397474270604542  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3587 training Loss: 0.16961438433999465 validation Loss: 0.1639414278841125  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3588 training Loss: 0.16958932802059457 validation Loss: 0.16390812498617027  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3589 training Loss: 0.1695643051337143 validation Loss: 0.16387483452597454  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3590 training Loss: 0.1695393154746018 validation Loss: 0.16384155703641498  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3591 training Loss: 0.16951435884462668 validation Loss: 0.16380829306630545  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3592 training Loss: 0.169489435050992 validation Loss: 0.16377504317746408  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3593 training Loss: 0.16946454390646085 validation Loss: 0.16374180794205997  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3594 training Loss: 0.16943968522909758 validation Loss: 0.16370858794020343  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3595 training Loss: 0.16941485884202181 validation Loss: 0.1636753837577631  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3596 training Loss: 0.16939006457317546 validation Loss: 0.16364219598438864  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3597 training Loss: 0.16936530225510146 validation Loss: 0.16360902521172424  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3598 training Loss: 0.16934057172473405 validation Loss: 0.1635758720317971  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3599 training Loss: 0.16931587282319915 validation Loss: 0.1635427370355661  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3600 training Loss: 0.1692912053956256 validation Loss: 0.16350962081161863  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3601 training Loss: 0.16926656929096523 validation Loss: 0.16347652394500167  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3602 training Loss: 0.16924196436182223 validation Loss: 0.16344344701617794  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3603 training Loss: 0.16921739046429143 validation Loss: 0.16341039060009552  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3604 training Loss: 0.16919284745780394 validation Loss: 0.1633773552653615  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3605 training Loss: 0.16916833520498104 validation Loss: 0.16334434157351205  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3606 training Loss: 0.16914385357149514 validation Loss: 0.16331135007836858  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3607 training Loss: 0.16911940242593765 validation Loss: 0.16327838132547556  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3608 training Loss: 0.1690949816396933 validation Loss: 0.16324543585161028  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3609 training Loss: 0.16907059145092784 validation Loss: 0.16321260763573384  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3610 training Loss: 0.16904623743924715 validation Loss: 0.16317979701146373  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3611 training Loss: 0.16902191337988198 validation Loss: 0.16314700503096205  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3612 training Loss: 0.1689976191550444 validation Loss: 0.1631142326938086  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3613 training Loss: 0.16897335464916519 validation Loss: 0.1630814809500597  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3614 training Loss: 0.16894911974880156 validation Loss: 0.16304875070309865  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3615 training Loss: 0.1689249143425493 validation Loss: 0.16301604281229334  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3616 training Loss: 0.16890073832095945 validation Loss: 0.16298335809547476  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3617 training Loss: 0.16887659157645904 validation Loss: 0.16295069733125064  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3618 training Loss: 0.16885247400327616 validation Loss: 0.162918061261164  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3619 training Loss: 0.16882838549736798 validation Loss: 0.16288545059171053  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3620 training Loss: 0.16880432595635297 validation Loss: 0.16285286599622276  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3621 training Loss: 0.16878029527944616 validation Loss: 0.1628203081166322  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3622 training Loss: 0.1687562933673976 validation Loss: 0.16278777756511714  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3623 training Loss: 0.16873232012243344 validation Loss: 0.16275527492564473  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3624 training Loss: 0.16870837544820058 validation Loss: 0.1627228007554159  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3625 training Loss: 0.1686844592497132 validation Loss: 0.16269035558621767  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3626 training Loss: 0.1686605714333022 validation Loss: 0.16265793992569177  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3627 training Loss: 0.1686367119065672 validation Loss: 0.16262555425852465  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3628 training Loss: 0.16861288057833018 validation Loss: 0.16259319904756372  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3629 training Loss: 0.16858907735859202 validation Loss: 0.16256087473486638  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3630 training Loss: 0.1685653021584907 validation Loss: 0.1625285817426849  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3631 training Loss: 0.1685415548902613 validation Loss: 0.16249632047439339  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3632 training Loss: 0.16851783546719823 validation Loss: 0.16246409131535947  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3633 training Loss: 0.16849414380361893 validation Loss: 0.16243189463376514  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3634 training Loss: 0.16847047981482935 validation Loss: 0.1623997307813797  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3635 training Loss: 0.1684468434170908 validation Loss: 0.16236760009428952  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3636 training Loss: 0.1684232345275887 validation Loss: 0.1623355028935852  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3637 training Loss: 0.16839965306440244 validation Loss: 0.16230343948601117  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3638 training Loss: 0.16837609894647662 validation Loss: 0.16227141016457902  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3639 training Loss: 0.1683525720935937 validation Loss: 0.1622394152091475  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3640 training Loss: 0.16832907242634781 validation Loss: 0.162207454886971  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3641 training Loss: 0.1683055998661198 validation Loss: 0.1621755294532191  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3642 training Loss: 0.16828215433505306 validation Loss: 0.16214363915146823  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3643 training Loss: 0.16825873575603112 validation Loss: 0.16211178421416855  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3644 training Loss: 0.1682353440526553 validation Loss: 0.1620799648630863  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3645 training Loss: 0.16821197914922426 validation Loss: 0.16204818130972362  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3646 training Loss: 0.16818864097071362 validation Loss: 0.16201643375571811  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3647 training Loss: 0.16816532944275714 validation Loss: 0.16198472239322237  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3648 training Loss: 0.16814204449162837 validation Loss: 0.16195304740526426  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3649 training Loss: 0.16811878604422295 validation Loss: 0.16192140896609114  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3650 training Loss: 0.16809555402804202 validation Loss: 0.1618898072414972  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3651 training Loss: 0.16807234837117613 validation Loss: 0.16185824238913576  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3652 training Loss: 0.1680491690022899 validation Loss: 0.16182671455881617  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3653 training Loss: 0.1680260158506072 validation Loss: 0.16179522389278905  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3654 training Loss: 0.16800288884589712 validation Loss: 0.16176377052601712  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3655 training Loss: 0.16797978791846033 validation Loss: 0.1617323545864348  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3656 training Loss: 0.1679567129991163 validation Loss: 0.16170097619519647  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3657 training Loss: 0.16793366401919074 validation Loss: 0.16166963546691385  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3658 training Loss: 0.16791064091050356 validation Loss: 0.1616383325098834  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3659 training Loss: 0.16788764360535777 validation Loss: 0.16160706742630435  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3660 training Loss: 0.16786467203652805 validation Loss: 0.16157584031248723  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3661 training Loss: 0.1678417261372506 validation Loss: 0.1615446512590546  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3662 training Loss: 0.16781880584121275 validation Loss: 0.1615135003511333  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3663 training Loss: 0.1677959110825435 validation Loss: 0.16148238766853884  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3664 training Loss: 0.1677730417958038 validation Loss: 0.1614513132859531  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3665 training Loss: 0.167750197915978 validation Loss: 0.16142027727309477  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3666 training Loss: 0.1677273793784649 validation Loss: 0.16138927969488354  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3667 training Loss: 0.16770458611906972 validation Loss: 0.16135832061159752  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3668 training Loss: 0.16768181807399585 validation Loss: 0.16132740007902535  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3669 training Loss: 0.16765907517983727 validation Loss: 0.16129651814861282  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3670 training Loss: 0.1676363573735714 validation Loss: 0.1612656748676034  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3671 training Loss: 0.16761366459255156 validation Loss: 0.16123487027917457  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3672 training Loss: 0.16759099677450057 validation Loss: 0.16120410442256816  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3673 training Loss: 0.16756835385750374 validation Loss: 0.16117337733321777  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3674 training Loss: 0.16754573578000284 validation Loss: 0.1611426890428695  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3675 training Loss: 0.16752314248078978 validation Loss: 0.16111203957970108  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3676 training Loss: 0.16750057389900083 validation Loss: 0.16108142896843447  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3677 training Loss: 0.16747802997411085 validation Loss: 0.16105085723044635  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3678 training Loss: 0.16745551064592779 validation Loss: 0.16102032438387404  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3679 training Loss: 0.16743301585458736 validation Loss: 0.16098983044371815  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3680 training Loss: 0.16741054554054813 validation Loss: 0.16095937542194202  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3681 training Loss: 0.1673880996445862 validation Loss: 0.16092895932756732  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3682 training Loss: 0.1673656781077908 validation Loss: 0.16089858216676742  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3683 training Loss: 0.1673432808715594 validation Loss: 0.16086824394295646  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3684 training Loss: 0.16732090787759343 validation Loss: 0.16083794465687745  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3685 training Loss: 0.16729855906789384 validation Loss: 0.16080768430668566  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3686 training Loss: 0.16727623438475703 validation Loss: 0.1607774628880304  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3687 training Loss: 0.16725393377077072 validation Loss: 0.16074728039413397  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3688 training Loss: 0.16723165716881008 validation Loss: 0.16071713681586838  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3689 training Loss: 0.1672094045220339 validation Loss: 0.16068703214182942  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3690 training Loss: 0.16718717577388106 validation Loss: 0.16065696635840854  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3691 training Loss: 0.16716497086806673 validation Loss: 0.1606269394498626  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3692 training Loss: 0.1671427897485791 validation Loss: 0.16059695139838137  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3693 training Loss: 0.1671206323596761 validation Loss: 0.16056700218415298  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3694 training Loss: 0.16709849864588194 validation Loss: 0.1605370917854278  valid acc: 0.9066666666666666  train Acc: 0.8742857142857143\n",
      "epoch: 3695 training Loss: 0.16707638855198406 validation Loss: 0.1605072201785796  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3696 training Loss: 0.16705405124538597 validation Loss: 0.1604668344772657  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3697 training Loss: 0.16702704447816147 validation Loss: 0.16042741386781148  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3698 training Loss: 0.16700015315037503 validation Loss: 0.16038887511475092  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3699 training Loss: 0.16697337083914687 validation Loss: 0.16035114190316743  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3700 training Loss: 0.16694669181798946 validation Loss: 0.16031414425467513  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3701 training Loss: 0.16692011096659165 validation Loss: 0.16027781799621896  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3702 training Loss: 0.16689362369313143 validation Loss: 0.16024210427638305  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3703 training Loss: 0.16686722586732794 validation Loss: 0.16020694912449684  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3704 training Loss: 0.16684091376270235 validation Loss: 0.16017230304835658  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3705 training Loss: 0.16681468400673924 validation Loss: 0.1601381206668391  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3706 training Loss: 0.16678853353782735 validation Loss: 0.16010436037408698  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3707 training Loss: 0.16676245956802124 validation Loss: 0.16007098403230388  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3708 training Loss: 0.16673645955080238 validation Loss: 0.1600379566905049  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3709 training Loss: 0.1667105311531366 validation Loss: 0.16000524632684662  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3710 training Loss: 0.16668467223122535 validation Loss: 0.15997282361240348  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3711 training Loss: 0.16665889628499206 validation Loss: 0.15994075833537658  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3712 training Loss: 0.166633201930399 validation Loss: 0.15990892167572804  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3713 training Loss: 0.16660757097985432 validation Loss: 0.15987729178906562  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3714 training Loss: 0.16658200189561448 validation Loss: 0.15984584870707816  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3715 training Loss: 0.166556493244148 validation Loss: 0.15981457418735426  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3716 training Loss: 0.16653104368588423 validation Loss: 0.15978345157526028  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3717 training Loss: 0.16650565196620928 validation Loss: 0.15975246567686513  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3718 training Loss: 0.1664803169075405 validation Loss: 0.15972160264198854  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3719 training Loss: 0.16645503740233533 validation Loss: 0.15969084985653542  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3720 training Loss: 0.16642981240691007 validation Loss: 0.15966019584335225  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3721 training Loss: 0.16640464093596125 validation Loss: 0.15962963017090928  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3722 training Loss: 0.16637952205769907 validation Loss: 0.15959914336917388  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3723 training Loss: 0.1663544548895122 validation Loss: 0.15956872685209395  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3724 training Loss: 0.16632943859409657 validation Loss: 0.15953837284616124  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3725 training Loss: 0.1663044781508995 validation Loss: 0.15950815608145757  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3726 training Loss: 0.16627958549780614 validation Loss: 0.1594781713434047  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3727 training Loss: 0.1662547544852157 validation Loss: 0.1594482130913039  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3728 training Loss: 0.16622997111050142 validation Loss: 0.15941827743587297  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3729 training Loss: 0.16620523472761 validation Loss: 0.159388385885202  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3730 training Loss: 0.16618054471809277 validation Loss: 0.15935856131484055  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3731 training Loss: 0.1661559004894272 validation Loss: 0.1593287496013969  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3732 training Loss: 0.166131301639531 validation Loss: 0.15929926141280282  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3733 training Loss: 0.16610677033916807 validation Loss: 0.15926976341891858  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3734 training Loss: 0.16608228326059335 validation Loss: 0.1592402551056621  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3735 training Loss: 0.16605784670181162 validation Loss: 0.15921085761419818  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3736 training Loss: 0.16603348259583953 validation Loss: 0.15918143953143638  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3737 training Loss: 0.16600916081412648 validation Loss: 0.1591520016084562  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3738 training Loss: 0.16598488091421 validation Loss: 0.15912254465479264  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3739 training Loss: 0.16596064246931577 validation Loss: 0.15909306953098182  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3740 training Loss: 0.1659364450676055 validation Loss: 0.159063577141735  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3741 training Loss: 0.16591228831146404 validation Loss: 0.15903406842969361  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3742 training Loss: 0.16588817181682503 validation Loss: 0.15900454436971975  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3743 training Loss: 0.16586409521253112 validation Loss: 0.15897500596368055  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3744 training Loss: 0.16584005813972794 validation Loss: 0.15894545423568915  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3745 training Loss: 0.16581606025128925 validation Loss: 0.158915890227766  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3746 training Loss: 0.16579210121127166 validation Loss: 0.158886314995888  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3747 training Loss: 0.16576818069439736 validation Loss: 0.1588598003329438  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3748 training Loss: 0.1657442983855632 validation Loss: 0.15883417545871972  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3749 training Loss: 0.16572045397937452 validation Loss: 0.15880855450952694  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3750 training Loss: 0.16569664717970267 validation Loss: 0.1587829381862099  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3751 training Loss: 0.16567287769926464 validation Loss: 0.1587573271985267  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3752 training Loss: 0.1656491452592239 validation Loss: 0.15873172226261956  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3753 training Loss: 0.1656254495888109 validation Loss: 0.15870612409872062  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3754 training Loss: 0.1656017904249628 validation Loss: 0.15868053342907473  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3755 training Loss: 0.16557816751198065 validation Loss: 0.15865495097606208  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3756 training Loss: 0.16555458060120384 validation Loss: 0.15862937746050426  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3757 training Loss: 0.16553102945070022 validation Loss: 0.15860381360013945  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3758 training Loss: 0.16550751382497184 validation Loss: 0.1585782601082537  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3759 training Loss: 0.16548403349467464 validation Loss: 0.15855271769245416  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3760 training Loss: 0.16546058823635215 validation Loss: 0.15852718705357494  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3761 training Loss: 0.16543717783218218 validation Loss: 0.1585016688847028  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3762 training Loss: 0.16541380206973522 validation Loss: 0.15847616387031413  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3763 training Loss: 0.16539046074174543 validation Loss: 0.15845067268551305  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3764 training Loss: 0.16536715364589177 validation Loss: 0.15842519599536356  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3765 training Loss: 0.16534388058459046 validation Loss: 0.1583997344543063  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3766 training Loss: 0.16532064136479663 validation Loss: 0.15837428870565398  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3767 training Loss: 0.16529743579781625 validation Loss: 0.15834885938115836  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3768 training Loss: 0.1652742636991263 validation Loss: 0.1583234471006424  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3769 training Loss: 0.16525112488820398 validation Loss: 0.1582980524716924  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3770 training Loss: 0.16522801918836405 validation Loss: 0.1582726760894046  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3771 training Loss: 0.16520493838042913 validation Loss: 0.15824478377042453  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3772 training Loss: 0.1651815847790378 validation Loss: 0.15821708469557644  valid acc: 0.9066666666666666  train Acc: 0.8757142857142857\n",
      "epoch: 3773 training Loss: 0.1651582645762267 validation Loss: 0.158189566762256  valid acc: 0.9066666666666666  train Acc: 0.8771428571428571\n",
      "epoch: 3774 training Loss: 0.1651329046475355 validation Loss: 0.15815361485549503  valid acc: 0.9066666666666666  train Acc: 0.8771428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3775 training Loss: 0.16510312287696174 validation Loss: 0.15811882462814666  valid acc: 0.9066666666666666  train Acc: 0.8771428571428571\n",
      "epoch: 3776 training Loss: 0.16507349372486113 validation Loss: 0.15808509788620081  valid acc: 0.9066666666666666  train Acc: 0.8771428571428571\n",
      "epoch: 3777 training Loss: 0.1650440090543798 validation Loss: 0.15805234457495612  valid acc: 0.9066666666666666  train Acc: 0.8771428571428571\n",
      "epoch: 3778 training Loss: 0.1650146616092005 validation Loss: 0.15802048208817168  valid acc: 0.9066666666666666  train Acc: 0.8771428571428571\n",
      "epoch: 3779 training Loss: 0.16498544489901215 validation Loss: 0.1579894346404429  valid acc: 0.9066666666666666  train Acc: 0.8771428571428571\n",
      "epoch: 3780 training Loss: 0.16495635310090412 validation Loss: 0.15795913269634032  valid acc: 0.9066666666666666  train Acc: 0.8771428571428571\n",
      "epoch: 3781 training Loss: 0.16492738097441295 validation Loss: 0.15792951245059692  valid acc: 0.9066666666666666  train Acc: 0.8771428571428571\n",
      "epoch: 3782 training Loss: 0.16489852378827538 validation Loss: 0.15790051535427616  valid acc: 0.9066666666666666  train Acc: 0.8771428571428571\n",
      "epoch: 3783 training Loss: 0.16486977725722504 validation Loss: 0.15787208768242175  valid acc: 0.9066666666666666  train Acc: 0.8771428571428571\n",
      "epoch: 3784 training Loss: 0.16484113748740717 validation Loss: 0.15784418013918738  valid acc: 0.9066666666666666  train Acc: 0.8771428571428571\n",
      "epoch: 3785 training Loss: 0.16481260092919361 validation Loss: 0.15781674749687605  valid acc: 0.9066666666666666  train Acc: 0.8771428571428571\n",
      "epoch: 3786 training Loss: 0.16478416433635243 validation Loss: 0.15778974826570558  valid acc: 0.9066666666666666  train Acc: 0.8771428571428571\n",
      "epoch: 3787 training Loss: 0.16475582473067893 validation Loss: 0.15776314439144773  valid acc: 0.9066666666666666  train Acc: 0.8771428571428571\n",
      "epoch: 3788 training Loss: 0.16472757937132143 validation Loss: 0.15773690097839113  valid acc: 0.9066666666666666  train Acc: 0.8771428571428571\n",
      "epoch: 3789 training Loss: 0.16469942572814444 validation Loss: 0.15771098603533196  valid acc: 0.9066666666666666  train Acc: 0.8771428571428571\n",
      "epoch: 3790 training Loss: 0.16467136145856806 validation Loss: 0.1576853702425354  valid acc: 0.9066666666666666  train Acc: 0.8785714285714286\n",
      "epoch: 3791 training Loss: 0.16464338438739884 validation Loss: 0.15766002673781113  valid acc: 0.9066666666666666  train Acc: 0.8785714285714286\n",
      "epoch: 3792 training Loss: 0.1646154924892399 validation Loss: 0.15763493092002928  valid acc: 0.9066666666666666  train Acc: 0.8785714285714286\n",
      "epoch: 3793 training Loss: 0.16458768387312425 validation Loss: 0.1576100602685673  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3794 training Loss: 0.16455995676906657 validation Loss: 0.15758437620973334  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3795 training Loss: 0.16453230951627243 validation Loss: 0.1575584443958202  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3796 training Loss: 0.16450474055277928 validation Loss: 0.15753267959620415  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3797 training Loss: 0.16447724840633746 validation Loss: 0.1575070659077704  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3798 training Loss: 0.16444983168636376 validation Loss: 0.1574815888240417  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3799 training Loss: 0.16442248907682647 validation Loss: 0.15745623512349463  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3800 training Loss: 0.16439521932993809 validation Loss: 0.1574309927666878  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3801 training Loss: 0.1643680212605507 validation Loss: 0.15740585080148475  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3802 training Loss: 0.16434089374116248 validation Loss: 0.15738079927571946  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3803 training Loss: 0.16431383569745794 validation Loss: 0.15735582915670523  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3804 training Loss: 0.16428684754880438 validation Loss: 0.1573311612173389  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3805 training Loss: 0.16425993430766717 validation Loss: 0.15730654611668796  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3806 training Loss: 0.1642330876462189 validation Loss: 0.15728197800900537  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3807 training Loss: 0.16420630666316632 validation Loss: 0.1572574516429586  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3808 training Loss: 0.1641795904935874 validation Loss: 0.15723296231263142  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3809 training Loss: 0.1641529383066569 validation Loss: 0.15720850581234258  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3810 training Loss: 0.1641263493035653 validation Loss: 0.15718407839498438  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3811 training Loss: 0.16409982271561022 validation Loss: 0.15715967673360823  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3812 training Loss: 0.1640733578024415 validation Loss: 0.1571352978860061  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3813 training Loss: 0.1640469538504439 validation Loss: 0.15711093926205705  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3814 training Loss: 0.16402061017124298 validation Loss: 0.15708659859362495  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3815 training Loss: 0.1639943261003221 validation Loss: 0.15706227390681196  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3816 training Loss: 0.1639681009957395 validation Loss: 0.15703796349638643  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3817 training Loss: 0.16394193423693537 validation Loss: 0.15701366590221827  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3818 training Loss: 0.16391582522362164 validation Loss: 0.15698937988756798  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3819 training Loss: 0.16388977337474578 validation Loss: 0.15696510441908734  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3820 training Loss: 0.16386377812752276 validation Loss: 0.15694083864840055  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3821 training Loss: 0.16383783893652898 validation Loss: 0.15691658189514496  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3822 training Loss: 0.1638119552728532 validation Loss: 0.15689233363135982  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3823 training Loss: 0.16378612662329955 validation Loss: 0.15686809346711936  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3824 training Loss: 0.16376035248963883 validation Loss: 0.15684386113731535  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3825 training Loss: 0.16373463238790392 validation Loss: 0.1568196364895015  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3826 training Loss: 0.16370896584772618 validation Loss: 0.15679541947271705  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3827 training Loss: 0.16368335241171006 validation Loss: 0.1567712101272165  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3828 training Loss: 0.16365779163484281 validation Loss: 0.15674700857503387  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3829 training Loss: 0.16363228308393699 validation Loss: 0.15672281501131882  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3830 training Loss: 0.16360682633710333 validation Loss: 0.15669862969638487  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3831 training Loss: 0.1635814209832525 validation Loss: 0.1566744529484151  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3832 training Loss: 0.16355606662162314 validation Loss: 0.15665028513677418  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3833 training Loss: 0.163530762861335 validation Loss: 0.15662612667588058  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3834 training Loss: 0.16350550932096536 validation Loss: 0.15660197801959508  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3835 training Loss: 0.16348030562814736 validation Loss: 0.1565778396560866  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3836 training Loss: 0.16345515141918895 validation Loss: 0.156553712103137  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3837 training Loss: 0.1634300463387112 validation Loss: 0.15652959590385146  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3838 training Loss: 0.16340499003930484 validation Loss: 0.1565054916227432  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3839 training Loss: 0.16337998218120386 validation Loss: 0.15648139984216228  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3840 training Loss: 0.16335502243197544 validation Loss: 0.15645732115904293  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3841 training Loss: 0.1633301104662252 validation Loss: 0.15643325618194284  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3842 training Loss: 0.1633052459653165 validation Loss: 0.15640920552835277  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3843 training Loss: 0.1632804286171041 validation Loss: 0.1563851698222542  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3844 training Loss: 0.16325565811567969 validation Loss: 0.15636114969190593  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3845 training Loss: 0.16323093416113088 validation Loss: 0.1563371457678413  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3846 training Loss: 0.16320625645931053 validation Loss: 0.1563131586810589  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3847 training Loss: 0.16318162472161785 validation Loss: 0.15628918906139166  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3848 training Loss: 0.16315703866478942 validation Loss: 0.15626523753603971  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3849 training Loss: 0.16313249801070026 validation Loss: 0.15624130472825395  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3850 training Loss: 0.16310800248617427 validation Loss: 0.1562173912561578  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3851 training Loss: 0.16308355182280312 validation Loss: 0.156193497731696  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3852 training Loss: 0.163059145756774 validation Loss: 0.15616962475970048  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3853 training Loss: 0.16303478402870505 validation Loss: 0.15614577293706228  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3854 training Loss: 0.16301046638348846 validation Loss: 0.1561219428520025  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3855 training Loss: 0.16298619257014074 validation Loss: 0.15609813508343234  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3856 training Loss: 0.16296196234165958 validation Loss: 0.15607435020039606  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3857 training Loss: 0.16293777545488783 validation Loss: 0.1560505887615886  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3858 training Loss: 0.16291385294083677 validation Loss: 0.15601603744084094  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3859 training Loss: 0.16289102924929258 validation Loss: 0.1559823656965499  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3860 training Loss: 0.16286827944367574 validation Loss: 0.15594950374828984  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3861 training Loss: 0.16284559974676566 validation Loss: 0.15591738742362077  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3862 training Loss: 0.16282298680800425 validation Loss: 0.15588595769595684  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3863 training Loss: 0.1628004376462032 validation Loss: 0.1558551602630306  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3864 training Loss: 0.16277794960034164 validation Loss: 0.15582494516202006  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3865 training Loss: 0.162755520287292 validation Loss: 0.15579526641783492  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3866 training Loss: 0.16273314756548343 validation Loss: 0.15576608172143275  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3867 training Loss: 0.1627108295036515 validation Loss: 0.15573735213536602  valid acc: 0.9066666666666666  train Acc: 0.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3868 training Loss: 0.16268856435394838 validation Loss: 0.1557090418240568  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3869 training Loss: 0.16266635052878956 validation Loss: 0.15568111780654756  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3870 training Loss: 0.162644186580906 validation Loss: 0.15565354972970902  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3871 training Loss: 0.1626220711861439 validation Loss: 0.1556263096600857  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3872 training Loss: 0.1626000031286224 validation Loss: 0.1555993718927394  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3873 training Loss: 0.16257798128791393 validation Loss: 0.15557271277561044  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3874 training Loss: 0.1625560046279613 validation Loss: 0.15554631054805793  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3875 training Loss: 0.16253407218748517 validation Loss: 0.15552014519236804  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3876 training Loss: 0.1625121830716726 validation Loss: 0.15549419829713154  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3877 training Loss: 0.1624903364449652 validation Loss: 0.1554684529314936  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3878 training Loss: 0.16246853152479399 validation Loss: 0.15544289352936944  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3879 training Loss: 0.16244676757612694 validation Loss: 0.15541750578280208  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3880 training Loss: 0.16242504390671705 validation Loss: 0.15539227654371138  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3881 training Loss: 0.16240335986295285 validation Loss: 0.15536719373334884  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3882 training Loss: 0.16238171482622843 validation Loss: 0.15534224625883353  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3883 training Loss: 0.16236010820976107 validation Loss: 0.15531742393619966  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3884 training Loss: 0.16233853945579513 validation Loss: 0.155292717419431  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3885 training Loss: 0.16231700803313975 validation Loss: 0.1552681181350075  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3886 training Loss: 0.16229551343499485 validation Loss: 0.15524361822152422  valid acc: 0.9066666666666666  train Acc: 0.88\n",
      "epoch: 3887 training Loss: 0.16227405517702698 validation Loss: 0.15521921047398288  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3888 training Loss: 0.1622516268037821 validation Loss: 0.15518961897898625  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3889 training Loss: 0.16222357556961464 validation Loss: 0.155161034994923  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3890 training Loss: 0.16219570356127389 validation Loss: 0.1551334564583038  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3891 training Loss: 0.1621680137034548 validation Loss: 0.1551067020249101  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3892 training Loss: 0.16214048353628238 validation Loss: 0.1550806951245191  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3893 training Loss: 0.16211310541998175 validation Loss: 0.15505536560305497  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3894 training Loss: 0.16208587242227132 validation Loss: 0.1550306491880393  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3895 training Loss: 0.1620587782332914 validation Loss: 0.15500648700142397  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3896 training Loss: 0.16203181709197148 validation Loss: 0.15498282511513436  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3897 training Loss: 0.16200498372223107 validation Loss: 0.15495961414517528  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3898 training Loss: 0.16197827327763703 validation Loss: 0.15493680888060765  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3899 training Loss: 0.1619516812933389 validation Loss: 0.15491436794409993  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3900 training Loss: 0.16192520364426838 validation Loss: 0.15489225348111474  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3901 training Loss: 0.1618988365087357 validation Loss: 0.1548704308750974  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3902 training Loss: 0.16187257633667668 validation Loss: 0.15484886848630436  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3903 training Loss: 0.16184641982191159 validation Loss: 0.1548275374121536  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3904 training Loss: 0.16182036387786558 validation Loss: 0.1548064112671888  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3905 training Loss: 0.16179440561627972 validation Loss: 0.154785465980939  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3906 training Loss: 0.1617685423285068 validation Loss: 0.1547646796121242  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3907 training Loss: 0.16174277146904342 validation Loss: 0.154744032177804  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3908 training Loss: 0.16171709064099984 validation Loss: 0.15472350549620079  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3909 training Loss: 0.16169149758324838 validation Loss: 0.15470308304204777  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3910 training Loss: 0.16166599015903074 validation Loss: 0.15468274981341804  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3911 training Loss: 0.16164056634583143 validation Loss: 0.15466249220908368  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3912 training Loss: 0.1616152242263547 validation Loss: 0.15464229791554387  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3913 training Loss: 0.1615899619804623 validation Loss: 0.1546221558029333  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3914 training Loss: 0.16156477787795054 validation Loss: 0.15460205582909253  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3915 training Loss: 0.1615396702720609 validation Loss: 0.15458198895114644  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3916 training Loss: 0.16151463759363374 validation Loss: 0.15456194704398887  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3917 training Loss: 0.1614896783458258 validation Loss: 0.15454192282512774  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3918 training Loss: 0.16146479109932443 validation Loss: 0.15452190978538505  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3919 training Loss: 0.16143997448799866 validation Loss: 0.1545019021249946  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3920 training Loss: 0.16141522720493698 validation Loss: 0.1544818946946718  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3921 training Loss: 0.1613905479988272 validation Loss: 0.15446188294126986  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3922 training Loss: 0.1613659356706396 validation Loss: 0.15444186285766526  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3923 training Loss: 0.16134138907058077 validation Loss: 0.15442183093654452  valid acc: 0.9066666666666666  train Acc: 0.8814285714285715\n",
      "epoch: 3924 training Loss: 0.1613169070952882 validation Loss: 0.15440178412779093  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3925 training Loss: 0.16129118966341877 validation Loss: 0.15437763416035036  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3926 training Loss: 0.16125839869108988 validation Loss: 0.15435436447356662  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3927 training Loss: 0.16122585801263342 validation Loss: 0.15433193824420313  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3928 training Loss: 0.1611935832543304 validation Loss: 0.15431021791771757  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3929 training Loss: 0.16116152855994478 validation Loss: 0.15428913049119067  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3930 training Loss: 0.161129684549537 validation Loss: 0.15426860926004826  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3931 training Loss: 0.16109804263184255 validation Loss: 0.15424859329006074  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3932 training Loss: 0.16106659491453212 validation Loss: 0.15422902693611648  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3933 training Loss: 0.16103533412624174 validation Loss: 0.1542098594031645  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3934 training Loss: 0.16100425354873701 validation Loss: 0.15419104434523104  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3935 training Loss: 0.160973346957809 validation Loss: 0.15417253949887283  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3936 training Loss: 0.16094260857169604 validation Loss: 0.1541543063478152  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3937 training Loss: 0.16091203300599932 validation Loss: 0.15413630981587711  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3938 training Loss: 0.16088161523420305 validation Loss: 0.154118517985587  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3939 training Loss: 0.16085135055303826 validation Loss: 0.15410090184016176  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3940 training Loss: 0.1608212345520341 validation Loss: 0.1540834350267586  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3941 training Loss: 0.16079126308669495 validation Loss: 0.15406609363912077  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3942 training Loss: 0.16076143225481787 validation Loss: 0.1540488560179214  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3943 training Loss: 0.16073173837553498 validation Loss: 0.15403170256727697  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3944 training Loss: 0.1607021966677957 validation Loss: 0.1540147236678051  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3945 training Loss: 0.16067286019025095 validation Loss: 0.15399778150715804  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3946 training Loss: 0.16064365627866398 validation Loss: 0.153981195692329  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3947 training Loss: 0.16061459499349115 validation Loss: 0.15396460376962193  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3948 training Loss: 0.16058565364196795 validation Loss: 0.15394799639996173  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3949 training Loss: 0.16055682960215162 validation Loss: 0.1539313653080637  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3950 training Loss: 0.16052812037163797 validation Loss: 0.15391470319300665  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3951 training Loss: 0.16049952355985253 validation Loss: 0.15389800364583464  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3952 training Loss: 0.16047104000772847 validation Loss: 0.15387007056547977  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3953 training Loss: 0.16044398401488552 validation Loss: 0.1538541902423721  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3954 training Loss: 0.16041689595050868 validation Loss: 0.15382703401185074  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3955 training Loss: 0.16038996992744053 validation Loss: 0.15380069985626504  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3956 training Loss: 0.16036321428949382 validation Loss: 0.15377511131524263  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3957 training Loss: 0.16033662119917738 validation Loss: 0.1537501983561061  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3958 training Loss: 0.1603101835351886 validation Loss: 0.15372589683469384  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3959 training Loss: 0.16028389480492042 validation Loss: 0.1537021480044001  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3960 training Loss: 0.16025774906891527 validation Loss: 0.15367889806863266  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3961 training Loss: 0.1602317408755733 validation Loss: 0.15365609777242964  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3962 training Loss: 0.16020586520466526 validation Loss: 0.15363370202944798  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3963 training Loss: 0.16018011741840682 validation Loss: 0.1536116695809492  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3964 training Loss: 0.16015449321903139 validation Loss: 0.15358996268377562  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3965 training Loss: 0.1601289886119473 validation Loss: 0.1535685468246243  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3966 training Loss: 0.16010359987369965 validation Loss: 0.15354739045821145  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3967 training Loss: 0.16007832352406443 validation Loss: 0.1535264647671655  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3968 training Loss: 0.16005315630170222 validation Loss: 0.15350574344170675  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3969 training Loss: 0.16002809514287678 validation Loss: 0.15348520247736624  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3970 training Loss: 0.1600031371628174 validation Loss: 0.15346481998916925  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3971 training Loss: 0.15997827963936134 validation Loss: 0.15344457604085743  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3972 training Loss: 0.15995351999856536 validation Loss: 0.15342445248786676  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3973 training Loss: 0.15992885580201943 validation Loss: 0.15340443283289312  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3974 training Loss: 0.15990428473563298 validation Loss: 0.1533845020929895  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3975 training Loss: 0.1598798045996967 validation Loss: 0.15336464667723537  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3976 training Loss: 0.15985541330005065 validation Loss: 0.15334485427410485  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3977 training Loss: 0.15983110884021326 validation Loss: 0.15332511374773772  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3978 training Loss: 0.15980688931434556 validation Loss: 0.15330541504239178  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3979 training Loss: 0.15978275290094368 validation Loss: 0.15328574909441148  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3980 training Loss: 0.1597586978571663 validation Loss: 0.15326610775111207  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3981 training Loss: 0.1597347225137174 validation Loss: 0.1532464836960247  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3982 training Loss: 0.15971082527021555 validation Loss: 0.1532268703799976  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3983 training Loss: 0.15968700459098978 validation Loss: 0.15320726195769083  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3984 training Loss: 0.15966325900125183 validation Loss: 0.15318765322903793  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3985 training Loss: 0.15963958708359882 validation Loss: 0.15316803958528707  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3986 training Loss: 0.1596159874748096 validation Loss: 0.15314841695926149  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3987 training Loss: 0.15959245886290027 validation Loss: 0.15312878177951258  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3988 training Loss: 0.15956899998441063 validation Loss: 0.15310913092806147  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3989 training Loss: 0.1595456096218966 validation Loss: 0.15308946170145205  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3990 training Loss: 0.15952228660160622 validation Loss: 0.1530697717748587  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3991 training Loss: 0.1594990297913208 validation Loss: 0.15305005916901357  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3992 training Loss: 0.15947583809834465 validation Loss: 0.15303032221973573  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3993 training Loss: 0.15945271046762816 validation Loss: 0.15301055954986187  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3994 training Loss: 0.1594296458800125 validation Loss: 0.15299077004339529  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3995 training Loss: 0.15940664579492014 validation Loss: 0.15297112544853933  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3996 training Loss: 0.15938371629894374 validation Loss: 0.15295143838628433  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3997 training Loss: 0.15936084688179156 validation Loss: 0.15293170954802474  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3998 training Loss: 0.15933803665827406 validation Loss: 0.15291193971139017  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n",
      "epoch: 3999 training Loss: 0.15931528477110168 validation Loss: 0.15289212973009914  valid acc: 0.9066666666666666  train Acc: 0.8828571428571429\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Neural_Network(2,3,1)\n",
    "model.train(trainX, trainY, epochs =4000, learningRate = 0.0001, validationX = validX, validationY = validY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the best model which you have trained, \n",
    "model.saveModel('task2bestmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: This below section will be used for the evaluation of this task, we need your model and we will run script below to evaluated your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8866666666666667"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create class object\n",
    "mm = Neural_Network()\n",
    "# load model which will be provided by you\n",
    "mm.loadModel('task2bestmodel.npz')\n",
    "# check accuracy of that model\n",
    "mm.accuracy(testX,testY)\n",
    "\n",
    "###  tanh----  train and validation accuracy = 100, 98\n",
    "###  relu----  train and validation accuracy = 88.8\n",
    "###  sigmoid----  train and validation accuracy = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
