{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include libraries which may use in implementation\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Neural_Network class\n",
    "class Neural_Network(object):    \n",
    "    def __init__(self,inputSize = 2,outputSize = 1 ):\n",
    "        \n",
    "        # size of layers\n",
    "        self.inputSize = inputSize\n",
    "        self.outputSize = outputSize    \n",
    "        self.W1 = np.random.rand(self.inputSize+1, self.outputSize)   \n",
    "\n",
    "    def feedforward(self, X):\n",
    "\n",
    "        z = np.dot(X, self.W1)\n",
    "        activated_z = self.sigmoid(z)\n",
    "        return activated_z\n",
    "\n",
    "    def sigmoid(self, s):\n",
    "        activated_output = 1 / (1 + (np.exp(-s)))\n",
    "        return activated_output\n",
    "\n",
    "    def sigmoid_derivative(self, s):\n",
    "        der_sigmoid = s * (1 - s)\n",
    "        return der_sigmoid \n",
    "\n",
    "    def backwardpropagate(self,X, Y, y_pred, lr):\n",
    "        derivate_j =  (y_pred-Y)\n",
    "        k = np.dot(np.transpose(X), derivate_j)\n",
    "        self.W1 = self.W1 - (lr * k)\n",
    "        return self.W1\n",
    "    \n",
    "    def crossentropy(self, Y, Y_pred):\n",
    "        error_ce =  -np.sum((Y * np.log2(Y_pred)) + ((1-Y) * np.log2(1- Y_pred)))\n",
    "        return error_ce \n",
    "\n",
    "    def train(self, trainX, trainY,epochs, learningRate, plot_err = True ,validationX = 'Null', validationY = 'Null'):\n",
    "        train_error = []\n",
    "        valid_error = []\n",
    "        train_accu = []\n",
    "        valid_accu = []\n",
    "        for i in range(epochs):\n",
    "            \n",
    "            pred = self.feedforward(trainX)\n",
    "            self.backwardpropagate(trainX, trainY, pred, learningRate)\n",
    "            train_error.append( self.crossentropy(trainY, pred))\n",
    "            if validationX != 'Null' and validationY != 'Null':\n",
    "                val_pred = self.predict(validationX)\n",
    "                valid_error.append(self.crossentropy(validationY, val_pred))\n",
    "                valid_accu.append(self.accuracy(validationX, validationY))\n",
    "            train_accu.append(self.accuracy(trainX, trainY))\n",
    "            print (\"epoch:\",i, \"training Loss:\" ,train_error[-1], \"validation Loss:\" ,valid_error[-1],\\\n",
    "                   \" valid acc:\", valid_accu[-1], \" train Acc:\", train_accu[-1])\n",
    "            \n",
    "        self.curve_plot(epochs, train_error, valid_error, train_accu, valid_accu)\n",
    "        \n",
    "        \n",
    "    def curve_plot(self, epochs, train_error, valid_error, train_accu, valid_accu):\n",
    "        \n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"loss curve\")\n",
    "        plt.plot(range(epochs), train_error)\n",
    "        plt.plot(range(epochs), valid_error)\n",
    "        plt.show()     \n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"accuracy Curve\")\n",
    "        plt.plot(range(epochs), train_accu)\n",
    "        plt.plot(range(epochs), valid_accu)\n",
    "        plt.show()\n",
    "#         print(train_accu[-1])\n",
    "#         print(valid_accu[-1])\n",
    "#         print(error[-1])\n",
    "\n",
    "    def predict(self, testX):\n",
    "        # predict the value of testX\n",
    "        test_pred = self.feedforward(testX)\n",
    "        return test_pred\n",
    "    \n",
    "    def accuracy(self, testX, testY):\n",
    "        # predict the value of trainX\n",
    "        test_pred = self.predict(testX)\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        test_pred[test_pred>=0.5] = 1\n",
    "        test_pred[test_pred<0.5] = 0\n",
    "            \n",
    "        x= np.mean(testY == test_pred)\n",
    "        return x     \n",
    "        \n",
    "    def saveModel(self,name):\n",
    "        np.save(name, self.W1)\n",
    "\n",
    "    def loadModel(self,name):\n",
    "        self.W1 = np.load(name, allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generating dataset point\n",
    "np.random.seed(1)\n",
    "no_of_samples = 2000\n",
    "dims = 2\n",
    "#Generating random points of values between 0 to 1\n",
    "class1=np.random.rand(no_of_samples,dims)\n",
    "#To add separability we will add a bias of 1.1\n",
    "class2=np.random.rand(no_of_samples,dims)+1.1\n",
    "class_1_label=np.array([1 for n in range(no_of_samples)])\n",
    "class_2_label=np.array([0 for n in range(no_of_samples)])\n",
    "#Lets visualize the dataset\n",
    "plt.scatter(class1[:,0],class1[:,1], marker='^', label=\"class 1\")\n",
    "plt.scatter(class2[:,0],class2[:,1], marker='o', label=\"class 2\")\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data concatenation\n",
    "data = np.concatenate((class1,class2),axis=0)\n",
    "label = np.concatenate((class_1_label,class_2_label),axis=0)\n",
    "\n",
    "#Note: shuffle this dataset before dividing it into three parts\n",
    "from sklearn.utils import shuffle\n",
    "X, y = shuffle(data, label, random_state=0)\n",
    "# Distribute this data into three parts i.e. training, validation and testing\n",
    "\n",
    "trainX, validX, testX = X[:2800], X[2800:3400], X[3400:]\n",
    "trainY, validY, testY = y[:2800], y[2800:3400], y[3400:]\n",
    "\n",
    "\n",
    "trainY = np.expand_dims(trainY, axis=1)\n",
    "testY = np.expand_dims(testY, axis=1)\n",
    "validY = np.expand_dims(validY, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m , n, o = len(trainX), len(validX), len(testX)\n",
    "trainX = np.hstack((trainX, np.atleast_2d(np.ones(m)).T))\n",
    "testX = np.hstack((testX, np.atleast_2d(np.ones(n)).T))\n",
    "validX = np.hstack((validX, np.atleast_2d(np.ones(o)).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mudasser Afzal\\Anaconda3\\envs\\testing\\lib\\site-packages\\ipykernel_launcher.py:44: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 training Loss: 5272.824289678203 validation Loss: 1107.9397722335927  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 1 training Loss: 5163.4365371363365 validation Loss: 1085.0689315577492  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 2 training Loss: 5056.324192486974 validation Loss: 1062.6988205314174  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 3 training Loss: 4951.536924880023 validation Loss: 1040.839095901648  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 4 training Loss: 4849.121114992826 validation Loss: 1019.4986063275567  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 5 training Loss: 4749.1194330249045 validation Loss: 998.6853075105673  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 6 training Loss: 4651.5704305737345 validation Loss: 978.4061821526091  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 7 training Loss: 4556.508153950657 validation Loss: 958.6671663254601  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 8 training Loss: 4463.9617865544415 validation Loss: 939.4730837885317  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 9 training Loss: 4373.955327742034 validation Loss: 920.8275896946861  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 10 training Loss: 4286.507315208612 validation Loss: 902.7331249743324  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 11 training Loss: 4201.630597210896 validation Loss: 885.1908824898288  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 12 training Loss: 4119.332160050509 validation Loss: 868.2007858106957  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 13 training Loss: 4039.6130151030143 validation Loss: 851.76148118338  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 14 training Loss: 3962.4681483705167 validation Loss: 835.8703429675791  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 15 training Loss: 3887.886534100023 validation Loss: 820.5234924963185  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 16 training Loss: 3815.85121250263 validation Loss: 805.7158300018727  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 17 training Loss: 3746.3394300916207 validation Loss: 791.4410789471162  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 18 training Loss: 3679.3228396932623 validation Loss: 777.691841824226  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 19 training Loss: 3614.767755832088 validation Loss: 764.4596662405577  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 20 training Loss: 3552.6354600057066 validation Loss: 751.7351199136579  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 21 training Loss: 3492.8825493857726 validation Loss: 739.5078730498744  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 22 training Loss: 3435.4613217427504 validation Loss: 727.7667864871873  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 23 training Loss: 3380.3201889098873 validation Loss: 716.5000039431775  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 24 training Loss: 3327.4041108802367 validation Loss: 705.6950467213029  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 25 training Loss: 3276.655042660297 validation Loss: 695.3389092883336  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 26 training Loss: 3228.0123862642145 validation Loss: 685.4181542365748  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 27 training Loss: 3181.413440693811 validation Loss: 675.919005278744  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 28 training Loss: 3136.793843375591 validation Loss: 666.8274370827262  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 29 training Loss: 3094.087997276536 validation Loss: 658.1292609293984  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 30 training Loss: 3053.229478754925 validation Loss: 649.8102053611428  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 31 training Loss: 3014.1514220811323 validation Loss: 641.8559911740886  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 32 training Loss: 2976.7868774501503 validation Loss: 634.2524002870925  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 33 training Loss: 2941.069140171029 validation Loss: 626.9853381897478  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 34 training Loss: 2906.932049532706 validation Loss: 620.0408898262563  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 35 training Loss: 2874.3102565910613 validation Loss: 613.405368909079  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 36 training Loss: 2843.139460784655 validation Loss: 607.0653607742481  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 37 training Loss: 2813.35661585835 validation Loss: 601.0077589885  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 38 training Loss: 2784.9001060515607 validation Loss: 595.219795997233  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 39 training Loss: 2757.7098938922227 validation Loss: 589.6890681626014  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 40 training Loss: 2731.727641232893 validation Loss: 584.4035555843186  valid acc: 0.49166666666666664  train Acc: 0.5014285714285714\n",
      "epoch: 41 training Loss: 2706.8968053784047 validation Loss: 579.3516371236435  valid acc: 0.49666666666666665  train Acc: 0.5042857142857143\n",
      "epoch: 42 training Loss: 2683.162712293575 validation Loss: 574.5221010655939  valid acc: 0.5266666666666666  train Acc: 0.5328571428571428\n",
      "epoch: 43 training Loss: 2660.47260895386 validation Loss: 569.9041518576287  valid acc: 0.57  train Acc: 0.5867857142857142\n",
      "epoch: 44 training Loss: 2638.7756969212783 validation Loss: 565.4874133568968  valid acc: 0.62  train Acc: 0.6517857142857143\n",
      "epoch: 45 training Loss: 2618.023149201971 validation Loss: 561.2619290045357  valid acc: 0.6733333333333333  train Acc: 0.7146428571428571\n",
      "epoch: 46 training Loss: 2598.1681123794483 validation Loss: 557.2181593261665  valid acc: 0.7266666666666667  train Acc: 0.7646428571428572\n",
      "epoch: 47 training Loss: 2579.1656959273537 validation Loss: 553.3469771342208  valid acc: 0.7766666666666666  train Acc: 0.8171428571428572\n",
      "epoch: 48 training Loss: 2560.9729504949296 validation Loss: 549.639660781407  valid acc: 0.8283333333333334  train Acc: 0.8596428571428572\n",
      "epoch: 49 training Loss: 2543.5488368338174 validation Loss: 546.0878857866311  valid acc: 0.865  train Acc: 0.9\n",
      "epoch: 50 training Loss: 2526.854186902001 validation Loss: 542.683715125985  valid acc: 0.905  train Acc: 0.9346428571428571\n",
      "epoch: 51 training Loss: 2510.8516585441807 validation Loss: 539.4195884527734  valid acc: 0.9533333333333334  train Acc: 0.9685714285714285\n",
      "epoch: 52 training Loss: 2495.5056850114433 validation Loss: 536.2883104825778  valid acc: 0.9983333333333333  train Acc: 0.9992857142857143\n",
      "epoch: 53 training Loss: 2480.7824204496483 validation Loss: 533.2830387524986  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 54 training Loss: 2466.649682357778 validation Loss: 530.3972709383238  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 55 training Loss: 2453.0768918961476 validation Loss: 527.6248318896373  valid acc: 0.9866666666666667  train Acc: 0.9932142857142857\n",
      "epoch: 56 training Loss: 2440.0350128109494 validation Loss: 524.9598605209675  valid acc: 0.9666666666666667  train Acc: 0.975\n",
      "epoch: 57 training Loss: 2427.4964896367946 validation Loss: 522.3967966770134  valid acc: 0.96  train Acc: 0.9642857142857143\n",
      "epoch: 58 training Loss: 2415.435185742951 validation Loss: 519.9303680718078  valid acc: 0.945  train Acc: 0.9517857142857142\n",
      "epoch: 59 training Loss: 2403.8263217019985 validation Loss: 517.5555773853177  valid acc: 0.93  train Acc: 0.9410714285714286\n",
      "epoch: 60 training Loss: 2392.6464143812946 validation Loss: 515.2676895863859  valid acc: 0.915  train Acc: 0.9317857142857143\n",
      "epoch: 61 training Loss: 2381.873217087832 validation Loss: 513.0622195379839  valid acc: 0.905  train Acc: 0.9207142857142857\n",
      "epoch: 62 training Loss: 2371.485661035096 validation Loss: 510.9349199293682  valid acc: 0.9033333333333333  train Acc: 0.91\n",
      "epoch: 63 training Loss: 2361.4637983460966 validation Loss: 508.8817695697872  valid acc: 0.8933333333333333  train Acc: 0.9014285714285715\n",
      "epoch: 64 training Loss: 2351.7887467591017 validation Loss: 506.8989620697561  valid acc: 0.8883333333333333  train Acc: 0.8910714285714286\n",
      "epoch: 65 training Loss: 2342.4426361613314 validation Loss: 504.98289492848676  valid acc: 0.875  train Acc: 0.8842857142857142\n",
      "epoch: 66 training Loss: 2333.408557040232 validation Loss: 503.13015903969364  valid acc: 0.8633333333333333  train Acc: 0.8771428571428571\n",
      "epoch: 67 training Loss: 2324.6705109115264 validation Loss: 501.33752862260275  valid acc: 0.8583333333333333  train Acc: 0.8717857142857143\n",
      "epoch: 68 training Loss: 2316.2133627573794 validation Loss: 499.6019515804451  valid acc: 0.855  train Acc: 0.865\n",
      "epoch: 69 training Loss: 2308.0227954862753 validation Loss: 497.9205402849288  valid acc: 0.845  train Acc: 0.8578571428571429\n",
      "epoch: 70 training Loss: 2300.085266408056 validation Loss: 496.29056278205974  valid acc: 0.8383333333333334  train Acc: 0.8517857142857143\n",
      "epoch: 71 training Loss: 2292.3879657026255 validation Loss: 494.70943441213535  valid acc: 0.8316666666666667  train Acc: 0.8478571428571429\n",
      "epoch: 72 training Loss: 2284.9187768486345 validation Loss: 493.1747098346956  valid acc: 0.8283333333333334  train Acc: 0.8414285714285714\n",
      "epoch: 73 training Loss: 2277.6662389686962 validation Loss: 491.68407544761004  valid acc: 0.825  train Acc: 0.8353571428571429\n",
      "epoch: 74 training Loss: 2270.6195110399954 validation Loss: 490.23534218824693  valid acc: 0.8216666666666667  train Acc: 0.8314285714285714\n",
      "epoch: 75 training Loss: 2263.7683379132395 validation Loss: 488.82643870375915  valid acc: 0.82  train Acc: 0.8257142857142857\n",
      "epoch: 76 training Loss: 2257.1030180785615 validation Loss: 487.45540487688106  valid acc: 0.8166666666666667  train Acc: 0.8221428571428572\n",
      "epoch: 77 training Loss: 2250.6143731138877 validation Loss: 486.12038569321777  valid acc: 0.8166666666666667  train Acc: 0.8189285714285715\n",
      "epoch: 78 training Loss: 2244.293718749318 validation Loss: 484.8196254357899  valid acc: 0.8116666666666666  train Acc: 0.8160714285714286\n",
      "epoch: 79 training Loss: 2238.1328374800014 validation Loss: 483.5514621925336  valid acc: 0.8066666666666666  train Acc: 0.8128571428571428\n",
      "epoch: 80 training Loss: 2232.123952659709 validation Loss: 482.31432266252966  valid acc: 0.8016666666666666  train Acc: 0.8089285714285714\n",
      "epoch: 81 training Loss: 2226.2597040076053 validation Loss: 481.1067172469087  valid acc: 0.7983333333333333  train Acc: 0.8064285714285714\n",
      "epoch: 82 training Loss: 2220.5331244615877 validation Loss: 479.92723541064754  valid acc: 0.7916666666666666  train Acc: 0.8032142857142858\n",
      "epoch: 83 training Loss: 2214.9376183127983 validation Loss: 478.7745413017975  valid acc: 0.79  train Acc: 0.8003571428571429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 84 training Loss: 2209.466940557483 validation Loss: 477.6473696150733  valid acc: 0.785  train Acc: 0.7978571428571428\n",
      "epoch: 85 training Loss: 2204.115177404213 validation Loss: 476.5445216871528  valid acc: 0.7833333333333333  train Acc: 0.7946428571428571\n",
      "epoch: 86 training Loss: 2198.8767278764626 validation Loss: 475.4648618114886  valid acc: 0.78  train Acc: 0.7917857142857143\n",
      "epoch: 87 training Loss: 2193.7462864527433 validation Loss: 474.40731376090514  valid acc: 0.7766666666666666  train Acc: 0.79\n",
      "epoch: 88 training Loss: 2188.7188266886606 validation Loss: 473.37085750673657  valid acc: 0.7716666666666666  train Acc: 0.7871428571428571\n",
      "epoch: 89 training Loss: 2183.7895857676185 validation Loss: 472.35452612374837  valid acc: 0.7716666666666666  train Acc: 0.7857142857142857\n",
      "epoch: 90 training Loss: 2178.954049929181 validation Loss: 471.357402870575  valid acc: 0.77  train Acc: 0.7842857142857143\n",
      "epoch: 91 training Loss: 2174.2079407264378 validation Loss: 470.3786184358867  valid acc: 0.7683333333333333  train Acc: 0.7825\n",
      "epoch: 92 training Loss: 2169.5472020660004 validation Loss: 469.4173483409771  valid acc: 0.7666666666666667  train Acc: 0.7803571428571429\n",
      "epoch: 93 training Loss: 2164.967987986538 validation Loss: 468.4728104899271  valid acc: 0.765  train Acc: 0.7796428571428572\n",
      "epoch: 94 training Loss: 2160.466651133966 validation Loss: 467.5442628589541  valid acc: 0.7616666666666667  train Acc: 0.7785714285714286\n",
      "epoch: 95 training Loss: 2156.0397318935425 validation Loss: 466.63100131699355  valid acc: 0.7583333333333333  train Acc: 0.7764285714285715\n",
      "epoch: 96 training Loss: 2151.6839481412317 validation Loss: 465.73235756998434  valid acc: 0.7583333333333333  train Acc: 0.7757142857142857\n",
      "epoch: 97 training Loss: 2147.3961855786847 validation Loss: 464.84769722173576  valid acc: 0.7566666666666667  train Acc: 0.775\n",
      "epoch: 98 training Loss: 2143.173488618142 validation Loss: 463.97641794464494  valid acc: 0.7566666666666667  train Acc: 0.7735714285714286\n",
      "epoch: 99 training Loss: 2139.0130517853913 validation Loss: 463.1179477539084  valid acc: 0.7566666666666667  train Acc: 0.7725\n",
      "epoch: 100 training Loss: 2134.9122116107264 validation Loss: 462.27174337922486  valid acc: 0.7566666666666667  train Acc: 0.7721428571428571\n",
      "epoch: 101 training Loss: 2130.8684389795017 validation Loss: 461.4372887283316  valid acc: 0.7566666666666667  train Acc: 0.7717857142857143\n",
      "epoch: 102 training Loss: 2126.879331915525 validation Loss: 460.6140934370346  valid acc: 0.7566666666666667  train Acc: 0.7714285714285715\n",
      "epoch: 103 training Loss: 2122.942608772056 validation Loss: 459.8016915007034  valid acc: 0.7566666666666667  train Acc: 0.7714285714285715\n",
      "epoch: 104 training Loss: 2119.0561018066155 validation Loss: 458.9996399824915  valid acc: 0.7566666666666667  train Acc: 0.7707142857142857\n",
      "epoch: 105 training Loss: 2115.2177511172267 validation Loss: 458.20751779381885  valid acc: 0.7566666666666667  train Acc: 0.7707142857142857\n",
      "epoch: 106 training Loss: 2111.4255989189796 validation Loss: 457.42492454291363  valid acc: 0.7566666666666667  train Acc: 0.77\n",
      "epoch: 107 training Loss: 2107.6777841410776 validation Loss: 456.6514794474581  valid acc: 0.7566666666666667  train Acc: 0.7696428571428572\n",
      "epoch: 108 training Loss: 2103.9725373256742 validation Loss: 455.8868203076159  valid acc: 0.755  train Acc: 0.7696428571428572\n",
      "epoch: 109 training Loss: 2100.3081758109183 validation Loss: 455.1306025359369  valid acc: 0.7533333333333333  train Acc: 0.7685714285714286\n",
      "epoch: 110 training Loss: 2096.683099181676 validation Loss: 454.3824982408453  valid acc: 0.7533333333333333  train Acc: 0.7685714285714286\n",
      "epoch: 111 training Loss: 2093.095784972361 validation Loss: 453.64219536060887  valid acc: 0.7533333333333333  train Acc: 0.7689285714285714\n",
      "epoch: 112 training Loss: 2089.5447846072475 validation Loss: 452.9093968448754  valid acc: 0.7533333333333333  train Acc: 0.7685714285714286\n",
      "epoch: 113 training Loss: 2086.0287195644987 validation Loss: 452.18381988103147  valid acc: 0.7533333333333333  train Acc: 0.7682142857142857\n",
      "epoch: 114 training Loss: 2082.5462777509815 validation Loss: 451.4651951628043  valid acc: 0.7533333333333333  train Acc: 0.7685714285714286\n",
      "epoch: 115 training Loss: 2079.096210075683 validation Loss: 450.753266198681  valid acc: 0.7533333333333333  train Acc: 0.7682142857142857\n",
      "epoch: 116 training Loss: 2075.677327210292 validation Loss: 450.04778865786227  valid acc: 0.7533333333333333  train Acc: 0.7682142857142857\n",
      "epoch: 117 training Loss: 2072.2884965261956 validation Loss: 449.3485297516045  valid acc: 0.7533333333333333  train Acc: 0.77\n",
      "epoch: 118 training Loss: 2068.928639197752 validation Loss: 448.6552676479315  valid acc: 0.755  train Acc: 0.77\n",
      "epoch: 119 training Loss: 2065.596727462341 validation Loss: 447.9677909178169  valid acc: 0.755  train Acc: 0.77\n",
      "epoch: 120 training Loss: 2062.291782028235 validation Loss: 447.2858980110508  valid acc: 0.755  train Acc: 0.77\n",
      "epoch: 121 training Loss: 2059.012869621881 validation Loss: 446.60939676011014  valid acc: 0.755  train Acc: 0.7703571428571429\n",
      "epoch: 122 training Loss: 2055.759100666663 validation Loss: 445.9381039104529  valid acc: 0.755  train Acc: 0.7707142857142857\n",
      "epoch: 123 training Loss: 2052.529627085723 validation Loss: 445.27184467574614  valid acc: 0.755  train Acc: 0.7710714285714285\n",
      "epoch: 124 training Loss: 2049.323640221808 validation Loss: 444.6104523166311  valid acc: 0.755  train Acc: 0.7710714285714285\n",
      "epoch: 125 training Loss: 2046.1403688675766 validation Loss: 443.9537677417054  valid acc: 0.755  train Acc: 0.7714285714285715\n",
      "epoch: 126 training Loss: 2042.979077400147 validation Loss: 443.30163912948336  valid acc: 0.755  train Acc: 0.7717857142857143\n",
      "epoch: 127 training Loss: 2039.8390640140594 validation Loss: 442.653921570168  valid acc: 0.755  train Acc: 0.7717857142857143\n",
      "epoch: 128 training Loss: 2036.7196590471583 validation Loss: 442.01047672613356  valid acc: 0.755  train Acc: 0.7725\n",
      "epoch: 129 training Loss: 2033.6202233942229 validation Loss: 441.37117251008624  valid acc: 0.755  train Acc: 0.7725\n",
      "epoch: 130 training Loss: 2030.540147003478 validation Loss: 440.7358827799264  valid acc: 0.755  train Acc: 0.7728571428571429\n",
      "epoch: 131 training Loss: 2027.4788474514087 validation Loss: 440.1044870493959  valid acc: 0.755  train Acc: 0.7742857142857142\n",
      "epoch: 132 training Loss: 2024.435768591547 validation Loss: 439.4768702136448  valid acc: 0.76  train Acc: 0.7746428571428572\n",
      "epoch: 133 training Loss: 2021.4103792731871 validation Loss: 438.8529222889028  valid acc: 0.7616666666666667  train Acc: 0.775\n",
      "epoch: 134 training Loss: 2018.4021721261784 validation Loss: 438.23253816548845  valid acc: 0.7616666666666667  train Acc: 0.7760714285714285\n",
      "epoch: 135 training Loss: 2015.410662408202 validation Loss: 437.6156173734313  valid acc: 0.7616666666666667  train Acc: 0.7764285714285715\n",
      "epoch: 136 training Loss: 2012.4353869111224 validation Loss: 437.0020638600249  valid acc: 0.7633333333333333  train Acc: 0.7778571428571428\n",
      "epoch: 137 training Loss: 2009.4759029232148 validation Loss: 436.3917857786687  valid acc: 0.7633333333333333  train Acc: 0.7789285714285714\n",
      "epoch: 138 training Loss: 2006.5317872442429 validation Loss: 435.7846952883905  valid acc: 0.7633333333333333  train Acc: 0.7796428571428572\n",
      "epoch: 139 training Loss: 2003.6026352505396 validation Loss: 435.1807083634786  valid acc: 0.7633333333333333  train Acc: 0.7814285714285715\n",
      "epoch: 140 training Loss: 2000.6880600074055 validation Loss: 434.5797446126834  valid acc: 0.7633333333333333  train Acc: 0.7825\n",
      "epoch: 141 training Loss: 1997.7876914262902 validation Loss: 433.9817271074792  valid acc: 0.7633333333333333  train Acc: 0.7828571428571428\n",
      "epoch: 142 training Loss: 1994.9011754643648 validation Loss: 433.3865822189063  valid acc: 0.765  train Acc: 0.7828571428571428\n",
      "epoch: 143 training Loss: 1992.028173364236 validation Loss: 432.79423946253837  valid acc: 0.765  train Acc: 0.7832142857142858\n",
      "epoch: 144 training Loss: 1989.1683609316642 validation Loss: 432.2046313511487  valid acc: 0.7666666666666667  train Acc: 0.7839285714285714\n",
      "epoch: 145 training Loss: 1986.3214278492846 validation Loss: 431.61769325466923  valid acc: 0.7683333333333333  train Acc: 0.7846428571428572\n",
      "epoch: 146 training Loss: 1983.487077024432 validation Loss: 431.033363267062  valid acc: 0.7683333333333333  train Acc: 0.7846428571428572\n",
      "epoch: 147 training Loss: 1980.665023969276 validation Loss: 430.45158207974123  valid acc: 0.7683333333333333  train Acc: 0.7860714285714285\n",
      "epoch: 148 training Loss: 1977.854996211579 validation Loss: 429.8722928612057  valid acc: 0.7683333333333333  train Acc: 0.7860714285714285\n",
      "epoch: 149 training Loss: 1975.056732734476 validation Loss: 429.2954411425597  valid acc: 0.7716666666666666  train Acc: 0.7867857142857143\n",
      "epoch: 150 training Loss: 1972.2699834437703 validation Loss: 428.7209747086172  valid acc: 0.7716666666666666  train Acc: 0.7878571428571428\n",
      "epoch: 151 training Loss: 1969.4945086613136 validation Loss: 428.1488434943027  valid acc: 0.7716666666666666  train Acc: 0.7878571428571428\n",
      "epoch: 152 training Loss: 1966.7300786431283 validation Loss: 427.57899948607667  valid acc: 0.7733333333333333  train Acc: 0.79\n",
      "epoch: 153 training Loss: 1963.9764731209916 validation Loss: 427.0113966281266  valid acc: 0.775  train Acc: 0.7910714285714285\n",
      "epoch: 154 training Loss: 1961.233480866281 validation Loss: 426.445990733083  valid acc: 0.7766666666666666  train Acc: 0.7925\n",
      "epoch: 155 training Loss: 1958.5008992749392 validation Loss: 425.88273939702873  valid acc: 0.7766666666666666  train Acc: 0.7928571428571428\n",
      "epoch: 156 training Loss: 1955.7785339724808 validation Loss: 425.321601918583  valid acc: 0.7783333333333333  train Acc: 0.7932142857142858\n",
      "epoch: 157 training Loss: 1953.0661984380224 validation Loss: 424.7625392218562  valid acc: 0.78  train Acc: 0.7932142857142858\n",
      "epoch: 158 training Loss: 1950.363713646368 validation Loss: 424.2055137830788  valid acc: 0.78  train Acc: 0.7939285714285714\n",
      "epoch: 159 training Loss: 1947.670907727242 validation Loss: 423.6504895607203  valid acc: 0.78  train Acc: 0.7946428571428571\n",
      "epoch: 160 training Loss: 1944.9876156407986 validation Loss: 423.09743192892273  valid acc: 0.78  train Acc: 0.795\n",
      "epoch: 161 training Loss: 1942.3136788685968 validation Loss: 422.5463076140844  valid acc: 0.78  train Acc: 0.795\n",
      "epoch: 162 training Loss: 1939.6489451192588 validation Loss: 421.9970846344354  valid acc: 0.78  train Acc: 0.7953571428571429\n",
      "epoch: 163 training Loss: 1936.9932680480833 validation Loss: 421.44973224245734  valid acc: 0.7816666666666666  train Acc: 0.7967857142857143\n",
      "epoch: 164 training Loss: 1934.3465069899175 validation Loss: 420.9042208700067  valid acc: 0.7833333333333333  train Acc: 0.7971428571428572\n",
      "epoch: 165 training Loss: 1931.7085267046264 validation Loss: 420.3605220760075  valid acc: 0.7833333333333333  train Acc: 0.7975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 166 training Loss: 1929.0791971345393 validation Loss: 419.8186084965863  valid acc: 0.785  train Acc: 0.7985714285714286\n",
      "epoch: 167 training Loss: 1926.4583931732795 validation Loss: 419.2784537975325  valid acc: 0.7883333333333333  train Acc: 0.7996428571428571\n",
      "epoch: 168 training Loss: 1923.845994445417 validation Loss: 418.740032628966  valid acc: 0.79  train Acc: 0.8\n",
      "epoch: 169 training Loss: 1921.241885096414 validation Loss: 418.20332058210823  valid acc: 0.79  train Acc: 0.8014285714285714\n",
      "epoch: 170 training Loss: 1918.6459535923586 validation Loss: 417.6682941480517  valid acc: 0.7916666666666666  train Acc: 0.8014285714285714\n",
      "epoch: 171 training Loss: 1916.0580925290053 validation Loss: 417.1349306784319  valid acc: 0.7916666666666666  train Acc: 0.8021428571428572\n",
      "epoch: 172 training Loss: 1913.4781984496767 validation Loss: 416.6032083479103  valid acc: 0.7916666666666666  train Acc: 0.8021428571428572\n",
      "epoch: 173 training Loss: 1910.9061716715876 validation Loss: 416.0731061183792  valid acc: 0.7916666666666666  train Acc: 0.8032142857142858\n",
      "epoch: 174 training Loss: 1908.34191612019 validation Loss: 415.5446037048074  valid acc: 0.7916666666666666  train Acc: 0.8035714285714286\n",
      "epoch: 175 training Loss: 1905.7853391711471 validation Loss: 415.0176815426464  valid acc: 0.7933333333333333  train Acc: 0.8039285714285714\n",
      "epoch: 176 training Loss: 1903.2363514995711 validation Loss: 414.49232075672364  valid acc: 0.7933333333333333  train Acc: 0.8053571428571429\n",
      "epoch: 177 training Loss: 1900.6948669361764 validation Loss: 413.96850313155085  valid acc: 0.7933333333333333  train Acc: 0.8071428571428572\n",
      "epoch: 178 training Loss: 1898.1608023300141 validation Loss: 413.4462110829806  valid acc: 0.795  train Acc: 0.8082142857142857\n",
      "epoch: 179 training Loss: 1895.6340774174741 validation Loss: 412.9254276311467  valid acc: 0.7966666666666666  train Acc: 0.8089285714285714\n",
      "epoch: 180 training Loss: 1893.1146146972605 validation Loss: 412.406136374628  valid acc: 0.7983333333333333  train Acc: 0.8092857142857143\n",
      "epoch: 181 training Loss: 1890.6023393110468 validation Loss: 411.8883214657769  valid acc: 0.7983333333333333  train Acc: 0.8110714285714286\n",
      "epoch: 182 training Loss: 1888.0971789295495 validation Loss: 411.37196758715925  valid acc: 0.7983333333333333  train Acc: 0.8114285714285714\n",
      "epoch: 183 training Loss: 1885.5990636437625 validation Loss: 410.8570599290513  valid acc: 0.7983333333333333  train Acc: 0.8121428571428572\n",
      "epoch: 184 training Loss: 1883.1079258611032 validation Loss: 410.34358416794583  valid acc: 0.8016666666666666  train Acc: 0.8121428571428572\n",
      "epoch: 185 training Loss: 1880.6237002062467 validation Loss: 409.8315264460195  valid acc: 0.8033333333333333  train Acc: 0.8125\n",
      "epoch: 186 training Loss: 1878.1463234264215 validation Loss: 409.3208733515161  valid acc: 0.8083333333333333  train Acc: 0.8146428571428571\n",
      "epoch: 187 training Loss: 1875.6757343009622 validation Loss: 408.8116119000041  valid acc: 0.8083333333333333  train Acc: 0.815\n",
      "epoch: 188 training Loss: 1873.2118735549145 validation Loss: 408.30372951646683  valid acc: 0.8083333333333333  train Acc: 0.8157142857142857\n",
      "epoch: 189 training Loss: 1870.7546837765103 validation Loss: 407.7972140181877  valid acc: 0.8083333333333333  train Acc: 0.8160714285714286\n",
      "epoch: 190 training Loss: 1868.304109338329 validation Loss: 407.2920535983923  valid acc: 0.8083333333333333  train Acc: 0.8164285714285714\n",
      "epoch: 191 training Loss: 1865.8600963219715 validation Loss: 406.7882368106141  valid acc: 0.81  train Acc: 0.8167857142857143\n",
      "epoch: 192 training Loss: 1863.422592446089 validation Loss: 406.28575255374903  valid acc: 0.8133333333333334  train Acc: 0.8178571428571428\n",
      "epoch: 193 training Loss: 1860.9915469976092 validation Loss: 405.784590057768  valid acc: 0.815  train Acc: 0.8182142857142857\n",
      "epoch: 194 training Loss: 1858.5669107660112 validation Loss: 405.28473887005725  valid acc: 0.815  train Acc: 0.8185714285714286\n",
      "epoch: 195 training Loss: 1856.1486359805099 validation Loss: 404.7861888423571  valid acc: 0.815  train Acc: 0.82\n",
      "epoch: 196 training Loss: 1853.7366762500176 validation Loss: 404.2889301182729  valid acc: 0.815  train Acc: 0.8207142857142857\n",
      "epoch: 197 training Loss: 1851.3309865057531 validation Loss: 403.7929531213312  valid acc: 0.8166666666666667  train Acc: 0.8221428571428572\n",
      "epoch: 198 training Loss: 1848.9315229463778 validation Loss: 403.2982485435564  valid acc: 0.8166666666666667  train Acc: 0.8232142857142857\n",
      "epoch: 199 training Loss: 1846.5382429855442 validation Loss: 402.80480733454493  valid acc: 0.8166666666666667  train Acc: 0.8235714285714286\n",
      "epoch: 200 training Loss: 1844.1511052017474 validation Loss: 402.312620691014  valid acc: 0.8166666666666667  train Acc: 0.8239285714285715\n",
      "epoch: 201 training Loss: 1841.7700692903722 validation Loss: 401.8216800468034  valid acc: 0.8166666666666667  train Acc: 0.825\n",
      "epoch: 202 training Loss: 1839.3950960178381 validation Loss: 401.33197706330986  valid acc: 0.8166666666666667  train Acc: 0.8260714285714286\n",
      "epoch: 203 training Loss: 1837.0261471777494 validation Loss: 400.84350362033524  valid acc: 0.8183333333333334  train Acc: 0.8278571428571428\n",
      "epoch: 204 training Loss: 1834.6631855489536 validation Loss: 400.3562518073288  valid acc: 0.8183333333333334  train Acc: 0.8289285714285715\n",
      "epoch: 205 training Loss: 1832.3061748554285 validation Loss: 399.87021391500707  valid acc: 0.8183333333333334  train Acc: 0.8307142857142857\n",
      "epoch: 206 training Loss: 1829.9550797279123 validation Loss: 399.3853824273334  valid acc: 0.8233333333333334  train Acc: 0.8310714285714286\n",
      "epoch: 207 training Loss: 1827.6098656671975 validation Loss: 398.9017500138423  valid acc: 0.825  train Acc: 0.8325\n",
      "epoch: 208 training Loss: 1825.2704990090174 validation Loss: 398.4193095222911  valid acc: 0.825  train Acc: 0.8335714285714285\n",
      "epoch: 209 training Loss: 1822.9369468904497 validation Loss: 397.93805397162816  valid acc: 0.8266666666666667  train Acc: 0.8335714285714285\n",
      "epoch: 210 training Loss: 1820.6091772177738 validation Loss: 397.4579765452586  valid acc: 0.8283333333333334  train Acc: 0.835\n",
      "epoch: 211 training Loss: 1818.2871586357128 validation Loss: 396.97907058459884  valid acc: 0.8283333333333334  train Acc: 0.8367857142857142\n",
      "epoch: 212 training Loss: 1815.970860498001 validation Loss: 396.50132958290453  valid acc: 0.8283333333333334  train Acc: 0.8375\n",
      "epoch: 213 training Loss: 1813.6602528392168 validation Loss: 396.0247471793609  valid acc: 0.83  train Acc: 0.8378571428571429\n",
      "epoch: 214 training Loss: 1811.3553063478275 validation Loss: 395.54931715342315  valid acc: 0.8316666666666667  train Acc: 0.8389285714285715\n",
      "epoch: 215 training Loss: 1809.0559923403857 validation Loss: 395.0750334193973  valid acc: 0.8316666666666667  train Acc: 0.8407142857142857\n",
      "epoch: 216 training Loss: 1806.7622827368352 validation Loss: 394.60189002124906  valid acc: 0.8316666666666667  train Acc: 0.8410714285714286\n",
      "epoch: 217 training Loss: 1804.4741500368714 validation Loss: 394.1298811276323  valid acc: 0.8333333333333334  train Acc: 0.8417857142857142\n",
      "epoch: 218 training Loss: 1802.191567297309 validation Loss: 393.65900102712686  valid acc: 0.835  train Acc: 0.8425\n",
      "epoch: 219 training Loss: 1799.9145081104186 validation Loss: 393.1892441236763  valid acc: 0.835  train Acc: 0.8425\n",
      "epoch: 220 training Loss: 1797.6429465831839 validation Loss: 392.7206049322173  valid acc: 0.835  train Acc: 0.8435714285714285\n",
      "epoch: 221 training Loss: 1795.376857317439 validation Loss: 392.2530780744928  valid acc: 0.835  train Acc: 0.845\n",
      "epoch: 222 training Loss: 1793.1162153908551 validation Loss: 391.786658275039  valid acc: 0.8383333333333334  train Acc: 0.8453571428571428\n",
      "epoch: 223 training Loss: 1790.8609963387266 validation Loss: 391.3213403573417  valid acc: 0.8383333333333334  train Acc: 0.8460714285714286\n",
      "epoch: 224 training Loss: 1788.6111761365346 validation Loss: 390.85711924015175  valid acc: 0.84  train Acc: 0.8467857142857143\n",
      "epoch: 225 training Loss: 1786.3667311832432 validation Loss: 390.3939899339539  valid acc: 0.8416666666666667  train Acc: 0.8492857142857143\n",
      "epoch: 226 training Loss: 1784.1276382853057 validation Loss: 389.9319475375835  valid acc: 0.845  train Acc: 0.85\n",
      "epoch: 227 training Loss: 1781.8938746413423 validation Loss: 389.4709872349825  valid acc: 0.8483333333333334  train Acc: 0.8507142857142858\n",
      "epoch: 228 training Loss: 1779.6654178274646 validation Loss: 389.0111042920905  valid acc: 0.8483333333333334  train Acc: 0.8517857142857143\n",
      "epoch: 229 training Loss: 1777.4422457832181 validation Loss: 388.55229405386433  valid acc: 0.85  train Acc: 0.8528571428571429\n",
      "epoch: 230 training Loss: 1775.2243367981152 validation Loss: 388.0945519414205  valid acc: 0.85  train Acc: 0.8546428571428571\n",
      "epoch: 231 training Loss: 1773.0116694987323 validation Loss: 387.6378734492955  valid acc: 0.85  train Acc: 0.8557142857142858\n",
      "epoch: 232 training Loss: 1770.804222836349 validation Loss: 387.1822541428198  valid acc: 0.8516666666666667  train Acc: 0.8564285714285714\n",
      "epoch: 233 training Loss: 1768.601976075105 validation Loss: 386.7276896555981  valid acc: 0.8533333333333334  train Acc: 0.8571428571428571\n",
      "epoch: 234 training Loss: 1766.4049087806502 validation Loss: 386.2741756870946  valid acc: 0.8533333333333334  train Acc: 0.8578571428571429\n",
      "epoch: 235 training Loss: 1764.2130008092709 validation Loss: 385.8217080003162  valid acc: 0.855  train Acc: 0.8585714285714285\n",
      "epoch: 236 training Loss: 1762.0262322974684 validation Loss: 385.37028241959115  valid acc: 0.8566666666666667  train Acc: 0.8596428571428572\n",
      "epoch: 237 training Loss: 1759.844583651972 validation Loss: 384.9198948284376  valid acc: 0.86  train Acc: 0.8610714285714286\n",
      "epoch: 238 training Loss: 1757.6680355401663 validation Loss: 384.4705411675197  valid acc: 0.8616666666666667  train Acc: 0.8617857142857143\n",
      "epoch: 239 training Loss: 1755.4965688809184 validation Loss: 384.02221743268694  valid acc: 0.8616666666666667  train Acc: 0.8621428571428571\n",
      "epoch: 240 training Loss: 1753.3301648357829 validation Loss: 383.5749196730926  valid acc: 0.8616666666666667  train Acc: 0.8625\n",
      "epoch: 241 training Loss: 1751.168804800573 validation Loss: 383.128643989389  valid acc: 0.8616666666666667  train Acc: 0.8628571428571429\n",
      "epoch: 242 training Loss: 1749.012470397282 validation Loss: 382.6833865319955  valid acc: 0.8616666666666667  train Acc: 0.8628571428571429\n",
      "epoch: 243 training Loss: 1746.8611434663342 validation Loss: 382.2391434994368  valid acc: 0.8616666666666667  train Acc: 0.8657142857142858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 244 training Loss: 1744.7148060591594 validation Loss: 381.7959111367484  valid acc: 0.8616666666666667  train Acc: 0.8660714285714286\n",
      "epoch: 245 training Loss: 1742.5734404310729 validation Loss: 381.35368573394544  valid acc: 0.8633333333333333  train Acc: 0.8667857142857143\n",
      "epoch: 246 training Loss: 1740.4370290344477 validation Loss: 380.9124636245548  valid acc: 0.8633333333333333  train Acc: 0.8682142857142857\n",
      "epoch: 247 training Loss: 1738.3055545121674 validation Loss: 380.47224118420377  valid acc: 0.8633333333333333  train Acc: 0.8685714285714285\n",
      "epoch: 248 training Loss: 1736.1789996913478 validation Loss: 380.03301482926776  valid acc: 0.8633333333333333  train Acc: 0.8692857142857143\n",
      "epoch: 249 training Loss: 1734.0573475773185 validation Loss: 379.59478101557045  valid acc: 0.8633333333333333  train Acc: 0.87\n",
      "epoch: 250 training Loss: 1731.9405813478481 validation Loss: 379.15753623713624  valid acc: 0.8633333333333333  train Acc: 0.8710714285714286\n",
      "epoch: 251 training Loss: 1729.8286843476044 validation Loss: 378.7212770249927  valid acc: 0.8633333333333333  train Acc: 0.8710714285714286\n",
      "epoch: 252 training Loss: 1727.7216400828456 validation Loss: 378.2859999460204  valid acc: 0.865  train Acc: 0.8721428571428571\n",
      "epoch: 253 training Loss: 1725.6194322163242 validation Loss: 377.8517016018485  valid acc: 0.865  train Acc: 0.8732142857142857\n",
      "epoch: 254 training Loss: 1723.5220445623986 validation Loss: 377.41837862779425  valid acc: 0.865  train Acc: 0.8735714285714286\n",
      "epoch: 255 training Loss: 1721.4294610823463 validation Loss: 376.9860276918439  valid acc: 0.865  train Acc: 0.8746428571428572\n",
      "epoch: 256 training Loss: 1719.3416658798644 validation Loss: 376.55464549367457  valid acc: 0.865  train Acc: 0.8753571428571428\n",
      "epoch: 257 training Loss: 1717.2586431967532 validation Loss: 376.1242287637143  valid acc: 0.8666666666666667  train Acc: 0.8760714285714286\n",
      "epoch: 258 training Loss: 1715.1803774087762 validation Loss: 375.69477426223887  valid acc: 0.8666666666666667  train Acc: 0.8778571428571429\n",
      "epoch: 259 training Loss: 1713.1068530216855 validation Loss: 375.26627877850433  valid acc: 0.8666666666666667  train Acc: 0.8792857142857143\n",
      "epoch: 260 training Loss: 1711.0380546674103 validation Loss: 374.83873912991373  valid acc: 0.8666666666666667  train Acc: 0.88\n",
      "epoch: 261 training Loss: 1708.9739671003945 validation Loss: 374.41215216121554  valid acc: 0.8666666666666667  train Acc: 0.88\n",
      "epoch: 262 training Loss: 1706.9145751940875 validation Loss: 373.9865147437345  valid acc: 0.8666666666666667  train Acc: 0.8807142857142857\n",
      "epoch: 263 training Loss: 1704.8598639375705 validation Loss: 373.5618237746314  valid acc: 0.87  train Acc: 0.8807142857142857\n",
      "epoch: 264 training Loss: 1702.8098184323221 validation Loss: 373.13807617619216  valid acc: 0.87  train Acc: 0.8814285714285715\n",
      "epoch: 265 training Loss: 1700.7644238891123 validation Loss: 372.71526889514433  valid acc: 0.87  train Acc: 0.8825\n",
      "epoch: 266 training Loss: 1698.7236656250193 validation Loss: 372.2933989019997  valid acc: 0.87  train Acc: 0.8825\n",
      "epoch: 267 training Loss: 1696.6875290605678 validation Loss: 371.8724631904229  valid acc: 0.8716666666666667  train Acc: 0.8839285714285714\n",
      "epoch: 268 training Loss: 1694.6559997169782 validation Loss: 371.45245877662364  valid acc: 0.8733333333333333  train Acc: 0.8846428571428572\n",
      "epoch: 269 training Loss: 1692.6290632135292 validation Loss: 371.0333826987727  valid acc: 0.8733333333333333  train Acc: 0.885\n",
      "epoch: 270 training Loss: 1690.6067052650221 validation Loss: 370.61523201644  valid acc: 0.875  train Acc: 0.8871428571428571\n",
      "epoch: 271 training Loss: 1688.5889116793455 validation Loss: 370.19800381005416  valid acc: 0.875  train Acc: 0.8875\n",
      "epoch: 272 training Loss: 1686.57566835514 validation Loss: 369.78169518038305  valid acc: 0.875  train Acc: 0.8878571428571429\n",
      "epoch: 273 training Loss: 1684.5669612795482 validation Loss: 369.36630324803343  valid acc: 0.875  train Acc: 0.8882142857142857\n",
      "epoch: 274 training Loss: 1682.562776526062 validation Loss: 368.95182515297006  valid acc: 0.8766666666666667  train Acc: 0.8889285714285714\n",
      "epoch: 275 training Loss: 1680.5631002524474 validation Loss: 368.53825805405245  valid acc: 0.8783333333333333  train Acc: 0.8892857142857142\n",
      "epoch: 276 training Loss: 1678.5679186987545 validation Loss: 368.12559912859  valid acc: 0.8816666666666667  train Acc: 0.89\n",
      "epoch: 277 training Loss: 1676.5772181854034 validation Loss: 367.71384557191277  valid acc: 0.8816666666666667  train Acc: 0.89\n",
      "epoch: 278 training Loss: 1674.5909851113465 validation Loss: 367.3029945969587  valid acc: 0.885  train Acc: 0.8903571428571428\n",
      "epoch: 279 training Loss: 1672.609205952302 validation Loss: 366.89304343387704  valid acc: 0.885  train Acc: 0.8910714285714286\n",
      "epoch: 280 training Loss: 1670.6318672590546 validation Loss: 366.483989329645  valid acc: 0.885  train Acc: 0.8921428571428571\n",
      "epoch: 281 training Loss: 1668.658955655825 validation Loss: 366.0758295477002  valid acc: 0.885  train Acc: 0.8928571428571429\n",
      "epoch: 282 training Loss: 1666.6904578386993 validation Loss: 365.6685613675856  valid acc: 0.885  train Acc: 0.8942857142857142\n",
      "epoch: 283 training Loss: 1664.7263605741218 validation Loss: 365.262182084608  valid acc: 0.885  train Acc: 0.895\n",
      "epoch: 284 training Loss: 1662.7666506974426 validation Loss: 364.85668900950935  valid acc: 0.885  train Acc: 0.8953571428571429\n",
      "epoch: 285 training Loss: 1660.8113151115256 validation Loss: 364.45207946814924  valid acc: 0.885  train Acc: 0.8953571428571429\n",
      "epoch: 286 training Loss: 1658.8603407854048 validation Loss: 364.04835080120006  valid acc: 0.885  train Acc: 0.8960714285714285\n",
      "epoch: 287 training Loss: 1656.9137147529955 validation Loss: 363.6455003638523  valid acc: 0.8866666666666667  train Acc: 0.8964285714285715\n",
      "epoch: 288 training Loss: 1654.9714241118527 validation Loss: 363.2435255255314  valid acc: 0.8883333333333333  train Acc: 0.8964285714285715\n",
      "epoch: 289 training Loss: 1653.0334560219765 validation Loss: 362.8424236696238  valid acc: 0.8883333333333333  train Acc: 0.8967857142857143\n",
      "epoch: 290 training Loss: 1651.0997977046648 validation Loss: 362.44219219321394  valid acc: 0.8883333333333333  train Acc: 0.8978571428571429\n",
      "epoch: 291 training Loss: 1649.1704364414068 validation Loss: 362.0428285068299  valid acc: 0.89  train Acc: 0.8992857142857142\n",
      "epoch: 292 training Loss: 1647.2453595728189 validation Loss: 361.64433003419833  valid acc: 0.89  train Acc: 0.9\n",
      "epoch: 293 training Loss: 1645.3245544976216 validation Loss: 361.2466942120085  valid acc: 0.8916666666666667  train Acc: 0.9003571428571429\n",
      "epoch: 294 training Loss: 1643.4080086716517 validation Loss: 360.8499184896839  valid acc: 0.8916666666666667  train Acc: 0.9010714285714285\n",
      "epoch: 295 training Loss: 1641.4957096069156 validation Loss: 360.4540003291623  valid acc: 0.8916666666666667  train Acc: 0.9014285714285715\n",
      "epoch: 296 training Loss: 1639.5876448706726 validation Loss: 360.0589372046842  valid acc: 0.8933333333333333  train Acc: 0.9021428571428571\n",
      "epoch: 297 training Loss: 1637.6838020845566 validation Loss: 359.66472660258745  valid acc: 0.8933333333333333  train Acc: 0.9035714285714286\n",
      "epoch: 298 training Loss: 1635.7841689237262 validation Loss: 359.27136602110954  valid acc: 0.8933333333333333  train Acc: 0.9035714285714286\n",
      "epoch: 299 training Loss: 1633.888733116049 validation Loss: 358.87885297019704  valid acc: 0.8966666666666666  train Acc: 0.9039285714285714\n",
      "epoch: 300 training Loss: 1631.9974824413143 validation Loss: 358.4871849713213  valid acc: 0.8966666666666666  train Acc: 0.9057142857142857\n",
      "epoch: 301 training Loss: 1630.1104047304734 validation Loss: 358.0963595573005  valid acc: 0.8983333333333333  train Acc: 0.9064285714285715\n",
      "epoch: 302 training Loss: 1628.2274878649096 validation Loss: 357.70637427212733  valid acc: 0.8983333333333333  train Acc: 0.9075\n",
      "epoch: 303 training Loss: 1626.3487197757308 validation Loss: 357.3172266708037  valid acc: 0.8983333333333333  train Acc: 0.9078571428571428\n",
      "epoch: 304 training Loss: 1624.474088443093 validation Loss: 356.9289143191795  valid acc: 0.8983333333333333  train Acc: 0.9085714285714286\n",
      "epoch: 305 training Loss: 1622.6035818955413 validation Loss: 356.54143479379826  valid acc: 0.9  train Acc: 0.9089285714285714\n",
      "epoch: 306 training Loss: 1620.7371882093798 validation Loss: 356.15478568174643  valid acc: 0.9  train Acc: 0.9089285714285714\n",
      "epoch: 307 training Loss: 1618.8748955080607 validation Loss: 355.7689645805091  valid acc: 0.9016666666666666  train Acc: 0.9089285714285714\n",
      "epoch: 308 training Loss: 1617.0166919615933 validation Loss: 355.38396909782944  valid acc: 0.9016666666666666  train Acc: 0.9092857142857143\n",
      "epoch: 309 training Loss: 1615.1625657859795 validation Loss: 354.99979685157325  valid acc: 0.9033333333333333  train Acc: 0.9092857142857143\n",
      "epoch: 310 training Loss: 1613.3125052426612 validation Loss: 354.61644546959775  valid acc: 0.9033333333333333  train Acc: 0.9096428571428572\n",
      "epoch: 311 training Loss: 1611.4664986379935 validation Loss: 354.23391258962477  valid acc: 0.905  train Acc: 0.9103571428571429\n",
      "epoch: 312 training Loss: 1609.6245343227306 validation Loss: 353.8521958591176  valid acc: 0.905  train Acc: 0.9107142857142857\n",
      "epoch: 313 training Loss: 1607.7866006915324 validation Loss: 353.47129293516247  valid acc: 0.905  train Acc: 0.9114285714285715\n",
      "epoch: 314 training Loss: 1605.952686182486 validation Loss: 353.09120148435284  valid acc: 0.905  train Acc: 0.9117857142857143\n",
      "epoch: 315 training Loss: 1604.1227792766458 validation Loss: 352.7119191826784  valid acc: 0.905  train Acc: 0.9121428571428571\n",
      "epoch: 316 training Loss: 1602.2968684975845 validation Loss: 352.3334437154167  valid acc: 0.905  train Acc: 0.9121428571428571\n",
      "epoch: 317 training Loss: 1600.4749424109618 validation Loss: 351.9557727770283  valid acc: 0.9066666666666666  train Acc: 0.9121428571428571\n",
      "epoch: 318 training Loss: 1598.6569896241085 validation Loss: 351.5789040710557  valid acc: 0.9066666666666666  train Acc: 0.9125\n",
      "epoch: 319 training Loss: 1596.8429987856211 validation Loss: 351.20283531002457  valid acc: 0.9066666666666666  train Acc: 0.9128571428571428\n",
      "epoch: 320 training Loss: 1595.03295858497 validation Loss: 350.8275642153484  valid acc: 0.9066666666666666  train Acc: 0.9135714285714286\n",
      "epoch: 321 training Loss: 1593.2268577521224 validation Loss: 350.4530885172362  valid acc: 0.9066666666666666  train Acc: 0.9139285714285714\n",
      "epoch: 322 training Loss: 1591.4246850571744 validation Loss: 350.07940595460224  valid acc: 0.9066666666666666  train Acc: 0.915\n",
      "epoch: 323 training Loss: 1589.6264293099975 validation Loss: 349.70651427497927  valid acc: 0.9083333333333333  train Acc: 0.915\n",
      "epoch: 324 training Loss: 1587.8320793598918 validation Loss: 349.33441123443384  valid acc: 0.9083333333333333  train Acc: 0.915\n",
      "epoch: 325 training Loss: 1586.0416240952552 validation Loss: 348.96309459748403  valid acc: 0.91  train Acc: 0.915\n",
      "epoch: 326 training Loss: 1584.2550524432581 validation Loss: 348.59256213701974  valid acc: 0.9116666666666666  train Acc: 0.9153571428571429\n",
      "epoch: 327 training Loss: 1582.4723533695312 validation Loss: 348.2228116342253  valid acc: 0.9116666666666666  train Acc: 0.9164285714285715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 328 training Loss: 1580.6935158778592 validation Loss: 347.85384087850395  valid acc: 0.9116666666666666  train Acc: 0.9167857142857143\n",
      "epoch: 329 training Loss: 1578.9185290098885 validation Loss: 347.4856476674046  valid acc: 0.9166666666666666  train Acc: 0.9167857142857143\n",
      "epoch: 330 training Loss: 1577.1473818448374 validation Loss: 347.11822980655086  valid acc: 0.9166666666666666  train Acc: 0.9175\n",
      "epoch: 331 training Loss: 1575.3800634992201 validation Loss: 346.7515851095715  valid acc: 0.9183333333333333  train Acc: 0.9175\n",
      "epoch: 332 training Loss: 1573.6165631265744 validation Loss: 346.3857113980334  valid acc: 0.9183333333333333  train Acc: 0.9182142857142858\n",
      "epoch: 333 training Loss: 1571.8568699172008 validation Loss: 346.02060650137554  valid acc: 0.9183333333333333  train Acc: 0.9192857142857143\n",
      "epoch: 334 training Loss: 1570.1009730979044 validation Loss: 345.6562682568455  valid acc: 0.9183333333333333  train Acc: 0.9196428571428571\n",
      "epoch: 335 training Loss: 1568.3488619317495 validation Loss: 345.2926945094374  valid acc: 0.9183333333333333  train Acc: 0.92\n",
      "epoch: 336 training Loss: 1566.600525717815 validation Loss: 344.9298831118309  valid acc: 0.9183333333333333  train Acc: 0.9203571428571429\n",
      "epoch: 337 training Loss: 1564.8559537909607 validation Loss: 344.56783192433244  valid acc: 0.92  train Acc: 0.9207142857142857\n",
      "epoch: 338 training Loss: 1563.1151355215982 validation Loss: 344.2065388148179  valid acc: 0.92  train Acc: 0.9214285714285714\n",
      "epoch: 339 training Loss: 1561.3780603154673 validation Loss: 343.84600165867636  valid acc: 0.92  train Acc: 0.9221428571428572\n",
      "epoch: 340 training Loss: 1559.6447176134188 validation Loss: 343.48621833875524  valid acc: 0.92  train Acc: 0.9228571428571428\n",
      "epoch: 341 training Loss: 1557.9150968912027 validation Loss: 343.1271867453073  valid acc: 0.92  train Acc: 0.9232142857142858\n",
      "epoch: 342 training Loss: 1556.1891876592615 validation Loss: 342.76890477593844  valid acc: 0.92  train Acc: 0.9235714285714286\n",
      "epoch: 343 training Loss: 1554.4669794625288 validation Loss: 342.4113703355566  valid acc: 0.92  train Acc: 0.9239285714285714\n",
      "epoch: 344 training Loss: 1552.7484618802323 validation Loss: 342.0545813363228  valid acc: 0.92  train Acc: 0.9239285714285714\n",
      "epoch: 345 training Loss: 1551.0336245257022 validation Loss: 341.69853569760187  valid acc: 0.9233333333333333  train Acc: 0.9239285714285714\n",
      "epoch: 346 training Loss: 1549.3224570461844 validation Loss: 341.34323134591546  valid acc: 0.9233333333333333  train Acc: 0.925\n",
      "epoch: 347 training Loss: 1547.6149491226552 validation Loss: 340.98866621489583  valid acc: 0.9233333333333333  train Acc: 0.925\n",
      "epoch: 348 training Loss: 1545.911090469644 validation Loss: 340.6348382452405  valid acc: 0.925  train Acc: 0.9253571428571429\n",
      "epoch: 349 training Loss: 1544.2108708350586 validation Loss: 340.2817453846677  valid acc: 0.925  train Acc: 0.9257142857142857\n",
      "epoch: 350 training Loss: 1542.5142800000121 validation Loss: 339.92938558787347  valid acc: 0.925  train Acc: 0.9260714285714285\n",
      "epoch: 351 training Loss: 1540.821307778657 validation Loss: 339.5777568164889  valid acc: 0.925  train Acc: 0.9267857142857143\n",
      "epoch: 352 training Loss: 1539.13194401802 validation Loss: 339.2268570390388  valid acc: 0.925  train Acc: 0.9267857142857143\n",
      "epoch: 353 training Loss: 1537.4461785978424 validation Loss: 338.876684230901  valid acc: 0.9266666666666666  train Acc: 0.9271428571428572\n",
      "epoch: 354 training Loss: 1535.7640014304216 validation Loss: 338.5272363742665  valid acc: 0.9266666666666666  train Acc: 0.9275\n",
      "epoch: 355 training Loss: 1534.0854024604573 validation Loss: 338.1785114581004  valid acc: 0.9266666666666666  train Acc: 0.9278571428571428\n",
      "epoch: 356 training Loss: 1532.4103716649004 validation Loss: 337.83050747810387  valid acc: 0.9266666666666666  train Acc: 0.9282142857142858\n",
      "epoch: 357 training Loss: 1530.738899052804 validation Loss: 337.4832224366761  valid acc: 0.9266666666666666  train Acc: 0.9285714285714286\n",
      "epoch: 358 training Loss: 1529.0709746651783 validation Loss: 337.13665434287805  valid acc: 0.9266666666666666  train Acc: 0.9289285714285714\n",
      "epoch: 359 training Loss: 1527.4065885748469 validation Loss: 336.7908012123962  valid acc: 0.9266666666666666  train Acc: 0.9289285714285714\n",
      "epoch: 360 training Loss: 1525.745730886308 validation Loss: 336.44566106750676  valid acc: 0.9266666666666666  train Acc: 0.9292857142857143\n",
      "epoch: 361 training Loss: 1524.0883917355936 validation Loss: 336.1012319370414  valid acc: 0.9266666666666666  train Acc: 0.9292857142857143\n",
      "epoch: 362 training Loss: 1522.4345612901361 validation Loss: 335.7575118563525  valid acc: 0.9266666666666666  train Acc: 0.9296428571428571\n",
      "epoch: 363 training Loss: 1520.7842297486345 validation Loss: 335.41449886728014  valid acc: 0.9266666666666666  train Acc: 0.9296428571428571\n",
      "epoch: 364 training Loss: 1519.1373873409232 validation Loss: 335.0721910181186  valid acc: 0.9266666666666666  train Acc: 0.9296428571428571\n",
      "epoch: 365 training Loss: 1517.494024327842 validation Loss: 334.73058636358417  valid acc: 0.9266666666666666  train Acc: 0.9314285714285714\n",
      "epoch: 366 training Loss: 1515.85413100111 validation Loss: 334.38968296478345  valid acc: 0.9283333333333333  train Acc: 0.9314285714285714\n",
      "epoch: 367 training Loss: 1514.2176976832006 validation Loss: 334.0494788891814  valid acc: 0.9283333333333333  train Acc: 0.9314285714285714\n",
      "epoch: 368 training Loss: 1512.5847147272175 validation Loss: 333.7099722105711  valid acc: 0.9283333333333333  train Acc: 0.9317857142857143\n",
      "epoch: 369 training Loss: 1510.955172516773 validation Loss: 333.3711610090429  valid acc: 0.9283333333333333  train Acc: 0.9325\n",
      "epoch: 370 training Loss: 1509.32906146587 validation Loss: 333.0330433709547  valid acc: 0.9283333333333333  train Acc: 0.9328571428571428\n",
      "epoch: 371 training Loss: 1507.706372018782 validation Loss: 332.6956173889026  valid acc: 0.9283333333333333  train Acc: 0.9328571428571428\n",
      "epoch: 372 training Loss: 1506.0870946499367 validation Loss: 332.3588811616915  valid acc: 0.9283333333333333  train Acc: 0.9328571428571428\n",
      "epoch: 373 training Loss: 1504.4712198638026 validation Loss: 332.02283279430696  valid acc: 0.9283333333333333  train Acc: 0.9335714285714286\n",
      "epoch: 374 training Loss: 1502.858738194774 validation Loss: 331.68747039788684  valid acc: 0.93  train Acc: 0.9339285714285714\n",
      "epoch: 375 training Loss: 1501.2496402070597 validation Loss: 331.3527920896934  valid acc: 0.93  train Acc: 0.9339285714285714\n",
      "epoch: 376 training Loss: 1499.643916494571 validation Loss: 331.0187959930861  valid acc: 0.9316666666666666  train Acc: 0.9339285714285714\n",
      "epoch: 377 training Loss: 1498.0415576808145 validation Loss: 330.6854802374944  valid acc: 0.9316666666666666  train Acc: 0.9346428571428571\n",
      "epoch: 378 training Loss: 1496.4425544187825 validation Loss: 330.35284295839125  valid acc: 0.9316666666666666  train Acc: 0.9353571428571429\n",
      "epoch: 379 training Loss: 1494.8468973908457 validation Loss: 330.0208822972668  valid acc: 0.9333333333333333  train Acc: 0.9357142857142857\n",
      "epoch: 380 training Loss: 1493.254577308649 validation Loss: 329.68959640160216  valid acc: 0.9333333333333333  train Acc: 0.9360714285714286\n",
      "epoch: 381 training Loss: 1491.6655849130052 validation Loss: 329.35898342484404  valid acc: 0.9333333333333333  train Acc: 0.9367857142857143\n",
      "epoch: 382 training Loss: 1490.079910973793 validation Loss: 329.02904152637956  valid acc: 0.9333333333333333  train Acc: 0.9371428571428572\n",
      "epoch: 383 training Loss: 1488.497546289853 validation Loss: 328.6997688715105  valid acc: 0.935  train Acc: 0.9375\n",
      "epoch: 384 training Loss: 1486.918481688888 validation Loss: 328.3711636314296  valid acc: 0.935  train Acc: 0.9378571428571428\n",
      "epoch: 385 training Loss: 1485.342708027361 validation Loss: 328.0432239831953  valid acc: 0.9366666666666666  train Acc: 0.9382142857142857\n",
      "epoch: 386 training Loss: 1483.7702161903962 validation Loss: 327.715948109708  valid acc: 0.9366666666666666  train Acc: 0.9382142857142857\n",
      "epoch: 387 training Loss: 1482.200997091681 validation Loss: 327.389334199686  valid acc: 0.9366666666666666  train Acc: 0.9385714285714286\n",
      "epoch: 388 training Loss: 1480.6350416733671 validation Loss: 327.06338044764215  valid acc: 0.9366666666666666  train Acc: 0.9389285714285714\n",
      "epoch: 389 training Loss: 1479.0723409059742 validation Loss: 326.73808505386006  valid acc: 0.9366666666666666  train Acc: 0.9392857142857143\n",
      "epoch: 390 training Loss: 1477.5128857882942 validation Loss: 326.41344622437146  valid acc: 0.9366666666666666  train Acc: 0.9392857142857143\n",
      "epoch: 391 training Loss: 1475.9566673472948 validation Loss: 326.08946217093285  valid acc: 0.9366666666666666  train Acc: 0.9396428571428571\n",
      "epoch: 392 training Loss: 1474.4036766380266 validation Loss: 325.76613111100323  valid acc: 0.9383333333333334  train Acc: 0.9396428571428571\n",
      "epoch: 393 training Loss: 1472.853904743528 validation Loss: 325.4434512677217  valid acc: 0.9383333333333334  train Acc: 0.9403571428571429\n",
      "epoch: 394 training Loss: 1471.307342774732 validation Loss: 325.1214208698848  valid acc: 0.9383333333333334  train Acc: 0.9407142857142857\n",
      "epoch: 395 training Loss: 1469.7639818703747 validation Loss: 324.80003815192515  valid acc: 0.94  train Acc: 0.9410714285714286\n",
      "epoch: 396 training Loss: 1468.223813196903 validation Loss: 324.4793013538891  valid acc: 0.94  train Acc: 0.9414285714285714\n",
      "epoch: 397 training Loss: 1466.6868279483833 validation Loss: 324.15920872141584  valid acc: 0.94  train Acc: 0.9414285714285714\n",
      "epoch: 398 training Loss: 1465.1530173464116 validation Loss: 323.83975850571517  valid acc: 0.94  train Acc: 0.9414285714285714\n",
      "epoch: 399 training Loss: 1463.6223726400228 validation Loss: 323.52094896354697  valid acc: 0.94  train Acc: 0.9414285714285714\n",
      "epoch: 400 training Loss: 1462.0948851056019 validation Loss: 323.20277835720015  valid acc: 0.94  train Acc: 0.9417857142857143\n",
      "epoch: 401 training Loss: 1460.5705460467948 validation Loss: 322.8852449544714  valid acc: 0.94  train Acc: 0.9417857142857143\n",
      "epoch: 402 training Loss: 1459.0493467944211 validation Loss: 322.5683470286449  valid acc: 0.94  train Acc: 0.9417857142857143\n",
      "epoch: 403 training Loss: 1457.5312787063845 validation Loss: 322.2520828584719  valid acc: 0.94  train Acc: 0.9425\n",
      "epoch: 404 training Loss: 1456.016333167588 validation Loss: 321.93645072815013  valid acc: 0.94  train Acc: 0.9428571428571428\n",
      "epoch: 405 training Loss: 1454.504501589845 validation Loss: 321.6214489273039  valid acc: 0.94  train Acc: 0.9428571428571428\n",
      "epoch: 406 training Loss: 1452.9957754117947 validation Loss: 321.307075750964  valid acc: 0.94  train Acc: 0.9428571428571428\n",
      "epoch: 407 training Loss: 1451.4901460988149 validation Loss: 320.9933294995477  valid acc: 0.9433333333333334  train Acc: 0.9432142857142857\n",
      "epoch: 408 training Loss: 1449.9876051429378 validation Loss: 320.6802084788397  valid acc: 0.9433333333333334  train Acc: 0.9435714285714286\n",
      "epoch: 409 training Loss: 1448.4881440627655 validation Loss: 320.3677109999716  valid acc: 0.9433333333333334  train Acc: 0.9435714285714286\n",
      "epoch: 410 training Loss: 1446.9917544033835 validation Loss: 320.05583537940345  valid acc: 0.9433333333333334  train Acc: 0.9435714285714286\n",
      "epoch: 411 training Loss: 1445.4984277362792 validation Loss: 319.74457993890377  valid acc: 0.9433333333333334  train Acc: 0.9435714285714286\n",
      "epoch: 412 training Loss: 1444.0081556592563 validation Loss: 319.43394300553103  valid acc: 0.945  train Acc: 0.9439285714285715\n",
      "epoch: 413 training Loss: 1442.5209297963534 validation Loss: 319.12392291161416  valid acc: 0.945  train Acc: 0.9446428571428571\n",
      "epoch: 414 training Loss: 1441.0367417977584 validation Loss: 318.81451799473393  valid acc: 0.945  train Acc: 0.9446428571428571\n",
      "epoch: 415 training Loss: 1439.5555833397293 validation Loss: 318.5057265977044  valid acc: 0.945  train Acc: 0.9446428571428571\n",
      "epoch: 416 training Loss: 1438.0774461245096 validation Loss: 318.1975470685537  valid acc: 0.945  train Acc: 0.9446428571428571\n",
      "epoch: 417 training Loss: 1436.6023218802482 validation Loss: 317.8899777605063  valid acc: 0.9466666666666667  train Acc: 0.9453571428571429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 418 training Loss: 1435.130202360916 validation Loss: 317.5830170319641  valid acc: 0.9466666666666667  train Acc: 0.9457142857142857\n",
      "epoch: 419 training Loss: 1433.6610793462278 validation Loss: 317.2766632464883  valid acc: 0.9466666666666667  train Acc: 0.9457142857142857\n",
      "epoch: 420 training Loss: 1432.1949446415579 validation Loss: 316.97091477278127  valid acc: 0.9466666666666667  train Acc: 0.9457142857142857\n",
      "epoch: 421 training Loss: 1430.7317900778626 validation Loss: 316.6657699846687  valid acc: 0.9466666666666667  train Acc: 0.9460714285714286\n",
      "epoch: 422 training Loss: 1429.271607511599 validation Loss: 316.36122726108135  valid acc: 0.9466666666666667  train Acc: 0.9460714285714286\n",
      "epoch: 423 training Loss: 1427.814388824645 validation Loss: 316.05728498603736  valid acc: 0.9466666666666667  train Acc: 0.9464285714285714\n",
      "epoch: 424 training Loss: 1426.3601259242191 validation Loss: 315.75394154862465  valid acc: 0.9466666666666667  train Acc: 0.9467857142857142\n",
      "epoch: 425 training Loss: 1424.9088107428038 validation Loss: 315.4511953429832  valid acc: 0.9466666666666667  train Acc: 0.9467857142857142\n",
      "epoch: 426 training Loss: 1423.4604352380632 validation Loss: 315.14904476828747  valid acc: 0.9466666666666667  train Acc: 0.9471428571428572\n",
      "epoch: 427 training Loss: 1422.0149913927667 validation Loss: 314.8474882287288  valid acc: 0.9466666666666667  train Acc: 0.9475\n",
      "epoch: 428 training Loss: 1420.5724712147103 validation Loss: 314.5465241334989  valid acc: 0.9466666666666667  train Acc: 0.9482142857142857\n",
      "epoch: 429 training Loss: 1419.1328667366372 validation Loss: 314.2461508967714  valid acc: 0.9466666666666667  train Acc: 0.9489285714285715\n",
      "epoch: 430 training Loss: 1417.6961700161612 validation Loss: 313.9463669376855  valid acc: 0.9466666666666667  train Acc: 0.9489285714285715\n",
      "epoch: 431 training Loss: 1416.2623731356894 validation Loss: 313.64717068032905  valid acc: 0.9466666666666667  train Acc: 0.9489285714285715\n",
      "epoch: 432 training Loss: 1414.8314682023433 validation Loss: 313.348560553721  valid acc: 0.9466666666666667  train Acc: 0.9496428571428571\n",
      "epoch: 433 training Loss: 1413.4034473478828 validation Loss: 313.05053499179473  valid acc: 0.9466666666666667  train Acc: 0.9496428571428571\n",
      "epoch: 434 training Loss: 1411.9783027286294 validation Loss: 312.75309243338154  valid acc: 0.9466666666666667  train Acc: 0.95\n",
      "epoch: 435 training Loss: 1410.5560265253896 validation Loss: 312.4562313221936  valid acc: 0.9466666666666667  train Acc: 0.9507142857142857\n",
      "epoch: 436 training Loss: 1409.1366109433777 validation Loss: 312.1599501068075  valid acc: 0.9466666666666667  train Acc: 0.9510714285714286\n",
      "epoch: 437 training Loss: 1407.7200482121402 validation Loss: 311.8642472406475  valid acc: 0.9466666666666667  train Acc: 0.9510714285714286\n",
      "epoch: 438 training Loss: 1406.3063305854812 validation Loss: 311.56912118196936  valid acc: 0.9466666666666667  train Acc: 0.9510714285714286\n",
      "epoch: 439 training Loss: 1404.895450341384 validation Loss: 311.27457039384376  valid acc: 0.9466666666666667  train Acc: 0.9517857142857142\n",
      "epoch: 440 training Loss: 1403.4873997819384 validation Loss: 310.98059334413983  valid acc: 0.9466666666666667  train Acc: 0.9517857142857142\n",
      "epoch: 441 training Loss: 1402.082171233263 validation Loss: 310.6871885055095  valid acc: 0.9466666666666667  train Acc: 0.9521428571428572\n",
      "epoch: 442 training Loss: 1400.6797570454328 validation Loss: 310.3943543553704  valid acc: 0.9466666666666667  train Acc: 0.9525\n",
      "epoch: 443 training Loss: 1399.2801495924018 validation Loss: 310.10208937589084  valid acc: 0.9466666666666667  train Acc: 0.9525\n",
      "epoch: 444 training Loss: 1397.8833412719312 validation Loss: 309.8103920539728  valid acc: 0.9466666666666667  train Acc: 0.9525\n",
      "epoch: 445 training Loss: 1396.4893245055123 validation Loss: 309.51926088123673  valid acc: 0.9483333333333334  train Acc: 0.9528571428571428\n",
      "epoch: 446 training Loss: 1395.098091738294 validation Loss: 309.22869435400514  valid acc: 0.95  train Acc: 0.9528571428571428\n",
      "epoch: 447 training Loss: 1393.7096354390087 validation Loss: 308.9386909732873  valid acc: 0.95  train Acc: 0.9528571428571428\n",
      "epoch: 448 training Loss: 1392.3239480998982 validation Loss: 308.64924924476287  valid acc: 0.95  train Acc: 0.9528571428571428\n",
      "epoch: 449 training Loss: 1390.94102223664 validation Loss: 308.3603676787669  valid acc: 0.95  train Acc: 0.9528571428571428\n",
      "epoch: 450 training Loss: 1389.560850388275 validation Loss: 308.0720447902737  valid acc: 0.95  train Acc: 0.9528571428571428\n",
      "epoch: 451 training Loss: 1388.183425117133 validation Loss: 307.78427909888165  valid acc: 0.95  train Acc: 0.9528571428571428\n",
      "epoch: 452 training Loss: 1386.8087390087605 validation Loss: 307.49706912879736  valid acc: 0.95  train Acc: 0.9528571428571428\n",
      "epoch: 453 training Loss: 1385.4367846718483 validation Loss: 307.2104134088206  valid acc: 0.95  train Acc: 0.9528571428571428\n",
      "epoch: 454 training Loss: 1384.0675547381582 validation Loss: 306.9243104723289  valid acc: 0.95  train Acc: 0.9532142857142857\n",
      "epoch: 455 training Loss: 1382.701041862451 validation Loss: 306.638758857262  valid acc: 0.95  train Acc: 0.9535714285714286\n",
      "epoch: 456 training Loss: 1381.3372387224144 validation Loss: 306.3537571061068  valid acc: 0.95  train Acc: 0.9539285714285715\n",
      "epoch: 457 training Loss: 1379.9761380185914 validation Loss: 306.0693037658823  valid acc: 0.9516666666666667  train Acc: 0.9539285714285715\n",
      "epoch: 458 training Loss: 1378.617732474307 validation Loss: 305.7853973881243  valid acc: 0.9533333333333334  train Acc: 0.9539285714285715\n",
      "epoch: 459 training Loss: 1377.262014835599 validation Loss: 305.5020365288704  valid acc: 0.9533333333333334  train Acc: 0.9542857142857143\n",
      "epoch: 460 training Loss: 1375.908977871145 validation Loss: 305.2192197486453  valid acc: 0.9533333333333334  train Acc: 0.955\n",
      "epoch: 461 training Loss: 1374.55861437219 validation Loss: 304.93694561244513  valid acc: 0.9533333333333334  train Acc: 0.955\n",
      "epoch: 462 training Loss: 1373.2109171524794 validation Loss: 304.6552126897235  valid acc: 0.9533333333333334  train Acc: 0.955\n",
      "epoch: 463 training Loss: 1371.8658790481836 validation Loss: 304.37401955437605  valid acc: 0.9533333333333334  train Acc: 0.955\n",
      "epoch: 464 training Loss: 1370.5234929178303 validation Loss: 304.0933647847259  valid acc: 0.9533333333333334  train Acc: 0.955\n",
      "epoch: 465 training Loss: 1369.1837516422324 validation Loss: 303.81324696350885  valid acc: 0.9533333333333334  train Acc: 0.9557142857142857\n",
      "epoch: 466 training Loss: 1367.846648124419 validation Loss: 303.53366467785895  valid acc: 0.9533333333333334  train Acc: 0.9557142857142857\n",
      "epoch: 467 training Loss: 1366.5121752895643 validation Loss: 303.2546165192937  valid acc: 0.9533333333333334  train Acc: 0.9557142857142857\n",
      "epoch: 468 training Loss: 1365.1803260849183 validation Loss: 302.9761010836995  valid acc: 0.9533333333333334  train Acc: 0.9557142857142857\n",
      "epoch: 469 training Loss: 1363.8510934797364 validation Loss: 302.69811697131723  valid acc: 0.9533333333333334  train Acc: 0.9557142857142857\n",
      "epoch: 470 training Loss: 1362.52447046521 validation Loss: 302.42066278672803  valid acc: 0.9533333333333334  train Acc: 0.9557142857142857\n",
      "epoch: 471 training Loss: 1361.200450054398 validation Loss: 302.1437371388384  valid acc: 0.9533333333333334  train Acc: 0.9557142857142857\n",
      "epoch: 472 training Loss: 1359.879025282156 validation Loss: 301.86733864086636  valid acc: 0.9533333333333334  train Acc: 0.9564285714285714\n",
      "epoch: 473 training Loss: 1358.5601892050686 validation Loss: 301.591465910327  valid acc: 0.955  train Acc: 0.9564285714285714\n",
      "epoch: 474 training Loss: 1357.2439349013798 validation Loss: 301.316117569018  valid acc: 0.9566666666666667  train Acc: 0.9564285714285714\n",
      "epoch: 475 training Loss: 1355.9302554709238 validation Loss: 301.04129224300596  valid acc: 0.9566666666666667  train Acc: 0.9567857142857142\n",
      "epoch: 476 training Loss: 1354.6191440350588 validation Loss: 300.766988562612  valid acc: 0.9566666666666667  train Acc: 0.9575\n",
      "epoch: 477 training Loss: 1353.3105937365954 validation Loss: 300.49320516239743  valid acc: 0.9566666666666667  train Acc: 0.9575\n",
      "epoch: 478 training Loss: 1352.0045977397308 validation Loss: 300.2199406811503  valid acc: 0.9566666666666667  train Acc: 0.9578571428571429\n",
      "epoch: 479 training Loss: 1350.7011492299798 validation Loss: 299.9471937618709  valid acc: 0.9566666666666667  train Acc: 0.9578571428571429\n",
      "epoch: 480 training Loss: 1349.4002414141073 validation Loss: 299.6749630517581  valid acc: 0.9583333333333334  train Acc: 0.9589285714285715\n",
      "epoch: 481 training Loss: 1348.1018675200608 validation Loss: 299.4032472021954  valid acc: 0.9583333333333334  train Acc: 0.9589285714285715\n",
      "epoch: 482 training Loss: 1346.8060207969022 validation Loss: 299.13204486873724  valid acc: 0.9583333333333334  train Acc: 0.9589285714285715\n",
      "epoch: 483 training Loss: 1345.5126945147408 validation Loss: 298.86135471109486  valid acc: 0.9583333333333334  train Acc: 0.9592857142857143\n",
      "epoch: 484 training Loss: 1344.2218819646673 validation Loss: 298.59117539312297  valid acc: 0.9583333333333334  train Acc: 0.9603571428571429\n",
      "epoch: 485 training Loss: 1342.933576458685 validation Loss: 298.32150558280586  valid acc: 0.9583333333333334  train Acc: 0.9607142857142857\n",
      "epoch: 486 training Loss: 1341.6477713296435 validation Loss: 298.05234395224375  valid acc: 0.9583333333333334  train Acc: 0.9607142857142857\n",
      "epoch: 487 training Loss: 1340.364459931174 validation Loss: 297.7836891776392  valid acc: 0.9583333333333334  train Acc: 0.9607142857142857\n",
      "epoch: 488 training Loss: 1339.0836356376199 validation Loss: 297.5155399392837  valid acc: 0.9583333333333334  train Acc: 0.9610714285714286\n",
      "epoch: 489 training Loss: 1337.805291843972 validation Loss: 297.24789492154395  valid acc: 0.9583333333333334  train Acc: 0.9610714285714286\n",
      "epoch: 490 training Loss: 1336.529421965803 validation Loss: 296.9807528128486  valid acc: 0.9583333333333334  train Acc: 0.9617857142857142\n",
      "epoch: 491 training Loss: 1335.2560194392004 validation Loss: 296.71411230567475  valid acc: 0.96  train Acc: 0.9621428571428572\n",
      "epoch: 492 training Loss: 1333.9850777207012 validation Loss: 296.44797209653456  valid acc: 0.96  train Acc: 0.9621428571428572\n",
      "epoch: 493 training Loss: 1332.7165902872268 validation Loss: 296.18233088596196  valid acc: 0.96  train Acc: 0.9625\n",
      "epoch: 494 training Loss: 1331.4505506360165 validation Loss: 295.9171873784993  valid acc: 0.96  train Acc: 0.9625\n",
      "epoch: 495 training Loss: 1330.1869522845632 validation Loss: 295.6525402826845  valid acc: 0.96  train Acc: 0.9625\n",
      "epoch: 496 training Loss: 1328.9257887705473 validation Loss: 295.3883883110371  valid acc: 0.96  train Acc: 0.9625\n",
      "epoch: 497 training Loss: 1327.6670536517736 validation Loss: 295.12473018004596  valid acc: 0.96  train Acc: 0.9625\n",
      "epoch: 498 training Loss: 1326.4107405061047 validation Loss: 294.8615646101557  valid acc: 0.96  train Acc: 0.9628571428571429\n",
      "epoch: 499 training Loss: 1325.1568429313973 validation Loss: 294.5988903257535  valid acc: 0.96  train Acc: 0.9628571428571429\n",
      "epoch: 500 training Loss: 1323.905354545437 validation Loss: 294.33670605515647  valid acc: 0.96  train Acc: 0.9628571428571429\n",
      "epoch: 501 training Loss: 1322.6562689858756 validation Loss: 294.0750105305988  valid acc: 0.96  train Acc: 0.9632142857142857\n",
      "epoch: 502 training Loss: 1321.4095799101663 validation Loss: 293.8138024882179  valid acc: 0.96  train Acc: 0.9632142857142857\n",
      "epoch: 503 training Loss: 1320.165280995498 validation Loss: 293.55308066804275  valid acc: 0.96  train Acc: 0.9632142857142857\n",
      "epoch: 504 training Loss: 1318.9233659387355 validation Loss: 293.29284381398037  valid acc: 0.96  train Acc: 0.9632142857142857\n",
      "epoch: 505 training Loss: 1317.6838284563514 validation Loss: 293.0330906738029  valid acc: 0.96  train Acc: 0.9632142857142857\n",
      "epoch: 506 training Loss: 1316.4466622843674 validation Loss: 292.77381999913524  valid acc: 0.96  train Acc: 0.9632142857142857\n",
      "epoch: 507 training Loss: 1315.2118611782864 validation Loss: 292.5150305454422  valid acc: 0.96  train Acc: 0.9632142857142857\n",
      "epoch: 508 training Loss: 1313.9794189130328 validation Loss: 292.2567210720157  valid acc: 0.96  train Acc: 0.9632142857142857\n",
      "epoch: 509 training Loss: 1312.7493292828885 validation Loss: 291.99889034196235  valid acc: 0.96  train Acc: 0.9635714285714285\n",
      "epoch: 510 training Loss: 1311.5215861014306 validation Loss: 291.7415371221907  valid acc: 0.96  train Acc: 0.9635714285714285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 511 training Loss: 1310.2961832014676 validation Loss: 291.4846601833987  valid acc: 0.96  train Acc: 0.9635714285714285\n",
      "epoch: 512 training Loss: 1309.0731144349784 validation Loss: 291.22825830006127  valid acc: 0.96  train Acc: 0.9635714285714285\n",
      "epoch: 513 training Loss: 1307.8523736730501 validation Loss: 290.9723302504177  valid acc: 0.96  train Acc: 0.9635714285714285\n",
      "epoch: 514 training Loss: 1306.6339548058143 validation Loss: 290.7168748164596  valid acc: 0.96  train Acc: 0.9635714285714285\n",
      "epoch: 515 training Loss: 1305.4178517423875 validation Loss: 290.4618907839178  valid acc: 0.96  train Acc: 0.9635714285714285\n",
      "epoch: 516 training Loss: 1304.204058410808 validation Loss: 290.2073769422508  valid acc: 0.9616666666666667  train Acc: 0.9635714285714285\n",
      "epoch: 517 training Loss: 1302.9925687579746 validation Loss: 289.95333208463194  valid acc: 0.9616666666666667  train Acc: 0.9639285714285715\n",
      "epoch: 518 training Loss: 1301.7833767495856 validation Loss: 289.69975500793737  valid acc: 0.9616666666666667  train Acc: 0.9639285714285715\n",
      "epoch: 519 training Loss: 1300.5764763700768 validation Loss: 289.44664451273354  valid acc: 0.9616666666666667  train Acc: 0.9639285714285715\n",
      "epoch: 520 training Loss: 1299.371861622562 validation Loss: 289.1939994032655  valid acc: 0.9616666666666667  train Acc: 0.9639285714285715\n",
      "epoch: 521 training Loss: 1298.16952652877 validation Loss: 288.9418184874444  valid acc: 0.9616666666666667  train Acc: 0.9639285714285715\n",
      "epoch: 522 training Loss: 1296.9694651289856 validation Loss: 288.6901005768352  valid acc: 0.9616666666666667  train Acc: 0.9639285714285715\n",
      "epoch: 523 training Loss: 1295.7716714819896 validation Loss: 288.4388444866454  valid acc: 0.9616666666666667  train Acc: 0.9639285714285715\n",
      "epoch: 524 training Loss: 1294.5761396649955 validation Loss: 288.18804903571186  valid acc: 0.9633333333333334  train Acc: 0.9639285714285715\n",
      "epoch: 525 training Loss: 1293.382863773593 validation Loss: 287.93771304648976  valid acc: 0.9633333333333334  train Acc: 0.9639285714285715\n",
      "epoch: 526 training Loss: 1292.1918379216854 validation Loss: 287.6878353450404  valid acc: 0.9633333333333334  train Acc: 0.9639285714285715\n",
      "epoch: 527 training Loss: 1291.00305624143 validation Loss: 287.43841476101886  valid acc: 0.9633333333333334  train Acc: 0.9639285714285715\n",
      "epoch: 528 training Loss: 1289.8165128831808 validation Loss: 287.1894501276628  valid acc: 0.9633333333333334  train Acc: 0.9639285714285715\n",
      "epoch: 529 training Loss: 1288.6322020154255 validation Loss: 286.94094028177994  valid acc: 0.9633333333333334  train Acc: 0.9639285714285715\n",
      "epoch: 530 training Loss: 1287.4501178247283 validation Loss: 286.69288406373704  valid acc: 0.9633333333333334  train Acc: 0.9639285714285715\n",
      "epoch: 531 training Loss: 1286.270254515671 validation Loss: 286.4452803174473  valid acc: 0.965  train Acc: 0.9642857142857143\n",
      "epoch: 532 training Loss: 1285.0926063107927 validation Loss: 286.1981278903594  valid acc: 0.965  train Acc: 0.9642857142857143\n",
      "epoch: 533 training Loss: 1283.9171674505312 validation Loss: 285.95142563344507  valid acc: 0.965  train Acc: 0.9642857142857143\n",
      "epoch: 534 training Loss: 1282.7439321931656 validation Loss: 285.7051724011882  valid acc: 0.965  train Acc: 0.9646428571428571\n",
      "epoch: 535 training Loss: 1281.5728948147557 validation Loss: 285.4593670515727  valid acc: 0.965  train Acc: 0.9646428571428571\n",
      "epoch: 536 training Loss: 1280.404049609086 validation Loss: 285.21400844607115  valid acc: 0.965  train Acc: 0.9646428571428571\n",
      "epoch: 537 training Loss: 1279.2373908876061 validation Loss: 284.9690954496332  valid acc: 0.965  train Acc: 0.9646428571428571\n",
      "epoch: 538 training Loss: 1278.072912979372 validation Loss: 284.724626930674  valid acc: 0.965  train Acc: 0.965\n",
      "epoch: 539 training Loss: 1276.9106102309897 validation Loss: 284.480601761063  valid acc: 0.965  train Acc: 0.965\n",
      "epoch: 540 training Loss: 1275.750477006558 validation Loss: 284.23701881611225  valid acc: 0.965  train Acc: 0.965\n",
      "epoch: 541 training Loss: 1274.5925076876088 validation Loss: 283.9938769745652  valid acc: 0.965  train Acc: 0.9653571428571428\n",
      "epoch: 542 training Loss: 1273.4366966730515 validation Loss: 283.7511751185851  valid acc: 0.965  train Acc: 0.9657142857142857\n",
      "epoch: 543 training Loss: 1272.2830383791152 validation Loss: 283.50891213374416  valid acc: 0.965  train Acc: 0.9660714285714286\n",
      "epoch: 544 training Loss: 1271.1315272392917 validation Loss: 283.26708690901165  valid acc: 0.965  train Acc: 0.9660714285714286\n",
      "epoch: 545 training Loss: 1269.9821577042792 validation Loss: 283.02569833674335  valid acc: 0.965  train Acc: 0.9660714285714286\n",
      "epoch: 546 training Loss: 1268.8349242419247 validation Loss: 282.78474531266966  valid acc: 0.965  train Acc: 0.9660714285714286\n",
      "epoch: 547 training Loss: 1267.689821337168 validation Loss: 282.5442267358849  valid acc: 0.965  train Acc: 0.9664285714285714\n",
      "epoch: 548 training Loss: 1266.5468434919862 validation Loss: 282.3041415088361  valid acc: 0.965  train Acc: 0.9664285714285714\n",
      "epoch: 549 training Loss: 1265.4059852253354 validation Loss: 282.0644885373117  valid acc: 0.965  train Acc: 0.9664285714285714\n",
      "epoch: 550 training Loss: 1264.267241073096 validation Loss: 281.8252667304309  valid acc: 0.965  train Acc: 0.9664285714285714\n",
      "epoch: 551 training Loss: 1263.1306055880186 validation Loss: 281.586475000632  valid acc: 0.965  train Acc: 0.9667857142857142\n",
      "epoch: 552 training Loss: 1261.9960733396642 validation Loss: 281.3481122636621  valid acc: 0.965  train Acc: 0.9667857142857142\n",
      "epoch: 553 training Loss: 1260.8636389143524 validation Loss: 281.11017743856564  valid acc: 0.965  train Acc: 0.9667857142857142\n",
      "epoch: 554 training Loss: 1259.733296915104 validation Loss: 280.87266944767396  valid acc: 0.965  train Acc: 0.9667857142857142\n",
      "epoch: 555 training Loss: 1258.6050419615863 validation Loss: 280.6355872165939  valid acc: 0.965  train Acc: 0.9671428571428572\n",
      "epoch: 556 training Loss: 1257.4788686900592 validation Loss: 280.3989296741971  valid acc: 0.965  train Acc: 0.9675\n",
      "epoch: 557 training Loss: 1256.3547717533183 validation Loss: 280.1626957526097  valid acc: 0.965  train Acc: 0.9675\n",
      "epoch: 558 training Loss: 1255.2327458206414 validation Loss: 279.92688438720063  valid acc: 0.965  train Acc: 0.9675\n",
      "epoch: 559 training Loss: 1254.1127855777347 validation Loss: 279.6914945165719  valid acc: 0.965  train Acc: 0.9675\n",
      "epoch: 560 training Loss: 1252.9948857266763 validation Loss: 279.45652508254705  valid acc: 0.965  train Acc: 0.9675\n",
      "epoch: 561 training Loss: 1251.8790409858643 validation Loss: 279.22197503016093  valid acc: 0.965  train Acc: 0.9675\n",
      "epoch: 562 training Loss: 1250.7652460899606 validation Loss: 278.987843307649  valid acc: 0.965  train Acc: 0.9675\n",
      "epoch: 563 training Loss: 1249.6534957898389 validation Loss: 278.7541288664365  valid acc: 0.965  train Acc: 0.9675\n",
      "epoch: 564 training Loss: 1248.54378485253 validation Loss: 278.52083066112823  valid acc: 0.965  train Acc: 0.9678571428571429\n",
      "epoch: 565 training Loss: 1247.4361080611682 validation Loss: 278.2879476494978  valid acc: 0.965  train Acc: 0.9678571428571429\n",
      "epoch: 566 training Loss: 1246.3304602149385 validation Loss: 278.0554787924773  valid acc: 0.965  train Acc: 0.9678571428571429\n",
      "epoch: 567 training Loss: 1245.2268361290228 validation Loss: 277.82342305414653  valid acc: 0.965  train Acc: 0.9682142857142857\n",
      "epoch: 568 training Loss: 1244.1252306345468 validation Loss: 277.59177940172276  valid acc: 0.965  train Acc: 0.9685714285714285\n",
      "epoch: 569 training Loss: 1243.0256385785274 validation Loss: 277.3605468055505  valid acc: 0.965  train Acc: 0.9689285714285715\n",
      "epoch: 570 training Loss: 1241.9280548238203 validation Loss: 277.1297242390908  valid acc: 0.965  train Acc: 0.9692857142857143\n",
      "epoch: 571 training Loss: 1240.8324742490663 validation Loss: 276.89931067891126  valid acc: 0.965  train Acc: 0.9692857142857143\n",
      "epoch: 572 training Loss: 1239.73889174864 validation Loss: 276.6693051046754  valid acc: 0.9666666666666667  train Acc: 0.9692857142857143\n",
      "epoch: 573 training Loss: 1238.6473022325968 validation Loss: 276.4397064991326  valid acc: 0.9666666666666667  train Acc: 0.9692857142857143\n",
      "epoch: 574 training Loss: 1237.5577006266215 validation Loss: 276.210513848108  valid acc: 0.9666666666666667  train Acc: 0.9692857142857143\n",
      "epoch: 575 training Loss: 1236.4700818719753 validation Loss: 275.98172614049184  valid acc: 0.9666666666666667  train Acc: 0.9692857142857143\n",
      "epoch: 576 training Loss: 1235.3844409254452 validation Loss: 275.75334236822994  valid acc: 0.9666666666666667  train Acc: 0.9696428571428571\n",
      "epoch: 577 training Loss: 1234.3007727592917 validation Loss: 275.52536152631285  valid acc: 0.9666666666666667  train Acc: 0.9696428571428571\n",
      "epoch: 578 training Loss: 1233.219072361197 validation Loss: 275.2977826127666  valid acc: 0.9683333333333334  train Acc: 0.9696428571428571\n",
      "epoch: 579 training Loss: 1232.1393347342146 validation Loss: 275.0706046286418  valid acc: 0.9683333333333334  train Acc: 0.9696428571428571\n",
      "epoch: 580 training Loss: 1231.0615548967176 validation Loss: 274.84382657800404  valid acc: 0.9683333333333334  train Acc: 0.9696428571428571\n",
      "epoch: 581 training Loss: 1229.9857278823479 validation Loss: 274.6174474679241  valid acc: 0.9683333333333334  train Acc: 0.9696428571428571\n",
      "epoch: 582 training Loss: 1228.9118487399655 validation Loss: 274.39146630846733  valid acc: 0.9683333333333334  train Acc: 0.97\n",
      "epoch: 583 training Loss: 1227.8399125335966 validation Loss: 274.1658821126845  valid acc: 0.9683333333333334  train Acc: 0.97\n",
      "epoch: 584 training Loss: 1226.7699143423852 validation Loss: 273.9406938966014  valid acc: 0.9683333333333334  train Acc: 0.97\n",
      "epoch: 585 training Loss: 1225.7018492605412 validation Loss: 273.71590067920874  valid acc: 0.9683333333333334  train Acc: 0.9703571428571428\n",
      "epoch: 586 training Loss: 1224.6357123972905 validation Loss: 273.49150148245315  valid acc: 0.9683333333333334  train Acc: 0.9703571428571428\n",
      "epoch: 587 training Loss: 1223.571498876826 validation Loss: 273.26749533122677  valid acc: 0.9683333333333334  train Acc: 0.9703571428571428\n",
      "epoch: 588 training Loss: 1222.509203838255 validation Loss: 273.0438812533575  valid acc: 0.9683333333333334  train Acc: 0.9703571428571428\n",
      "epoch: 589 training Loss: 1221.4488224355532 validation Loss: 272.82065827959934  valid acc: 0.9683333333333334  train Acc: 0.9703571428571428\n",
      "epoch: 590 training Loss: 1220.390349837512 validation Loss: 272.5978254436229  valid acc: 0.9683333333333334  train Acc: 0.9703571428571428\n",
      "epoch: 591 training Loss: 1219.3337812276914 validation Loss: 272.37538178200543  valid acc: 0.9683333333333334  train Acc: 0.9703571428571428\n",
      "epoch: 592 training Loss: 1218.2791118043685 validation Loss: 272.1533263342213  valid acc: 0.9683333333333334  train Acc: 0.9703571428571428\n",
      "epoch: 593 training Loss: 1217.2263367804903 validation Loss: 271.9316581426323  valid acc: 0.9683333333333334  train Acc: 0.9703571428571428\n",
      "epoch: 594 training Loss: 1216.175451383624 validation Loss: 271.7103762524783  valid acc: 0.9683333333333334  train Acc: 0.9703571428571428\n",
      "epoch: 595 training Loss: 1215.1264508559073 validation Loss: 271.48947971186755  valid acc: 0.9683333333333334  train Acc: 0.9707142857142858\n",
      "epoch: 596 training Loss: 1214.079330454002 validation Loss: 271.2689675717669  valid acc: 0.97  train Acc: 0.9707142857142858\n",
      "epoch: 597 training Loss: 1213.0340854490437 validation Loss: 271.04883888599295  valid acc: 0.97  train Acc: 0.9707142857142858\n",
      "epoch: 598 training Loss: 1211.9907111265936 validation Loss: 270.82909271120207  valid acc: 0.9716666666666667  train Acc: 0.9710714285714286\n",
      "epoch: 599 training Loss: 1210.9492027865913 validation Loss: 270.60972810688105  valid acc: 0.9716666666666667  train Acc: 0.9710714285714286\n",
      "epoch: 600 training Loss: 1209.9095557433056 validation Loss: 270.3907441353379  valid acc: 0.9716666666666667  train Acc: 0.9710714285714286\n",
      "epoch: 601 training Loss: 1208.871765325288 validation Loss: 270.17213986169236  valid acc: 0.9716666666666667  train Acc: 0.9710714285714286\n",
      "epoch: 602 training Loss: 1207.835826875323 validation Loss: 269.9539143538666  valid acc: 0.9716666666666667  train Acc: 0.9710714285714286\n",
      "epoch: 603 training Loss: 1206.8017357503832 validation Loss: 269.73606668257594  valid acc: 0.9716666666666667  train Acc: 0.9710714285714286\n",
      "epoch: 604 training Loss: 1205.7694873215792 validation Loss: 269.51859592131956  valid acc: 0.9716666666666667  train Acc: 0.9710714285714286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 605 training Loss: 1204.7390769741146 validation Loss: 269.30150114637127  valid acc: 0.9733333333333334  train Acc: 0.9710714285714286\n",
      "epoch: 606 training Loss: 1203.710500107237 validation Loss: 269.0847814367703  valid acc: 0.9733333333333334  train Acc: 0.9710714285714286\n",
      "epoch: 607 training Loss: 1202.683752134193 validation Loss: 268.86843587431224  valid acc: 0.975  train Acc: 0.9710714285714286\n",
      "epoch: 608 training Loss: 1201.6588284821792 validation Loss: 268.6524635435397  valid acc: 0.975  train Acc: 0.9714285714285714\n",
      "epoch: 609 training Loss: 1200.6357245922973 validation Loss: 268.43686353173314  valid acc: 0.975  train Acc: 0.9714285714285714\n",
      "epoch: 610 training Loss: 1199.6144359195068 validation Loss: 268.2216349289023  valid acc: 0.975  train Acc: 0.9714285714285714\n",
      "epoch: 611 training Loss: 1198.5949579325793 validation Loss: 268.0067768277765  valid acc: 0.975  train Acc: 0.9714285714285714\n",
      "epoch: 612 training Loss: 1197.5772861140515 validation Loss: 267.7922883237959  valid acc: 0.975  train Acc: 0.9717857142857143\n",
      "epoch: 613 training Loss: 1196.5614159601794 validation Loss: 267.5781685151027  valid acc: 0.975  train Acc: 0.9721428571428572\n",
      "epoch: 614 training Loss: 1195.5473429808924 validation Loss: 267.36441650253175  valid acc: 0.975  train Acc: 0.9721428571428572\n",
      "epoch: 615 training Loss: 1194.535062699748 validation Loss: 267.1510313896021  valid acc: 0.975  train Acc: 0.9725\n",
      "epoch: 616 training Loss: 1193.5245706538863 validation Loss: 266.9380122825077  valid acc: 0.975  train Acc: 0.9725\n",
      "epoch: 617 training Loss: 1192.515862393983 validation Loss: 266.72535829010866  valid acc: 0.975  train Acc: 0.9725\n",
      "epoch: 618 training Loss: 1191.508933484206 validation Loss: 266.5130685239224  valid acc: 0.975  train Acc: 0.9725\n",
      "epoch: 619 training Loss: 1190.503779502169 validation Loss: 266.30114209811506  valid acc: 0.975  train Acc: 0.9725\n",
      "epoch: 620 training Loss: 1189.5003960388876 validation Loss: 266.0895781294922  valid acc: 0.975  train Acc: 0.9725\n",
      "epoch: 621 training Loss: 1188.4987786987326 validation Loss: 265.87837573749084  valid acc: 0.975  train Acc: 0.9728571428571429\n",
      "epoch: 622 training Loss: 1187.4989230993883 validation Loss: 265.6675340441696  valid acc: 0.975  train Acc: 0.9728571428571429\n",
      "epoch: 623 training Loss: 1186.5008248718045 validation Loss: 265.4570521742011  valid acc: 0.975  train Acc: 0.9728571428571429\n",
      "epoch: 624 training Loss: 1185.5044796601542 validation Loss: 265.2469292548628  valid acc: 0.975  train Acc: 0.9728571428571429\n",
      "epoch: 625 training Loss: 1184.509883121789 validation Loss: 265.0371644160283  valid acc: 0.975  train Acc: 0.9728571428571429\n",
      "epoch: 626 training Loss: 1183.5170309271948 validation Loss: 264.8277567901588  valid acc: 0.9766666666666667  train Acc: 0.9728571428571429\n",
      "epoch: 627 training Loss: 1182.5259187599472 validation Loss: 264.6187055122948  valid acc: 0.9766666666666667  train Acc: 0.9732142857142857\n",
      "epoch: 628 training Loss: 1181.5365423166693 validation Loss: 264.410009720047  valid acc: 0.9766666666666667  train Acc: 0.9732142857142857\n",
      "epoch: 629 training Loss: 1180.548897306986 validation Loss: 264.2016685535883  valid acc: 0.9766666666666667  train Acc: 0.9735714285714285\n",
      "epoch: 630 training Loss: 1179.5629794534816 validation Loss: 263.993681155645  valid acc: 0.9766666666666667  train Acc: 0.9735714285714285\n",
      "epoch: 631 training Loss: 1178.578784491657 validation Loss: 263.78604667148835  valid acc: 0.9766666666666667  train Acc: 0.9739285714285715\n",
      "epoch: 632 training Loss: 1177.5963081698849 validation Loss: 263.5787642489263  valid acc: 0.9766666666666667  train Acc: 0.9739285714285715\n",
      "epoch: 633 training Loss: 1176.6155462493675 validation Loss: 263.3718330382949  valid acc: 0.9766666666666667  train Acc: 0.9742857142857143\n",
      "epoch: 634 training Loss: 1175.6364945040937 validation Loss: 263.16525219244994  valid acc: 0.9783333333333334  train Acc: 0.9746428571428571\n",
      "epoch: 635 training Loss: 1174.6591487207957 validation Loss: 262.9590208667588  valid acc: 0.9783333333333334  train Acc: 0.9746428571428571\n",
      "epoch: 636 training Loss: 1173.6835046989072 validation Loss: 262.75313821909185  valid acc: 0.9783333333333334  train Acc: 0.975\n",
      "epoch: 637 training Loss: 1172.7095582505199 validation Loss: 262.54760340981426  valid acc: 0.9783333333333334  train Acc: 0.975\n",
      "epoch: 638 training Loss: 1171.737305200341 validation Loss: 262.3424156017778  valid acc: 0.9783333333333334  train Acc: 0.975\n",
      "epoch: 639 training Loss: 1170.7667413856527 validation Loss: 262.1375739603126  valid acc: 0.9783333333333334  train Acc: 0.975\n",
      "epoch: 640 training Loss: 1169.7978626562676 validation Loss: 261.93307765321856  valid acc: 0.9783333333333334  train Acc: 0.975\n",
      "epoch: 641 training Loss: 1168.8306648744883 validation Loss: 261.7289258507579  valid acc: 0.9783333333333334  train Acc: 0.975\n",
      "epoch: 642 training Loss: 1167.8651439150653 validation Loss: 261.52511772564634  valid acc: 0.9783333333333334  train Acc: 0.975\n",
      "epoch: 643 training Loss: 1166.901295665155 validation Loss: 261.3216524530453  valid acc: 0.9783333333333334  train Acc: 0.975\n",
      "epoch: 644 training Loss: 1165.9391160242785 validation Loss: 261.11852921055373  valid acc: 0.9783333333333334  train Acc: 0.975\n",
      "epoch: 645 training Loss: 1164.9786009042796 validation Loss: 260.91574717819975  valid acc: 0.9783333333333334  train Acc: 0.975\n",
      "epoch: 646 training Loss: 1164.019746229284 validation Loss: 260.7133055384333  valid acc: 0.9783333333333334  train Acc: 0.975\n",
      "epoch: 647 training Loss: 1163.0625479356581 validation Loss: 260.5112034761173  valid acc: 0.9783333333333334  train Acc: 0.975\n",
      "epoch: 648 training Loss: 1162.1070019719682 validation Loss: 260.3094401785203  valid acc: 0.9783333333333334  train Acc: 0.9757142857142858\n",
      "epoch: 649 training Loss: 1161.1531042989386 validation Loss: 260.108014835308  valid acc: 0.9783333333333334  train Acc: 0.9757142857142858\n",
      "epoch: 650 training Loss: 1160.2008508894123 validation Loss: 259.90692663853577  valid acc: 0.9783333333333334  train Acc: 0.9757142857142858\n",
      "epoch: 651 training Loss: 1159.2502377283095 validation Loss: 259.7061747826405  valid acc: 0.9783333333333334  train Acc: 0.9757142857142858\n",
      "epoch: 652 training Loss: 1158.3012608125873 validation Loss: 259.5057584644327  valid acc: 0.9783333333333334  train Acc: 0.9764285714285714\n",
      "epoch: 653 training Loss: 1157.353916151199 validation Loss: 259.3056768830885  valid acc: 0.9783333333333334  train Acc: 0.9764285714285714\n",
      "epoch: 654 training Loss: 1156.4081997650549 validation Loss: 259.1059292401424  valid acc: 0.9783333333333334  train Acc: 0.9764285714285714\n",
      "epoch: 655 training Loss: 1155.4641076869816 validation Loss: 258.9065147394787  valid acc: 0.9783333333333334  train Acc: 0.9764285714285714\n",
      "epoch: 656 training Loss: 1154.521635961682 validation Loss: 258.7074325873243  valid acc: 0.9783333333333334  train Acc: 0.9764285714285714\n",
      "epoch: 657 training Loss: 1153.5807806456955 validation Loss: 258.50868199224055  valid acc: 0.9783333333333334  train Acc: 0.9764285714285714\n",
      "epoch: 658 training Loss: 1152.6415378073589 validation Loss: 258.3102621651158  valid acc: 0.9783333333333334  train Acc: 0.9764285714285714\n",
      "epoch: 659 training Loss: 1151.7039035267662 validation Loss: 258.1121723191576  valid acc: 0.9783333333333334  train Acc: 0.9764285714285714\n",
      "epoch: 660 training Loss: 1150.7678738957293 validation Loss: 257.914411669885  valid acc: 0.9783333333333334  train Acc: 0.9764285714285714\n",
      "epoch: 661 training Loss: 1149.8334450177397 validation Loss: 257.716979435121  valid acc: 0.9783333333333334  train Acc: 0.9764285714285714\n",
      "epoch: 662 training Loss: 1148.9006130079279 validation Loss: 257.51987483498465  valid acc: 0.9783333333333334  train Acc: 0.9764285714285714\n",
      "epoch: 663 training Loss: 1147.9693739930253 validation Loss: 257.32309709188394  valid acc: 0.9783333333333334  train Acc: 0.9764285714285714\n",
      "epoch: 664 training Loss: 1147.0397241113258 validation Loss: 257.1266454305077  valid acc: 0.9783333333333334  train Acc: 0.9764285714285714\n",
      "epoch: 665 training Loss: 1146.1116595126457 validation Loss: 256.9305190778184  valid acc: 0.9783333333333334  train Acc: 0.9767857142857143\n",
      "epoch: 666 training Loss: 1145.1851763582863 validation Loss: 256.7347172630445  valid acc: 0.9783333333333334  train Acc: 0.9767857142857143\n",
      "epoch: 667 training Loss: 1144.2602708209947 validation Loss: 256.53923921767307  valid acc: 0.9783333333333334  train Acc: 0.9767857142857143\n",
      "epoch: 668 training Loss: 1143.3369390849266 validation Loss: 256.3440841754421  valid acc: 0.9783333333333334  train Acc: 0.9767857142857143\n",
      "epoch: 669 training Loss: 1142.415177345606 validation Loss: 256.14925137233337  valid acc: 0.9783333333333334  train Acc: 0.9771428571428571\n",
      "epoch: 670 training Loss: 1141.4949818098903 validation Loss: 255.9547400465646  valid acc: 0.9783333333333334  train Acc: 0.9775\n",
      "epoch: 671 training Loss: 1140.5763486959286 validation Loss: 255.76054943858264  valid acc: 0.9783333333333334  train Acc: 0.9775\n",
      "epoch: 672 training Loss: 1139.659274233127 validation Loss: 255.56667879105552  valid acc: 0.9783333333333334  train Acc: 0.9778571428571429\n",
      "epoch: 673 training Loss: 1138.7437546621097 validation Loss: 255.37312734886544  valid acc: 0.9783333333333334  train Acc: 0.9778571428571429\n",
      "epoch: 674 training Loss: 1137.8297862346806 validation Loss: 255.1798943591014  valid acc: 0.9783333333333334  train Acc: 0.9778571428571429\n",
      "epoch: 675 training Loss: 1136.9173652137874 validation Loss: 254.98697907105196  valid acc: 0.9783333333333334  train Acc: 0.9778571428571429\n",
      "epoch: 676 training Loss: 1136.0064878734843 validation Loss: 254.79438073619775  valid acc: 0.9783333333333334  train Acc: 0.9778571428571429\n",
      "epoch: 677 training Loss: 1135.097150498893 validation Loss: 254.60209860820447  valid acc: 0.9783333333333334  train Acc: 0.9782142857142857\n",
      "epoch: 678 training Loss: 1134.1893493861671 validation Loss: 254.4101319429156  valid acc: 0.9783333333333334  train Acc: 0.9785714285714285\n",
      "epoch: 679 training Loss: 1133.2830808424555 validation Loss: 254.21847999834517  valid acc: 0.9783333333333334  train Acc: 0.9785714285714285\n",
      "epoch: 680 training Loss: 1132.378341185865 validation Loss: 254.0271420346706  valid acc: 0.9783333333333334  train Acc: 0.9785714285714285\n",
      "epoch: 681 training Loss: 1131.4751267454235 validation Loss: 253.8361173142255  valid acc: 0.9783333333333334  train Acc: 0.9785714285714285\n",
      "epoch: 682 training Loss: 1130.5734338610437 validation Loss: 253.6454051014928  valid acc: 0.9783333333333334  train Acc: 0.9789285714285715\n",
      "epoch: 683 training Loss: 1129.6732588834861 validation Loss: 253.45500466309736  valid acc: 0.9783333333333334  train Acc: 0.9789285714285715\n",
      "epoch: 684 training Loss: 1128.7745981743249 validation Loss: 253.26491526779907  valid acc: 0.9783333333333334  train Acc: 0.9792857142857143\n",
      "epoch: 685 training Loss: 1127.8774481059086 validation Loss: 253.07513618648562  valid acc: 0.9783333333333334  train Acc: 0.9792857142857143\n",
      "epoch: 686 training Loss: 1126.9818050613262 validation Loss: 252.88566669216584  valid acc: 0.9783333333333334  train Acc: 0.9792857142857143\n",
      "epoch: 687 training Loss: 1126.0876654343704 validation Loss: 252.6965060599623  valid acc: 0.9783333333333334  train Acc: 0.9792857142857143\n",
      "epoch: 688 training Loss: 1125.195025629503 validation Loss: 252.50765356710454  valid acc: 0.9783333333333334  train Acc: 0.9792857142857143\n",
      "epoch: 689 training Loss: 1124.3038820618174 validation Loss: 252.31910849292217  valid acc: 0.9783333333333334  train Acc: 0.9792857142857143\n",
      "epoch: 690 training Loss: 1123.4142311570045 validation Loss: 252.13087011883778  valid acc: 0.9783333333333334  train Acc: 0.9792857142857143\n",
      "epoch: 691 training Loss: 1122.5260693513164 validation Loss: 251.94293772836016  valid acc: 0.98  train Acc: 0.9792857142857143\n",
      "epoch: 692 training Loss: 1121.6393930915322 validation Loss: 251.75531060707743  valid acc: 0.98  train Acc: 0.9792857142857143\n",
      "epoch: 693 training Loss: 1120.754198834921 validation Loss: 251.5679880426501  valid acc: 0.98  train Acc: 0.9792857142857143\n",
      "epoch: 694 training Loss: 1119.870483049209 validation Loss: 251.3809693248042  valid acc: 0.98  train Acc: 0.9792857142857143\n",
      "epoch: 695 training Loss: 1118.988242212543 validation Loss: 251.19425374532477  valid acc: 0.98  train Acc: 0.9796428571428571\n",
      "epoch: 696 training Loss: 1118.1074728134563 validation Loss: 251.0078405980487  valid acc: 0.98  train Acc: 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 697 training Loss: 1117.228171350834 validation Loss: 250.8217291788581  valid acc: 0.98  train Acc: 0.98\n",
      "epoch: 698 training Loss: 1116.350334333878 validation Loss: 250.6359187856738  valid acc: 0.98  train Acc: 0.98\n",
      "epoch: 699 training Loss: 1115.473958282072 validation Loss: 250.4504087184482  valid acc: 0.98  train Acc: 0.98\n",
      "epoch: 700 training Loss: 1114.59903972515 validation Loss: 250.2651982791589  valid acc: 0.98  train Acc: 0.98\n",
      "epoch: 701 training Loss: 1113.7255752030587 validation Loss: 250.08028677180207  valid acc: 0.98  train Acc: 0.98\n",
      "epoch: 702 training Loss: 1112.8535612659248 validation Loss: 249.8956735023855  valid acc: 0.98  train Acc: 0.98\n",
      "epoch: 703 training Loss: 1111.9829944740213 validation Loss: 249.71135777892226  valid acc: 0.98  train Acc: 0.98\n",
      "epoch: 704 training Loss: 1111.113871397733 validation Loss: 249.52733891142387  valid acc: 0.98  train Acc: 0.98\n",
      "epoch: 705 training Loss: 1110.2461886175233 validation Loss: 249.34361621189407  valid acc: 0.98  train Acc: 0.9803571428571428\n",
      "epoch: 706 training Loss: 1109.3799427239012 validation Loss: 249.16018899432186  valid acc: 0.98  train Acc: 0.9807142857142858\n",
      "epoch: 707 training Loss: 1108.5151303173857 validation Loss: 248.97705657467523  valid acc: 0.98  train Acc: 0.9807142857142858\n",
      "epoch: 708 training Loss: 1107.6517480084738 validation Loss: 248.79421827089465  valid acc: 0.98  train Acc: 0.9807142857142858\n",
      "epoch: 709 training Loss: 1106.7897924176082 validation Loss: 248.61167340288642  valid acc: 0.98  train Acc: 0.9807142857142858\n",
      "epoch: 710 training Loss: 1105.9292601751422 validation Loss: 248.42942129251628  valid acc: 0.98  train Acc: 0.9807142857142858\n",
      "epoch: 711 training Loss: 1105.0701479213076 validation Loss: 248.24746126360313  valid acc: 0.98  train Acc: 0.9807142857142858\n",
      "epoch: 712 training Loss: 1104.2124523061811 validation Loss: 248.06579264191234  valid acc: 0.98  train Acc: 0.9807142857142858\n",
      "epoch: 713 training Loss: 1103.356169989654 validation Loss: 247.88441475514952  valid acc: 0.98  train Acc: 0.9807142857142858\n",
      "epoch: 714 training Loss: 1102.5012976413955 validation Loss: 247.70332693295418  valid acc: 0.98  train Acc: 0.9810714285714286\n",
      "epoch: 715 training Loss: 1101.647831940823 validation Loss: 247.52252850689325  valid acc: 0.98  train Acc: 0.9810714285714286\n",
      "epoch: 716 training Loss: 1100.7957695770688 validation Loss: 247.34201881045482  valid acc: 0.98  train Acc: 0.9810714285714286\n",
      "epoch: 717 training Loss: 1099.9451072489478 validation Loss: 247.1617971790418  valid acc: 0.98  train Acc: 0.9810714285714286\n",
      "epoch: 718 training Loss: 1099.095841664925 validation Loss: 246.98186294996572  valid acc: 0.98  train Acc: 0.9810714285714286\n",
      "epoch: 719 training Loss: 1098.2479695430843 validation Loss: 246.80221546244033  valid acc: 0.98  train Acc: 0.9814285714285714\n",
      "epoch: 720 training Loss: 1097.4014876110946 validation Loss: 246.6228540575755  valid acc: 0.98  train Acc: 0.9814285714285714\n",
      "epoch: 721 training Loss: 1096.5563926061793 validation Loss: 246.4437780783709  valid acc: 0.98  train Acc: 0.9814285714285714\n",
      "epoch: 722 training Loss: 1095.712681275085 validation Loss: 246.2649868697098  valid acc: 0.98  train Acc: 0.9817857142857143\n",
      "epoch: 723 training Loss: 1094.8703503740485 validation Loss: 246.0864797783529  valid acc: 0.98  train Acc: 0.9817857142857143\n",
      "epoch: 724 training Loss: 1094.029396668766 validation Loss: 245.9082561529323  valid acc: 0.98  train Acc: 0.9817857142857143\n",
      "epoch: 725 training Loss: 1093.1898169343608 validation Loss: 245.7303153439451  valid acc: 0.98  train Acc: 0.9817857142857143\n",
      "epoch: 726 training Loss: 1092.3516079553538 validation Loss: 245.55265670374766  valid acc: 0.98  train Acc: 0.9817857142857143\n",
      "epoch: 727 training Loss: 1091.5147665256295 validation Loss: 245.37527958654906  valid acc: 0.98  train Acc: 0.9817857142857143\n",
      "epoch: 728 training Loss: 1090.6792894484074 validation Loss: 245.1981833484054  valid acc: 0.98  train Acc: 0.9817857142857143\n",
      "epoch: 729 training Loss: 1089.8451735362094 validation Loss: 245.02136734721358  valid acc: 0.98  train Acc: 0.9821428571428571\n",
      "epoch: 730 training Loss: 1089.0124156108302 validation Loss: 244.8448309427053  valid acc: 0.98  train Acc: 0.9821428571428571\n",
      "epoch: 731 training Loss: 1088.1810125033048 validation Loss: 244.66857349644098  valid acc: 0.98  train Acc: 0.9821428571428571\n",
      "epoch: 732 training Loss: 1087.3509610538788 validation Loss: 244.49259437180388  valid acc: 0.98  train Acc: 0.9821428571428571\n",
      "epoch: 733 training Loss: 1086.5222581119783 validation Loss: 244.31689293399413  valid acc: 0.98  train Acc: 0.9821428571428571\n",
      "epoch: 734 training Loss: 1085.6949005361785 validation Loss: 244.1414685500228  valid acc: 0.98  train Acc: 0.9821428571428571\n",
      "epoch: 735 training Loss: 1084.8688851941738 validation Loss: 243.9663205887058  valid acc: 0.98  train Acc: 0.9821428571428571\n",
      "epoch: 736 training Loss: 1084.0442089627463 validation Loss: 243.79144842065807  valid acc: 0.98  train Acc: 0.9821428571428571\n",
      "epoch: 737 training Loss: 1083.2208687277384 validation Loss: 243.61685141828787  valid acc: 0.98  train Acc: 0.9821428571428571\n",
      "epoch: 738 training Loss: 1082.3988613840202 validation Loss: 243.44252895579072  valid acc: 0.98  train Acc: 0.9821428571428571\n",
      "epoch: 739 training Loss: 1081.5781838354596 validation Loss: 243.2684804091435  valid acc: 0.98  train Acc: 0.9825\n",
      "epoch: 740 training Loss: 1080.7588329948949 validation Loss: 243.09470515609885  valid acc: 0.98  train Acc: 0.9825\n",
      "epoch: 741 training Loss: 1079.940805784102 validation Loss: 242.92120257617918  valid acc: 0.98  train Acc: 0.9825\n",
      "epoch: 742 training Loss: 1079.124099133766 validation Loss: 242.747972050671  valid acc: 0.98  train Acc: 0.9825\n",
      "epoch: 743 training Loss: 1078.308709983453 validation Loss: 242.5750129626191  valid acc: 0.98  train Acc: 0.9825\n",
      "epoch: 744 training Loss: 1077.4946352815787 validation Loss: 242.40232469682076  valid acc: 0.98  train Acc: 0.9825\n",
      "epoch: 745 training Loss: 1076.6818719853795 validation Loss: 242.22990663982011  valid acc: 0.98  train Acc: 0.9825\n",
      "epoch: 746 training Loss: 1075.8704170608842 validation Loss: 242.05775817990252  valid acc: 0.98  train Acc: 0.9825\n",
      "epoch: 747 training Loss: 1075.060267482884 validation Loss: 241.88587870708858  valid acc: 0.98  train Acc: 0.9825\n",
      "epoch: 748 training Loss: 1074.2514202349034 validation Loss: 241.7142676131288  valid acc: 0.98  train Acc: 0.9825\n",
      "epoch: 749 training Loss: 1073.4438723091723 validation Loss: 241.54292429149794  valid acc: 0.98  train Acc: 0.9825\n",
      "epoch: 750 training Loss: 1072.637620706596 validation Loss: 241.37184813738895  valid acc: 0.98  train Acc: 0.9825\n",
      "epoch: 751 training Loss: 1071.8326624367267 validation Loss: 241.2010385477079  valid acc: 0.98  train Acc: 0.9825\n",
      "epoch: 752 training Loss: 1071.0289945177356 validation Loss: 241.03049492106817  valid acc: 0.98  train Acc: 0.9825\n",
      "epoch: 753 training Loss: 1070.2266139763842 validation Loss: 240.8602166577848  valid acc: 0.98  train Acc: 0.9825\n",
      "epoch: 754 training Loss: 1069.4255178479948 validation Loss: 240.69020315986918  valid acc: 0.98  train Acc: 0.9825\n",
      "epoch: 755 training Loss: 1068.6257031764235 validation Loss: 240.52045383102325  valid acc: 0.98  train Acc: 0.9828571428571429\n",
      "epoch: 756 training Loss: 1067.8271670140311 validation Loss: 240.35096807663413  valid acc: 0.98  train Acc: 0.9828571428571429\n",
      "epoch: 757 training Loss: 1067.0299064216563 validation Loss: 240.1817453037687  valid acc: 0.98  train Acc: 0.9832142857142857\n",
      "epoch: 758 training Loss: 1066.2339184685854 validation Loss: 240.01278492116802  valid acc: 0.98  train Acc: 0.9835714285714285\n",
      "epoch: 759 training Loss: 1065.4392002325267 validation Loss: 239.84408633924193  valid acc: 0.98  train Acc: 0.9835714285714285\n",
      "epoch: 760 training Loss: 1064.645748799581 validation Loss: 239.67564897006355  valid acc: 0.98  train Acc: 0.9835714285714285\n",
      "epoch: 761 training Loss: 1063.853561264215 validation Loss: 239.50747222736396  valid acc: 0.98  train Acc: 0.9835714285714285\n",
      "epoch: 762 training Loss: 1063.0626347292332 validation Loss: 239.3395555265268  valid acc: 0.98  train Acc: 0.9835714285714285\n",
      "epoch: 763 training Loss: 1062.2729663057507 validation Loss: 239.1718982845828  valid acc: 0.98  train Acc: 0.9839285714285714\n",
      "epoch: 764 training Loss: 1061.484553113164 validation Loss: 239.00449992020452  valid acc: 0.98  train Acc: 0.9839285714285714\n",
      "epoch: 765 training Loss: 1060.6973922791276 validation Loss: 238.837359853701  valid acc: 0.98  train Acc: 0.9839285714285714\n",
      "epoch: 766 training Loss: 1059.9114809395219 validation Loss: 238.67047750701244  valid acc: 0.98  train Acc: 0.9839285714285714\n",
      "epoch: 767 training Loss: 1059.12681623843 validation Loss: 238.50385230370475  valid acc: 0.98  train Acc: 0.9842857142857143\n",
      "epoch: 768 training Loss: 1058.3433953281083 validation Loss: 238.33748366896458  valid acc: 0.98  train Acc: 0.9842857142857143\n",
      "epoch: 769 training Loss: 1057.5612153689603 validation Loss: 238.17137102959373  valid acc: 0.98  train Acc: 0.9842857142857143\n",
      "epoch: 770 training Loss: 1056.7802735295113 validation Loss: 238.00551381400425  valid acc: 0.98  train Acc: 0.9842857142857143\n",
      "epoch: 771 training Loss: 1056.0005669863776 validation Loss: 237.83991145221273  valid acc: 0.98  train Acc: 0.9842857142857143\n",
      "epoch: 772 training Loss: 1055.2220929242449 validation Loss: 237.6745633758357  valid acc: 0.98  train Acc: 0.9842857142857143\n",
      "epoch: 773 training Loss: 1054.444848535837 validation Loss: 237.5094690180839  valid acc: 0.98  train Acc: 0.9842857142857143\n",
      "epoch: 774 training Loss: 1053.6688310218933 validation Loss: 237.34462781375737  valid acc: 0.98  train Acc: 0.9842857142857143\n",
      "epoch: 775 training Loss: 1052.8940375911393 validation Loss: 237.1800391992403  valid acc: 0.98  train Acc: 0.9842857142857143\n",
      "epoch: 776 training Loss: 1052.1204654602625 validation Loss: 237.01570261249577  valid acc: 0.98  train Acc: 0.9842857142857143\n",
      "epoch: 777 training Loss: 1051.3481118538853 validation Loss: 236.85161749306093  valid acc: 0.98  train Acc: 0.9846428571428572\n",
      "epoch: 778 training Loss: 1050.576974004538 validation Loss: 236.68778328204138  valid acc: 0.98  train Acc: 0.9846428571428572\n",
      "epoch: 779 training Loss: 1049.8070491526348 validation Loss: 236.52419942210662  valid acc: 0.9816666666666667  train Acc: 0.9846428571428572\n",
      "epoch: 780 training Loss: 1049.038334546446 validation Loss: 236.36086535748456  valid acc: 0.9816666666666667  train Acc: 0.9846428571428572\n",
      "epoch: 781 training Loss: 1048.2708274420738 validation Loss: 236.19778053395686  valid acc: 0.9816666666666667  train Acc: 0.9846428571428572\n",
      "epoch: 782 training Loss: 1047.5045251034248 validation Loss: 236.03494439885367  valid acc: 0.9816666666666667  train Acc: 0.9846428571428572\n",
      "epoch: 783 training Loss: 1046.7394248021856 validation Loss: 235.87235640104848  valid acc: 0.9816666666666667  train Acc: 0.9846428571428572\n",
      "epoch: 784 training Loss: 1045.9755238177975 validation Loss: 235.7100159909536  valid acc: 0.9816666666666667  train Acc: 0.9846428571428572\n",
      "epoch: 785 training Loss: 1045.2128194374293 validation Loss: 235.54792262051458  valid acc: 0.9816666666666667  train Acc: 0.9846428571428572\n",
      "epoch: 786 training Loss: 1044.4513089559537 validation Loss: 235.38607574320582  valid acc: 0.9816666666666667  train Acc: 0.9846428571428572\n",
      "epoch: 787 training Loss: 1043.6909896759216 validation Loss: 235.22447481402526  valid acc: 0.9816666666666667  train Acc: 0.985\n",
      "epoch: 788 training Loss: 1042.9318589075365 validation Loss: 235.06311928948952  valid acc: 0.9816666666666667  train Acc: 0.985\n",
      "epoch: 789 training Loss: 1042.1739139686292 validation Loss: 234.9020086276291  valid acc: 0.9816666666666667  train Acc: 0.985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 790 training Loss: 1041.417152184634 validation Loss: 234.74114228798337  valid acc: 0.9816666666666667  train Acc: 0.985\n",
      "epoch: 791 training Loss: 1040.6615708885613 validation Loss: 234.5805197315958  valid acc: 0.9816666666666667  train Acc: 0.985\n",
      "epoch: 792 training Loss: 1039.9071674209758 validation Loss: 234.42014042100914  valid acc: 0.9816666666666667  train Acc: 0.985\n",
      "epoch: 793 training Loss: 1039.1539391299702 validation Loss: 234.2600038202603  valid acc: 0.9816666666666667  train Acc: 0.985\n",
      "epoch: 794 training Loss: 1038.40188337114 validation Loss: 234.10010939487583  valid acc: 0.9816666666666667  train Acc: 0.985\n",
      "epoch: 795 training Loss: 1037.6509975075596 validation Loss: 233.9404566118671  valid acc: 0.9816666666666667  train Acc: 0.985\n",
      "epoch: 796 training Loss: 1036.9012789097585 validation Loss: 233.7810449397252  valid acc: 0.9816666666666667  train Acc: 0.985\n",
      "epoch: 797 training Loss: 1036.1527249556952 validation Loss: 233.62187384841667  valid acc: 0.9816666666666667  train Acc: 0.9853571428571428\n",
      "epoch: 798 training Loss: 1035.4053330307345 validation Loss: 233.46294280937818  valid acc: 0.9833333333333333  train Acc: 0.9853571428571428\n",
      "epoch: 799 training Loss: 1034.6591005276223 validation Loss: 233.30425129551233  valid acc: 0.9833333333333333  train Acc: 0.9853571428571428\n",
      "epoch: 800 training Loss: 1033.9140248464614 validation Loss: 233.1457987811825  valid acc: 0.9833333333333333  train Acc: 0.9857142857142858\n",
      "epoch: 801 training Loss: 1033.1701033946888 validation Loss: 232.9875847422084  valid acc: 0.9833333333333333  train Acc: 0.9860714285714286\n",
      "epoch: 802 training Loss: 1032.4273335870498 validation Loss: 232.8296086558613  valid acc: 0.9833333333333333  train Acc: 0.9860714285714286\n",
      "epoch: 803 training Loss: 1031.6857128455754 validation Loss: 232.67187000085934  valid acc: 0.9833333333333333  train Acc: 0.9860714285714286\n",
      "epoch: 804 training Loss: 1030.9452385995576 validation Loss: 232.51436825736286  valid acc: 0.9833333333333333  train Acc: 0.9860714285714286\n",
      "epoch: 805 training Loss: 1030.2059082855264 validation Loss: 232.35710290696989  valid acc: 0.9833333333333333  train Acc: 0.9860714285714286\n",
      "epoch: 806 training Loss: 1029.4677193472266 validation Loss: 232.2000734327113  valid acc: 0.9833333333333333  train Acc: 0.9860714285714286\n",
      "epoch: 807 training Loss: 1028.7306692355926 validation Loss: 232.04327931904638  valid acc: 0.9833333333333333  train Acc: 0.9860714285714286\n",
      "epoch: 808 training Loss: 1027.994755408726 validation Loss: 231.88672005185828  valid acc: 0.9833333333333333  train Acc: 0.9860714285714286\n",
      "epoch: 809 training Loss: 1027.2599753318727 validation Loss: 231.73039511844917  valid acc: 0.9833333333333333  train Acc: 0.9860714285714286\n",
      "epoch: 810 training Loss: 1026.526326477399 validation Loss: 231.57430400753603  valid acc: 0.9833333333333333  train Acc: 0.9860714285714286\n",
      "epoch: 811 training Loss: 1025.7938063247675 validation Loss: 231.41844620924576  valid acc: 0.9833333333333333  train Acc: 0.9860714285714286\n",
      "epoch: 812 training Loss: 1025.062412360515 validation Loss: 231.262821215111  valid acc: 0.9833333333333333  train Acc: 0.9860714285714286\n",
      "epoch: 813 training Loss: 1024.3321420782304 validation Loss: 231.10742851806532  valid acc: 0.9833333333333333  train Acc: 0.9860714285714286\n",
      "epoch: 814 training Loss: 1023.6029929785293 validation Loss: 230.95226761243885  valid acc: 0.9833333333333333  train Acc: 0.9860714285714286\n",
      "epoch: 815 training Loss: 1022.8749625690328 validation Loss: 230.79733799395387  valid acc: 0.985  train Acc: 0.9860714285714286\n",
      "epoch: 816 training Loss: 1022.1480483643444 validation Loss: 230.6426391597202  valid acc: 0.985  train Acc: 0.9860714285714286\n",
      "epoch: 817 training Loss: 1021.4222478860269 validation Loss: 230.48817060823086  valid acc: 0.985  train Acc: 0.9860714285714286\n",
      "epoch: 818 training Loss: 1020.6975586625801 validation Loss: 230.33393183935763  valid acc: 0.985  train Acc: 0.9860714285714286\n",
      "epoch: 819 training Loss: 1019.973978229418 validation Loss: 230.1799223543465  valid acc: 0.985  train Acc: 0.9860714285714286\n",
      "epoch: 820 training Loss: 1019.2515041288464 validation Loss: 230.02614165581352  valid acc: 0.985  train Acc: 0.9860714285714286\n",
      "epoch: 821 training Loss: 1018.5301339100404 validation Loss: 229.87258924774005  valid acc: 0.985  train Acc: 0.9860714285714286\n",
      "epoch: 822 training Loss: 1017.8098651290225 validation Loss: 229.7192646354688  valid acc: 0.985  train Acc: 0.9860714285714286\n",
      "epoch: 823 training Loss: 1017.0906953486394 validation Loss: 229.5661673256992  valid acc: 0.985  train Acc: 0.9860714285714286\n",
      "epoch: 824 training Loss: 1016.3726221385409 validation Loss: 229.41329682648313  valid acc: 0.985  train Acc: 0.9860714285714286\n",
      "epoch: 825 training Loss: 1015.6556430751571 validation Loss: 229.26065264722055  valid acc: 0.985  train Acc: 0.9860714285714286\n",
      "epoch: 826 training Loss: 1014.9397557416762 validation Loss: 229.1082342986553  valid acc: 0.985  train Acc: 0.9860714285714286\n",
      "epoch: 827 training Loss: 1014.2249577280237 validation Loss: 228.9560412928708  valid acc: 0.985  train Acc: 0.9860714285714286\n",
      "epoch: 828 training Loss: 1013.5112466308393 validation Loss: 228.8040731432855  valid acc: 0.985  train Acc: 0.9860714285714286\n",
      "epoch: 829 training Loss: 1012.7986200534555 validation Loss: 228.65232936464906  valid acc: 0.985  train Acc: 0.9860714285714286\n",
      "epoch: 830 training Loss: 1012.087075605876 validation Loss: 228.5008094730377  valid acc: 0.985  train Acc: 0.9860714285714286\n",
      "epoch: 831 training Loss: 1011.376610904754 validation Loss: 228.34951298585008  valid acc: 0.985  train Acc: 0.9860714285714286\n",
      "epoch: 832 training Loss: 1010.6672235733708 validation Loss: 228.1984394218034  valid acc: 0.985  train Acc: 0.9860714285714286\n",
      "epoch: 833 training Loss: 1009.9589112416144 validation Loss: 228.04758830092862  valid acc: 0.985  train Acc: 0.9864285714285714\n",
      "epoch: 834 training Loss: 1009.2516715459572 validation Loss: 227.89695914456672  valid acc: 0.985  train Acc: 0.9864285714285714\n",
      "epoch: 835 training Loss: 1008.5455021294358 validation Loss: 227.74655147536433  valid acc: 0.985  train Acc: 0.9864285714285714\n",
      "epoch: 836 training Loss: 1007.8404006416292 validation Loss: 227.59636481726957  valid acc: 0.985  train Acc: 0.9871428571428571\n",
      "epoch: 837 training Loss: 1007.136364738638 validation Loss: 227.44639869552805  valid acc: 0.985  train Acc: 0.9871428571428571\n",
      "epoch: 838 training Loss: 1006.4333920830625 validation Loss: 227.29665263667846  valid acc: 0.985  train Acc: 0.9871428571428571\n",
      "epoch: 839 training Loss: 1005.7314803439822 validation Loss: 227.14712616854877  valid acc: 0.985  train Acc: 0.9871428571428571\n",
      "epoch: 840 training Loss: 1005.0306271969351 validation Loss: 226.9978188202518  valid acc: 0.985  train Acc: 0.9871428571428571\n",
      "epoch: 841 training Loss: 1004.3308303238966 validation Loss: 226.8487301221815  valid acc: 0.985  train Acc: 0.9871428571428571\n",
      "epoch: 842 training Loss: 1003.6320874132581 validation Loss: 226.69985960600854  valid acc: 0.985  train Acc: 0.9871428571428571\n",
      "epoch: 843 training Loss: 1002.9343961598074 validation Loss: 226.55120680467638  valid acc: 0.985  train Acc: 0.9871428571428571\n",
      "epoch: 844 training Loss: 1002.2377542647068 validation Loss: 226.4027712523973  valid acc: 0.985  train Acc: 0.9875\n",
      "epoch: 845 training Loss: 1001.5421594354741 validation Loss: 226.25455248464817  valid acc: 0.985  train Acc: 0.9875\n",
      "epoch: 846 training Loss: 1000.8476093859597 validation Loss: 226.1065500381667  valid acc: 0.985  train Acc: 0.9875\n",
      "epoch: 847 training Loss: 1000.154101836329 validation Loss: 225.95876345094712  valid acc: 0.985  train Acc: 0.9875\n",
      "epoch: 848 training Loss: 999.4616345130396 validation Loss: 225.81119226223652  valid acc: 0.985  train Acc: 0.9875\n",
      "epoch: 849 training Loss: 998.7702051488226 validation Loss: 225.6638360125305  valid acc: 0.985  train Acc: 0.9875\n",
      "epoch: 850 training Loss: 998.0798114826614 validation Loss: 225.5166942435697  valid acc: 0.985  train Acc: 0.9875\n",
      "epoch: 851 training Loss: 997.3904512597724 validation Loss: 225.36976649833525  valid acc: 0.985  train Acc: 0.9875\n",
      "epoch: 852 training Loss: 996.7021222315839 validation Loss: 225.22305232104534  valid acc: 0.985  train Acc: 0.9878571428571429\n",
      "epoch: 853 training Loss: 996.0148221557171 validation Loss: 225.07655125715107  valid acc: 0.985  train Acc: 0.9878571428571429\n",
      "epoch: 854 training Loss: 995.3285487959652 validation Loss: 224.93026285333252  valid acc: 0.985  train Acc: 0.9878571428571429\n",
      "epoch: 855 training Loss: 994.6432999222743 validation Loss: 224.7841866574949  valid acc: 0.985  train Acc: 0.9882142857142857\n",
      "epoch: 856 training Loss: 993.9590733107235 validation Loss: 224.63832221876473  valid acc: 0.985  train Acc: 0.9882142857142857\n",
      "epoch: 857 training Loss: 993.2758667435046 validation Loss: 224.49266908748592  valid acc: 0.985  train Acc: 0.9882142857142857\n",
      "epoch: 858 training Loss: 992.5936780089033 validation Loss: 224.34722681521595  valid acc: 0.985  train Acc: 0.9882142857142857\n",
      "epoch: 859 training Loss: 991.9125049012787 validation Loss: 224.20199495472184  valid acc: 0.985  train Acc: 0.9882142857142857\n",
      "epoch: 860 training Loss: 991.2323452210443 validation Loss: 224.05697305997668  valid acc: 0.985  train Acc: 0.9882142857142857\n",
      "epoch: 861 training Loss: 990.5531967746485 validation Loss: 223.9121606861554  valid acc: 0.985  train Acc: 0.9882142857142857\n",
      "epoch: 862 training Loss: 989.8750573745555 validation Loss: 223.76755738963132  valid acc: 0.985  train Acc: 0.9885714285714285\n",
      "epoch: 863 training Loss: 989.197924839225 validation Loss: 223.62316272797221  valid acc: 0.985  train Acc: 0.9885714285714285\n",
      "epoch: 864 training Loss: 988.5217969930943 validation Loss: 223.47897625993647  valid acc: 0.985  train Acc: 0.9885714285714285\n",
      "epoch: 865 training Loss: 987.8466716665578 validation Loss: 223.33499754546952  valid acc: 0.9866666666666667  train Acc: 0.9885714285714285\n",
      "epoch: 866 training Loss: 987.1725466959489 validation Loss: 223.19122614569983  valid acc: 0.9866666666666667  train Acc: 0.9885714285714285\n",
      "epoch: 867 training Loss: 986.4994199235207 validation Loss: 223.04766162293535  valid acc: 0.9866666666666667  train Acc: 0.9889285714285714\n",
      "epoch: 868 training Loss: 985.8272891974262 validation Loss: 222.9043035406599  valid acc: 0.9866666666666667  train Acc: 0.9889285714285714\n",
      "epoch: 869 training Loss: 985.1561523717012 validation Loss: 222.76115146352907  valid acc: 0.9866666666666667  train Acc: 0.9889285714285714\n",
      "epoch: 870 training Loss: 984.4860073062432 validation Loss: 222.6182049573669  valid acc: 0.9866666666666667  train Acc: 0.9889285714285714\n",
      "epoch: 871 training Loss: 983.8168518667944 validation Loss: 222.4754635891622  valid acc: 0.9866666666666667  train Acc: 0.9889285714285714\n",
      "epoch: 872 training Loss: 983.1486839249227 validation Loss: 222.33292692706434  valid acc: 0.9866666666666667  train Acc: 0.9889285714285714\n",
      "epoch: 873 training Loss: 982.4815013580015 validation Loss: 222.1905945403804  valid acc: 0.9866666666666667  train Acc: 0.9889285714285714\n",
      "epoch: 874 training Loss: 981.8153020491936 validation Loss: 222.04846599957088  valid acc: 0.9866666666666667  train Acc: 0.9889285714285714\n",
      "epoch: 875 training Loss: 981.1500838874307 validation Loss: 221.90654087624642  valid acc: 0.9866666666666667  train Acc: 0.9889285714285714\n",
      "epoch: 876 training Loss: 980.4858447673959 validation Loss: 221.76481874316403  valid acc: 0.9866666666666667  train Acc: 0.9889285714285714\n",
      "epoch: 877 training Loss: 979.8225825895049 validation Loss: 221.6232991742235  valid acc: 0.9866666666666667  train Acc: 0.9889285714285714\n",
      "epoch: 878 training Loss: 979.1602952598881 validation Loss: 221.48198174446378  valid acc: 0.9866666666666667  train Acc: 0.9889285714285714\n",
      "epoch: 879 training Loss: 978.4989806903725 validation Loss: 221.34086603005957  valid acc: 0.9866666666666667  train Acc: 0.9889285714285714\n",
      "epoch: 880 training Loss: 977.8386367984618 validation Loss: 221.1999516083175  valid acc: 0.9866666666666667  train Acc: 0.9889285714285714\n",
      "epoch: 881 training Loss: 977.1792615073211 validation Loss: 221.05923805767276  valid acc: 0.9866666666666667  train Acc: 0.9889285714285714\n",
      "epoch: 882 training Loss: 976.5208527457565 validation Loss: 220.91872495768544  valid acc: 0.9866666666666667  train Acc: 0.9892857142857143\n",
      "epoch: 883 training Loss: 975.8634084481981 validation Loss: 220.7784118890371  valid acc: 0.9866666666666667  train Acc: 0.9892857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 884 training Loss: 975.2069265546822 validation Loss: 220.63829843352718  valid acc: 0.9866666666666667  train Acc: 0.9892857142857143\n",
      "epoch: 885 training Loss: 974.5514050108327 validation Loss: 220.49838417406949  valid acc: 0.9866666666666667  train Acc: 0.9892857142857143\n",
      "epoch: 886 training Loss: 973.896841767844 validation Loss: 220.35866869468867  valid acc: 0.9866666666666667  train Acc: 0.9892857142857143\n",
      "epoch: 887 training Loss: 973.2432347824627 validation Loss: 220.21915158051692  valid acc: 0.9866666666666667  train Acc: 0.9892857142857143\n",
      "epoch: 888 training Loss: 972.5905820169708 validation Loss: 220.07983241779027  valid acc: 0.9866666666666667  train Acc: 0.9892857142857143\n",
      "epoch: 889 training Loss: 971.9388814391668 validation Loss: 219.94071079384526  valid acc: 0.9866666666666667  train Acc: 0.9892857142857143\n",
      "epoch: 890 training Loss: 971.2881310223496 validation Loss: 219.80178629711546  valid acc: 0.9866666666666667  train Acc: 0.9896428571428572\n",
      "epoch: 891 training Loss: 970.6383287452993 validation Loss: 219.66305851712798  valid acc: 0.9866666666666667  train Acc: 0.9896428571428572\n",
      "epoch: 892 training Loss: 969.9894725922614 validation Loss: 219.52452704450016  valid acc: 0.9866666666666667  train Acc: 0.9896428571428572\n",
      "epoch: 893 training Loss: 969.3415605529285 validation Loss: 219.38619147093615  valid acc: 0.9866666666666667  train Acc: 0.9896428571428572\n",
      "epoch: 894 training Loss: 968.6945906224234 validation Loss: 219.2480513892233  valid acc: 0.9866666666666667  train Acc: 0.9896428571428572\n",
      "epoch: 895 training Loss: 968.0485608012814 validation Loss: 219.1101063932291  valid acc: 0.9866666666666667  train Acc: 0.9896428571428572\n",
      "epoch: 896 training Loss: 967.4034690954333 validation Loss: 218.97235607789753  valid acc: 0.9866666666666667  train Acc: 0.9896428571428572\n",
      "epoch: 897 training Loss: 966.7593135161885 validation Loss: 218.83480003924578  valid acc: 0.9866666666666667  train Acc: 0.9896428571428572\n",
      "epoch: 898 training Loss: 966.1160920802182 validation Loss: 218.69743787436104  valid acc: 0.9866666666666667  train Acc: 0.9896428571428572\n",
      "epoch: 899 training Loss: 965.4738028095371 validation Loss: 218.56026918139693  valid acc: 0.9866666666666667  train Acc: 0.9896428571428572\n",
      "epoch: 900 training Loss: 964.8324437314882 validation Loss: 218.4232935595703  valid acc: 0.9866666666666667  train Acc: 0.9896428571428572\n",
      "epoch: 901 training Loss: 964.1920128787243 validation Loss: 218.28651060915797  valid acc: 0.9866666666666667  train Acc: 0.9896428571428572\n",
      "epoch: 902 training Loss: 963.5525082891924 validation Loss: 218.14991993149317  valid acc: 0.9866666666666667  train Acc: 0.9896428571428572\n",
      "epoch: 903 training Loss: 962.9139280061162 validation Loss: 218.01352112896257  valid acc: 0.9866666666666667  train Acc: 0.9896428571428572\n",
      "epoch: 904 training Loss: 962.2762700779797 validation Loss: 217.87731380500276  valid acc: 0.9866666666666667  train Acc: 0.9896428571428572\n",
      "epoch: 905 training Loss: 961.6395325585104 validation Loss: 217.7412975640971  valid acc: 0.9866666666666667  train Acc: 0.9896428571428572\n",
      "epoch: 906 training Loss: 961.0037135066628 validation Loss: 217.60547201177238  valid acc: 0.9866666666666667  train Acc: 0.9896428571428572\n",
      "epoch: 907 training Loss: 960.3688109866018 validation Loss: 217.4698367545957  valid acc: 0.9866666666666667  train Acc: 0.9896428571428572\n",
      "epoch: 908 training Loss: 959.7348230676864 validation Loss: 217.33439140017097  valid acc: 0.9866666666666667  train Acc: 0.9896428571428572\n",
      "epoch: 909 training Loss: 959.101747824453 validation Loss: 217.19913555713606  valid acc: 0.9866666666666667  train Acc: 0.99\n",
      "epoch: 910 training Loss: 958.4695833365992 validation Loss: 217.06406883515925  valid acc: 0.9866666666666667  train Acc: 0.99\n",
      "epoch: 911 training Loss: 957.8383276889674 validation Loss: 216.92919084493616  valid acc: 0.9866666666666667  train Acc: 0.99\n",
      "epoch: 912 training Loss: 957.2079789715287 validation Loss: 216.7945011981866  valid acc: 0.9866666666666667  train Acc: 0.99\n",
      "epoch: 913 training Loss: 956.5785352793666 validation Loss: 216.65999950765124  valid acc: 0.9866666666666667  train Acc: 0.9907142857142858\n",
      "epoch: 914 training Loss: 955.9499947126611 validation Loss: 216.5256853870887  valid acc: 0.9866666666666667  train Acc: 0.9907142857142858\n",
      "epoch: 915 training Loss: 955.3223553766721 validation Loss: 216.3915584512721  valid acc: 0.9866666666666667  train Acc: 0.9907142857142858\n",
      "epoch: 916 training Loss: 954.6956153817239 validation Loss: 216.25761831598595  valid acc: 0.9866666666666667  train Acc: 0.9907142857142858\n",
      "epoch: 917 training Loss: 954.0697728431891 validation Loss: 216.12386459802337  valid acc: 0.9866666666666667  train Acc: 0.9907142857142858\n",
      "epoch: 918 training Loss: 953.4448258814723 validation Loss: 215.99029691518245  valid acc: 0.9866666666666667  train Acc: 0.9907142857142858\n",
      "epoch: 919 training Loss: 952.8207726219948 validation Loss: 215.85691488626338  valid acc: 0.9866666666666667  train Acc: 0.9907142857142858\n",
      "epoch: 920 training Loss: 952.1976111951788 validation Loss: 215.72371813106548  valid acc: 0.9866666666666667  train Acc: 0.9907142857142858\n",
      "epoch: 921 training Loss: 951.5753397364307 validation Loss: 215.59070627038386  valid acc: 0.9866666666666667  train Acc: 0.9907142857142858\n",
      "epoch: 922 training Loss: 950.9539563861272 validation Loss: 215.45787892600646  valid acc: 0.9866666666666667  train Acc: 0.9907142857142858\n",
      "epoch: 923 training Loss: 950.3334592895977 validation Loss: 215.32523572071085  valid acc: 0.9866666666666667  train Acc: 0.9907142857142858\n",
      "epoch: 924 training Loss: 949.7138465971102 validation Loss: 215.19277627826148  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 925 training Loss: 949.0951164638549 validation Loss: 215.06050022340617  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 926 training Loss: 948.4772670499292 validation Loss: 214.92840718187347  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 927 training Loss: 947.860296520322 validation Loss: 214.79649678036935  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 928 training Loss: 947.2442030448988 validation Loss: 214.6647686465745  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 929 training Loss: 946.6289847983859 validation Loss: 214.53322240914085  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 930 training Loss: 946.0146399603549 validation Loss: 214.40185769768902  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 931 training Loss: 945.4011667152087 validation Loss: 214.2706741428051  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 932 training Loss: 944.788563252165 validation Loss: 214.13967137603765  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 933 training Loss: 944.176827765242 validation Loss: 214.00884902989486  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 934 training Loss: 943.5659584532432 validation Loss: 213.8782067378415  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 935 training Loss: 942.955953519742 validation Loss: 213.747744134296  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 936 training Loss: 942.3468111730679 validation Loss: 213.61746085462752  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 937 training Loss: 941.7385296262897 validation Loss: 213.48735653515297  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 938 training Loss: 941.1311070972029 validation Loss: 213.35743081313413  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 939 training Loss: 940.5245418083133 validation Loss: 213.22768332677475  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 940 training Loss: 939.9188319868229 validation Loss: 213.09811371521772  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 941 training Loss: 939.313975864615 validation Loss: 212.96872161854193  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 942 training Loss: 938.70997167824 validation Loss: 212.83950667775974  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 943 training Loss: 938.1068176689001 validation Loss: 212.71046853481377  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 944 training Loss: 937.5045120824358 validation Loss: 212.5816068325743  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 945 training Loss: 936.9030531693103 validation Loss: 212.45292121483632  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 946 training Loss: 936.3024391845959 validation Loss: 212.32441132631664  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 947 training Loss: 935.7026683879592 validation Loss: 212.19607681265123  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 948 training Loss: 935.103739043647 validation Loss: 212.06791732039213  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 949 training Loss: 934.5056494204722 validation Loss: 211.93993249700492  valid acc: 0.9866666666666667  train Acc: 0.9910714285714286\n",
      "epoch: 950 training Loss: 933.9083977917987 validation Loss: 211.8121219908657  valid acc: 0.9866666666666667  train Acc: 0.9914285714285714\n",
      "epoch: 951 training Loss: 933.3119824355288 validation Loss: 211.68448545125847  valid acc: 0.9866666666666667  train Acc: 0.9914285714285714\n",
      "epoch: 952 training Loss: 932.7164016340871 validation Loss: 211.55702252837227  valid acc: 0.9866666666666667  train Acc: 0.9914285714285714\n",
      "epoch: 953 training Loss: 932.1216536744083 validation Loss: 211.42973287329826  valid acc: 0.9866666666666667  train Acc: 0.9914285714285714\n",
      "epoch: 954 training Loss: 931.5277368479219 validation Loss: 211.30261613802733  valid acc: 0.9866666666666667  train Acc: 0.9914285714285714\n",
      "epoch: 955 training Loss: 930.9346494505389 validation Loss: 211.17567197544702  valid acc: 0.9866666666666667  train Acc: 0.9914285714285714\n",
      "epoch: 956 training Loss: 930.3423897826376 validation Loss: 211.0489000393388  valid acc: 0.9866666666666667  train Acc: 0.9914285714285714\n",
      "epoch: 957 training Loss: 929.7509561490494 validation Loss: 210.92229998437557  valid acc: 0.9866666666666667  train Acc: 0.9914285714285714\n",
      "epoch: 958 training Loss: 929.1603468590462 validation Loss: 210.7958714661187  valid acc: 0.9866666666666667  train Acc: 0.9917857142857143\n",
      "epoch: 959 training Loss: 928.5705602263249 validation Loss: 210.66961414101536  valid acc: 0.9866666666666667  train Acc: 0.9917857142857143\n",
      "epoch: 960 training Loss: 927.9815945689948 validation Loss: 210.54352766639596  valid acc: 0.9866666666666667  train Acc: 0.9917857142857143\n",
      "epoch: 961 training Loss: 927.393448209564 validation Loss: 210.41761170047118  valid acc: 0.9866666666666667  train Acc: 0.9917857142857143\n",
      "epoch: 962 training Loss: 926.8061194749248 validation Loss: 210.29186590232965  valid acc: 0.9866666666666667  train Acc: 0.9917857142857143\n",
      "epoch: 963 training Loss: 926.2196066963414 validation Loss: 210.16628993193493  valid acc: 0.9866666666666667  train Acc: 0.9917857142857143\n",
      "epoch: 964 training Loss: 925.633908209435 validation Loss: 210.04088345012298  valid acc: 0.9866666666666667  train Acc: 0.9917857142857143\n",
      "epoch: 965 training Loss: 925.0490223541716 validation Loss: 209.91564611859957  valid acc: 0.9866666666666667  train Acc: 0.9917857142857143\n",
      "epoch: 966 training Loss: 924.4649474748471 validation Loss: 209.79057759993748  valid acc: 0.9866666666666667  train Acc: 0.9921428571428571\n",
      "epoch: 967 training Loss: 923.8816819200756 validation Loss: 209.665677557574  valid acc: 0.9866666666666667  train Acc: 0.9921428571428571\n",
      "epoch: 968 training Loss: 923.2992240427748 validation Loss: 209.54094565580826  valid acc: 0.9866666666666667  train Acc: 0.9921428571428571\n",
      "epoch: 969 training Loss: 922.7175722001529 validation Loss: 209.41638155979848  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 970 training Loss: 922.1367247536957 validation Loss: 209.29198493555953  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 971 training Loss: 921.5566800691535 validation Loss: 209.16775544996023  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 972 training Loss: 920.9774365165267 validation Loss: 209.04369277072072  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 973 training Loss: 920.3989924700545 validation Loss: 208.91979656641007  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 974 training Loss: 919.8213463082006 validation Loss: 208.79606650644342  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 975 training Loss: 919.2444964136404 validation Loss: 208.67250226107961  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 976 training Loss: 918.6684411732479 validation Loss: 208.54910350141859  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 977 training Loss: 918.0931789780834 validation Loss: 208.42586989939866  valid acc: 0.9866666666666667  train Acc: 0.9925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 978 training Loss: 917.5187082233798 validation Loss: 208.30280112779434  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 979 training Loss: 916.9450273085301 validation Loss: 208.1798968602134  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 980 training Loss: 916.3721346370749 validation Loss: 208.05715677109467  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 981 training Loss: 915.8000286166888 validation Loss: 207.93458053570524  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 982 training Loss: 915.2287076591685 validation Loss: 207.81216783013818  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 983 training Loss: 914.6581701804199 validation Loss: 207.68991833130997  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 984 training Loss: 914.088414600445 validation Loss: 207.56783171695787  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 985 training Loss: 913.5194393433304 validation Loss: 207.44590766563775  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 986 training Loss: 912.9512428372333 validation Loss: 207.32414585672115  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 987 training Loss: 912.3838235143699 validation Loss: 207.20254597039332  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 988 training Loss: 911.8171798110029 validation Loss: 207.0811076876504  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 989 training Loss: 911.2513101674291 validation Loss: 206.95983069029714  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 990 training Loss: 910.6862130279667 validation Loss: 206.83871466094433  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 991 training Loss: 910.1218868409433 validation Loss: 206.71775928300661  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 992 training Loss: 909.5583300586832 validation Loss: 206.59696424069966  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 993 training Loss: 908.9955411374957 validation Loss: 206.4763292190382  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 994 training Loss: 908.4335185376626 validation Loss: 206.3558539038333  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 995 training Loss: 907.8722607234258 validation Loss: 206.23553798169004  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 996 training Loss: 907.311766162976 validation Loss: 206.11538114000524  valid acc: 0.9866666666666667  train Acc: 0.9925\n",
      "epoch: 997 training Loss: 906.7520333284392 validation Loss: 205.99538306696485  valid acc: 0.9866666666666667  train Acc: 0.9928571428571429\n",
      "epoch: 998 training Loss: 906.1930606958663 validation Loss: 205.8755434515418  valid acc: 0.9866666666666667  train Acc: 0.9928571428571429\n",
      "epoch: 999 training Loss: 905.6348467452201 validation Loss: 205.7558619834935  valid acc: 0.9866666666666667  train Acc: 0.9928571428571429\n",
      "epoch: 1000 training Loss: 905.0773899603635 validation Loss: 205.63633835335955  valid acc: 0.9866666666666667  train Acc: 0.9932142857142857\n",
      "epoch: 1001 training Loss: 904.5206888290477 validation Loss: 205.5169722524593  valid acc: 0.9866666666666667  train Acc: 0.9935714285714285\n",
      "epoch: 1002 training Loss: 903.9647418429001 validation Loss: 205.3977633728896  valid acc: 0.9866666666666667  train Acc: 0.9935714285714285\n",
      "epoch: 1003 training Loss: 903.4095474974133 validation Loss: 205.2787114075224  valid acc: 0.9866666666666667  train Acc: 0.9935714285714285\n",
      "epoch: 1004 training Loss: 902.8551042919319 validation Loss: 205.1598160500024  valid acc: 0.9866666666666667  train Acc: 0.9935714285714285\n",
      "epoch: 1005 training Loss: 902.3014107296419 validation Loss: 205.0410769947449  valid acc: 0.9866666666666667  train Acc: 0.9935714285714285\n",
      "epoch: 1006 training Loss: 901.7484653175588 validation Loss: 204.92249393693334  valid acc: 0.9866666666666667  train Acc: 0.9935714285714285\n",
      "epoch: 1007 training Loss: 901.1962665665155 validation Loss: 204.80406657251686  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1008 training Loss: 900.6448129911512 validation Loss: 204.6857945982083  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1009 training Loss: 900.0941031098988 validation Loss: 204.5676777114818  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1010 training Loss: 899.5441354449749 validation Loss: 204.44971561057037  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1011 training Loss: 898.9949085223675 validation Loss: 204.33190799446385  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1012 training Loss: 898.4464208718238 validation Loss: 204.2142545629065  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1013 training Loss: 897.89867102684 validation Loss: 204.09675501639472  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1014 training Loss: 897.3516575246497 validation Loss: 203.97940905617497  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1015 training Loss: 896.8053789062112 validation Loss: 203.86221638424132  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1016 training Loss: 896.2598337161985 validation Loss: 203.74517670333333  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1017 training Loss: 895.7150205029878 validation Loss: 203.6282897169338  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1018 training Loss: 895.1709378186478 validation Loss: 203.51155512926658  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1019 training Loss: 894.6275842189275 validation Loss: 203.39497264529427  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1020 training Loss: 894.0849582632459 validation Loss: 203.27854197071616  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1021 training Loss: 893.5430585146803 validation Loss: 203.16226281196583  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1022 training Loss: 893.001883539955 validation Loss: 203.04613487620918  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1023 training Loss: 892.4614319094312 validation Loss: 202.93015787134206  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1024 training Loss: 891.9217021970951 validation Loss: 202.8143315059882  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1025 training Loss: 891.3826929805473 validation Loss: 202.69865548949713  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1026 training Loss: 890.844402840992 validation Loss: 202.58312953194172  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1027 training Loss: 890.3068303632258 validation Loss: 202.46775334411635  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1028 training Loss: 889.7699741356269 validation Loss: 202.3525266375346  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1029 training Loss: 889.2338327501444 validation Loss: 202.2374491244271  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1030 training Loss: 888.698404802288 validation Loss: 202.12252051773947  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1031 training Loss: 888.1636888911156 validation Loss: 202.0077405311302  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1032 training Loss: 887.6296836192249 validation Loss: 201.8931088789684  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1033 training Loss: 887.0963875927409 validation Loss: 201.77862527633187  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1034 training Loss: 886.5637994213063 validation Loss: 201.66428943900493  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1035 training Loss: 886.0319177180702 validation Loss: 201.5501010834762  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1036 training Loss: 885.5007410996777 validation Loss: 201.43605992693676  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1037 training Loss: 884.9702681862605 validation Loss: 201.32216568727773  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1038 training Loss: 884.4404976014241 validation Loss: 201.20841808308865  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1039 training Loss: 883.91142797224 validation Loss: 201.09481683365493  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1040 training Loss: 883.3830579292328 validation Loss: 200.98136165895608  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1041 training Loss: 882.8553861063722 validation Loss: 200.86805227966371  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1042 training Loss: 882.3284111410604 validation Loss: 200.7548884171392  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1043 training Loss: 881.8021316741234 validation Loss: 200.64186979343185  valid acc: 0.9866666666666667  train Acc: 0.9939285714285714\n",
      "epoch: 1044 training Loss: 881.2765463497999 validation Loss: 200.52899613127693  valid acc: 0.9866666666666667  train Acc: 0.9942857142857143\n",
      "epoch: 1045 training Loss: 880.7516538157317 validation Loss: 200.4162671540934  valid acc: 0.9866666666666667  train Acc: 0.9946428571428572\n",
      "epoch: 1046 training Loss: 880.2274527229526 validation Loss: 200.30368258598213  valid acc: 0.9866666666666667  train Acc: 0.9946428571428572\n",
      "epoch: 1047 training Loss: 879.7039417258793 validation Loss: 200.1912421517237  valid acc: 0.9866666666666667  train Acc: 0.9946428571428572\n",
      "epoch: 1048 training Loss: 879.1811194823 validation Loss: 200.07894557677662  valid acc: 0.9866666666666667  train Acc: 0.9946428571428572\n",
      "epoch: 1049 training Loss: 878.6589846533657 validation Loss: 199.96679258727505  valid acc: 0.9866666666666667  train Acc: 0.9946428571428572\n",
      "epoch: 1050 training Loss: 878.137535903579 validation Loss: 199.85478291002698  valid acc: 0.9866666666666667  train Acc: 0.9946428571428572\n",
      "epoch: 1051 training Loss: 877.6167719007844 validation Loss: 199.74291627251233  valid acc: 0.9866666666666667  train Acc: 0.9946428571428572\n",
      "epoch: 1052 training Loss: 877.0966913161592 validation Loss: 199.6311924028807  valid acc: 0.9866666666666667  train Acc: 0.995\n",
      "epoch: 1053 training Loss: 876.5772928242015 validation Loss: 199.51961102994966  valid acc: 0.9866666666666667  train Acc: 0.995\n",
      "epoch: 1054 training Loss: 876.0585751027226 validation Loss: 199.4081718832026  valid acc: 0.9883333333333333  train Acc: 0.995\n",
      "epoch: 1055 training Loss: 875.5405368328352 validation Loss: 199.2968746927869  valid acc: 0.9883333333333333  train Acc: 0.995\n",
      "epoch: 1056 training Loss: 875.023176698945 validation Loss: 199.18571918951193  valid acc: 0.9883333333333333  train Acc: 0.995\n",
      "epoch: 1057 training Loss: 874.5064933887395 validation Loss: 199.074705104847  valid acc: 0.9883333333333333  train Acc: 0.995\n",
      "epoch: 1058 training Loss: 873.9904855931798 validation Loss: 198.9638321709196  valid acc: 0.9883333333333333  train Acc: 0.995\n",
      "epoch: 1059 training Loss: 873.4751520064887 validation Loss: 198.8531001205135  valid acc: 0.9883333333333333  train Acc: 0.995\n",
      "epoch: 1060 training Loss: 872.9604913261433 validation Loss: 198.7425086870664  valid acc: 0.9883333333333333  train Acc: 0.995\n",
      "epoch: 1061 training Loss: 872.4465022528636 validation Loss: 198.63205760466855  valid acc: 0.9883333333333333  train Acc: 0.995\n",
      "epoch: 1062 training Loss: 871.9331834906042 validation Loss: 198.5217466080606  valid acc: 0.9883333333333333  train Acc: 0.995\n",
      "epoch: 1063 training Loss: 871.4205337465426 validation Loss: 198.41157543263154  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1064 training Loss: 870.9085517310722 validation Loss: 198.3015438144171  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1065 training Loss: 870.3972361577904 validation Loss: 198.19165149009768  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1066 training Loss: 869.886585743491 validation Loss: 198.08189819699638  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1067 training Loss: 869.3765992081533 validation Loss: 197.97228367307736  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1068 training Loss: 868.8672752749328 validation Loss: 197.86280765694372  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1069 training Loss: 868.3586126701531 validation Loss: 197.75346988783588  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1070 training Loss: 867.8506101232945 validation Loss: 197.64427010562937  valid acc: 0.99  train Acc: 0.995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1071 training Loss: 867.343266366986 validation Loss: 197.53520805083343  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1072 training Loss: 866.8365801369955 validation Loss: 197.4262834645887  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1073 training Loss: 866.3305501722209 validation Loss: 197.31749608866562  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1074 training Loss: 865.8251752146796 validation Loss: 197.20884566546266  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1075 training Loss: 865.3204540095008 validation Loss: 197.10033193800422  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1076 training Loss: 864.8163853049157 validation Loss: 196.99195464993912  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1077 training Loss: 864.312967852248 validation Loss: 196.88371354553854  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1078 training Loss: 863.8102004059043 validation Loss: 196.77560836969423  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1079 training Loss: 863.3080817233669 validation Loss: 196.66763886791688  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1080 training Loss: 862.8066105651823 validation Loss: 196.55980478633415  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1081 training Loss: 862.3057856949538 validation Loss: 196.45210587168884  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1082 training Loss: 861.8056058793319 validation Loss: 196.34454187133736  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1083 training Loss: 861.3060698880053 validation Loss: 196.2371125332475  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1084 training Loss: 860.8071764936918 validation Loss: 196.1298176059972  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1085 training Loss: 860.30892447213 validation Loss: 196.0226568387723  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1086 training Loss: 859.8113126020692 validation Loss: 195.91562998136504  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1087 training Loss: 859.3143396652617 validation Loss: 195.8087367841722  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1088 training Loss: 858.8180044464534 validation Loss: 195.70197699819335  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1089 training Loss: 858.3223057333747 validation Loss: 195.59535037502917  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1090 training Loss: 857.8272423167327 validation Loss: 195.48885666687968  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1091 training Loss: 857.3328129902011 validation Loss: 195.38249562654238  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1092 training Loss: 856.8390165504129 validation Loss: 195.27626700741064  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1093 training Loss: 856.34585179695 validation Loss: 195.17017056347197  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1094 training Loss: 855.8533175323364 validation Loss: 195.0642060493064  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1095 training Loss: 855.361412562028 validation Loss: 194.95837322008438  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1096 training Loss: 854.870135694405 validation Loss: 194.85267183156554  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1097 training Loss: 854.3794857407624 validation Loss: 194.74710164009676  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1098 training Loss: 853.8894615153026 validation Loss: 194.64166240261036  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1099 training Loss: 853.4000618351258 validation Loss: 194.53635387662274  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1100 training Loss: 852.9112855202222 validation Loss: 194.43117582023226  valid acc: 0.99  train Acc: 0.995\n",
      "epoch: 1101 training Loss: 852.4231313934627 validation Loss: 194.32612799211802  valid acc: 0.99  train Acc: 0.9953571428571428\n",
      "epoch: 1102 training Loss: 851.9355982805919 validation Loss: 194.22121015153778  valid acc: 0.99  train Acc: 0.9953571428571428\n",
      "epoch: 1103 training Loss: 851.448685010218 validation Loss: 194.11642205832658  valid acc: 0.99  train Acc: 0.9953571428571428\n",
      "epoch: 1104 training Loss: 850.9623904138055 validation Loss: 194.01176347289493  valid acc: 0.99  train Acc: 0.9953571428571428\n",
      "epoch: 1105 training Loss: 850.4767133256667 validation Loss: 193.9072341562271  valid acc: 0.99  train Acc: 0.9953571428571428\n",
      "epoch: 1106 training Loss: 849.9916525829526 validation Loss: 193.8028338698797  valid acc: 0.99  train Acc: 0.9953571428571428\n",
      "epoch: 1107 training Loss: 849.5072070256463 validation Loss: 193.69856237597975  valid acc: 0.99  train Acc: 0.9953571428571428\n",
      "epoch: 1108 training Loss: 849.0233754965523 validation Loss: 193.59441943722322  valid acc: 0.99  train Acc: 0.9953571428571428\n",
      "epoch: 1109 training Loss: 848.5401568412904 validation Loss: 193.49040481687337  valid acc: 0.99  train Acc: 0.9953571428571428\n",
      "epoch: 1110 training Loss: 848.0575499082865 validation Loss: 193.38651827875898  valid acc: 0.99  train Acc: 0.9953571428571428\n",
      "epoch: 1111 training Loss: 847.5755535487648 validation Loss: 193.28275958727295  valid acc: 0.99  train Acc: 0.9953571428571428\n",
      "epoch: 1112 training Loss: 847.0941666167386 validation Loss: 193.1791285073705  valid acc: 0.99  train Acc: 0.9953571428571428\n",
      "epoch: 1113 training Loss: 846.6133879690037 validation Loss: 193.07562480456764  valid acc: 0.99  train Acc: 0.9953571428571428\n",
      "epoch: 1114 training Loss: 846.1332164651292 validation Loss: 192.97224824493946  valid acc: 0.99  train Acc: 0.9953571428571428\n",
      "epoch: 1115 training Loss: 845.6536509674497 validation Loss: 192.8689985951187  valid acc: 0.99  train Acc: 0.9953571428571428\n",
      "epoch: 1116 training Loss: 845.1746903410576 validation Loss: 192.765875622294  valid acc: 0.99  train Acc: 0.9953571428571428\n",
      "epoch: 1117 training Loss: 844.6963334537943 validation Loss: 192.66287909420834  valid acc: 0.99  train Acc: 0.9953571428571428\n",
      "epoch: 1118 training Loss: 844.2185791762431 validation Loss: 192.56000877915744  valid acc: 0.99  train Acc: 0.9953571428571428\n",
      "epoch: 1119 training Loss: 843.7414263817204 validation Loss: 192.45726444598827  valid acc: 0.99  train Acc: 0.9957142857142857\n",
      "epoch: 1120 training Loss: 843.2648739462682 validation Loss: 192.35464586409736  valid acc: 0.99  train Acc: 0.9957142857142857\n",
      "epoch: 1121 training Loss: 842.7889207486464 validation Loss: 192.25215280342928  valid acc: 0.99  train Acc: 0.9957142857142857\n",
      "epoch: 1122 training Loss: 842.3135656703242 validation Loss: 192.14978503447497  valid acc: 0.99  train Acc: 0.9957142857142857\n",
      "epoch: 1123 training Loss: 841.8388075954729 validation Loss: 192.04754232827042  valid acc: 0.99  train Acc: 0.9957142857142857\n",
      "epoch: 1124 training Loss: 841.3646454109573 validation Loss: 191.94542445639485  valid acc: 0.99  train Acc: 0.9957142857142857\n",
      "epoch: 1125 training Loss: 840.8910780063289 validation Loss: 191.84343119096923  valid acc: 0.99  train Acc: 0.9957142857142857\n",
      "epoch: 1126 training Loss: 840.4181042738173 validation Loss: 191.74156230465485  valid acc: 0.99  train Acc: 0.9957142857142857\n",
      "epoch: 1127 training Loss: 839.9457231083227 validation Loss: 191.6398175706516  valid acc: 0.99  train Acc: 0.9957142857142857\n",
      "epoch: 1128 training Loss: 839.473933407408 validation Loss: 191.5381967626966  valid acc: 0.99  train Acc: 0.9957142857142857\n",
      "epoch: 1129 training Loss: 839.0027340712916 validation Loss: 191.43669965506257  valid acc: 0.99  train Acc: 0.9960714285714286\n",
      "epoch: 1130 training Loss: 838.5321240028388 validation Loss: 191.33532602255627  valid acc: 0.99  train Acc: 0.9960714285714286\n",
      "epoch: 1131 training Loss: 838.0621021075552 validation Loss: 191.23407564051712  valid acc: 0.99  train Acc: 0.9964285714285714\n",
      "epoch: 1132 training Loss: 837.5926672935783 validation Loss: 191.13294828481543  valid acc: 0.99  train Acc: 0.9964285714285714\n",
      "epoch: 1133 training Loss: 837.1238184716698 validation Loss: 191.03194373185121  valid acc: 0.99  train Acc: 0.9964285714285714\n",
      "epoch: 1134 training Loss: 836.6555545552087 validation Loss: 190.9310617585524  valid acc: 0.99  train Acc: 0.9964285714285714\n",
      "epoch: 1135 training Loss: 836.1878744601834 validation Loss: 190.83030214237345  valid acc: 0.99  train Acc: 0.9964285714285714\n",
      "epoch: 1136 training Loss: 835.7207771051835 validation Loss: 190.72966466129395  valid acc: 0.99  train Acc: 0.9964285714285714\n",
      "epoch: 1137 training Loss: 835.254261411394 validation Loss: 190.62914909381686  valid acc: 0.99  train Acc: 0.9964285714285714\n",
      "epoch: 1138 training Loss: 834.7883263025853 validation Loss: 190.5287552189673  valid acc: 0.9916666666666667  train Acc: 0.9964285714285714\n",
      "epoch: 1139 training Loss: 834.3229707051084 validation Loss: 190.4284828162909  valid acc: 0.9916666666666667  train Acc: 0.9964285714285714\n",
      "epoch: 1140 training Loss: 833.8581935478855 validation Loss: 190.32833166585237  valid acc: 0.9916666666666667  train Acc: 0.9964285714285714\n",
      "epoch: 1141 training Loss: 833.3939937624034 validation Loss: 190.22830154823407  valid acc: 0.9916666666666667  train Acc: 0.9964285714285714\n",
      "epoch: 1142 training Loss: 832.930370282706 validation Loss: 190.12839224453438  valid acc: 0.9916666666666667  train Acc: 0.9964285714285714\n",
      "epoch: 1143 training Loss: 832.4673220453872 validation Loss: 190.0286035363665  valid acc: 0.9916666666666667  train Acc: 0.9964285714285714\n",
      "epoch: 1144 training Loss: 832.0048479895826 validation Loss: 189.9289352058567  valid acc: 0.9916666666666667  train Acc: 0.9964285714285714\n",
      "epoch: 1145 training Loss: 831.5429470569635 validation Loss: 189.8293870356431  valid acc: 0.9916666666666667  train Acc: 0.9964285714285714\n",
      "epoch: 1146 training Loss: 831.0816181917286 validation Loss: 189.72995880887407  valid acc: 0.9933333333333333  train Acc: 0.9964285714285714\n",
      "epoch: 1147 training Loss: 830.6208603405973 validation Loss: 189.6306503092069  valid acc: 0.9933333333333333  train Acc: 0.9964285714285714\n",
      "epoch: 1148 training Loss: 830.1606724528024 validation Loss: 189.53146132080627  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1149 training Loss: 829.701053480082 validation Loss: 189.43239162834277  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1150 training Loss: 829.242002376674 validation Loss: 189.33344101699169  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1151 training Loss: 828.783518099307 validation Loss: 189.23460927243124  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1152 training Loss: 828.3255996071948 validation Loss: 189.13589618084154  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1153 training Loss: 827.8682458620281 validation Loss: 189.03730152890287  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1154 training Loss: 827.4114558279684 validation Loss: 188.93882510379441  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1155 training Loss: 826.9552284716397 validation Loss: 188.84046669319275  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1156 training Loss: 826.4995627621227 validation Loss: 188.74222608527066  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1157 training Loss: 826.0444576709467 validation Loss: 188.64410306869547  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1158 training Loss: 825.5899121720834 validation Loss: 188.5460974326278  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1159 training Loss: 825.1359252419396 validation Loss: 188.4482089667201  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1160 training Loss: 824.68249585935 validation Loss: 188.35043746111543  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1161 training Loss: 824.2296230055706 validation Loss: 188.25278270644577  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1162 training Loss: 823.7773056642718 validation Loss: 188.15524449383105  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1163 training Loss: 823.3255428215311 validation Loss: 188.05782261487732  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1164 training Loss: 822.8743334658266 validation Loss: 187.9605168616758  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1165 training Loss: 822.4236765880298 validation Loss: 187.86332702680133  valid acc: 0.995  train Acc: 0.9967857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1166 training Loss: 821.9735711813996 validation Loss: 187.7662529033108  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1167 training Loss: 821.5240162415744 validation Loss: 187.6692942847423  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1168 training Loss: 821.0750107665656 validation Loss: 187.57245096511332  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1169 training Loss: 820.6265537567518 validation Loss: 187.47572273891956  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1170 training Loss: 820.1786442148706 validation Loss: 187.37910940113363  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1171 training Loss: 819.7312811460129 validation Loss: 187.2826107472037  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1172 training Loss: 819.2844635576158 validation Loss: 187.1862265730521  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1173 training Loss: 818.8381904594562 validation Loss: 187.089956675074  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1174 training Loss: 818.3924608636438 validation Loss: 186.99380085013627  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1175 training Loss: 817.9472737846145 validation Loss: 186.8977588955758  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1176 training Loss: 817.5026282391242 validation Loss: 186.80183060919853  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1177 training Loss: 817.0585232462417 validation Loss: 186.70601578927796  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1178 training Loss: 816.6149578273424 validation Loss: 186.61031423455393  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1179 training Loss: 816.1719310061021 validation Loss: 186.51472574423116  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1180 training Loss: 815.7294418084894 validation Loss: 186.4192501179782  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1181 training Loss: 815.2874892627603 validation Loss: 186.32388715592595  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1182 training Loss: 814.8460723994518 validation Loss: 186.2286366586663  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1183 training Loss: 814.4051902513738 validation Loss: 186.1334984272511  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1184 training Loss: 813.9648418536049 validation Loss: 186.0384722631907  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1185 training Loss: 813.525026243484 validation Loss: 185.94355796845275  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1186 training Loss: 813.0857424606056 validation Loss: 185.84875534546072  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1187 training Loss: 812.6469895468122 validation Loss: 185.75406419709296  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1188 training Loss: 812.2087665461881 validation Loss: 185.65948432668122  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1189 training Loss: 811.7710725050542 validation Loss: 185.56501553800948  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1190 training Loss: 811.3339064719601 validation Loss: 185.47065763531248  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1191 training Loss: 810.8972674976783 validation Loss: 185.37641042327485  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1192 training Loss: 810.461154635199 validation Loss: 185.2822737070295  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1193 training Loss: 810.0255669397225 validation Loss: 185.18824729215663  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1194 training Loss: 809.5905034686532 validation Loss: 185.09433098468224  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1195 training Loss: 809.1559632815943 validation Loss: 185.00052459107707  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1196 training Loss: 808.7219454403403 validation Loss: 184.90682791825546  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1197 training Loss: 808.288449008872 validation Loss: 184.81324077357382  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1198 training Loss: 807.8554730533497 validation Loss: 184.7197629648296  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1199 training Loss: 807.4230166421069 validation Loss: 184.62639430026007  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1200 training Loss: 806.991078845645 validation Loss: 184.53313458854115  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1201 training Loss: 806.559658736626 validation Loss: 184.43998363878586  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1202 training Loss: 806.1287553898678 validation Loss: 184.34694126054364  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1203 training Loss: 805.6983678823374 validation Loss: 184.25400726379874  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1204 training Loss: 805.2684952931443 validation Loss: 184.16118145896914  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1205 training Loss: 804.8391367035358 validation Loss: 184.0684636569053  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1206 training Loss: 804.4102911968901 validation Loss: 183.97585366888916  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1207 training Loss: 803.9819578587105 validation Loss: 183.88335130663276  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1208 training Loss: 803.5541357766195 validation Loss: 183.79095638227702  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1209 training Loss: 803.1268240403529 validation Loss: 183.6986687083907  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1210 training Loss: 802.700021741754 validation Loss: 183.6064880979692  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1211 training Loss: 802.2737279747671 validation Loss: 183.5144143644332  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1212 training Loss: 801.8479418354325 validation Loss: 183.42244732162777  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1213 training Loss: 801.4226624218797 validation Loss: 183.330586783821  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1214 training Loss: 800.997888834323 validation Loss: 183.23883256570286  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1215 training Loss: 800.5736201750537 validation Loss: 183.14718448238415  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1216 training Loss: 800.149855548436 validation Loss: 183.0556423493952  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1217 training Loss: 799.7265940609 validation Loss: 182.9642059826849  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1218 training Loss: 799.303834820937 validation Loss: 182.87287519861917  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1219 training Loss: 798.8815769390932 validation Loss: 182.7816498139804  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1220 training Loss: 798.4598195279633 validation Loss: 182.6905296459657  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1221 training Loss: 798.0385617021865 validation Loss: 182.59951451218623  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1222 training Loss: 797.6178025784388 validation Loss: 182.5086042306658  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1223 training Loss: 797.1975412754294 validation Loss: 182.41779861983977  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1224 training Loss: 796.7777769138931 validation Loss: 182.3270974985541  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1225 training Loss: 796.3585086165859 validation Loss: 182.236500686064  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1226 training Loss: 795.9397355082789 validation Loss: 182.1460080020328  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1227 training Loss: 795.5214567157527 validation Loss: 182.05561926653115  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1228 training Loss: 795.1036713677922 validation Loss: 181.96533430003552  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1229 training Loss: 794.6863785951806 validation Loss: 181.87515292342732  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1230 training Loss: 794.269577530694 validation Loss: 181.7850749579917  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1231 training Loss: 793.853267309096 validation Loss: 181.69510022541647  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1232 training Loss: 793.4374470671318 validation Loss: 181.60522854779106  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1233 training Loss: 793.0221159435231 validation Loss: 181.5154597476053  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1234 training Loss: 792.6072730789625 validation Loss: 181.42579364774843  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1235 training Loss: 792.1929176161084 validation Loss: 181.336230071508  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1236 training Loss: 791.7790486995783 validation Loss: 181.24676884256866  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1237 training Loss: 791.365665475945 validation Loss: 181.15740978501128  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1238 training Loss: 790.9527670937305 validation Loss: 181.06815272331173  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1239 training Loss: 790.5403527033998 validation Loss: 180.97899748233976  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1240 training Loss: 790.128421457357 validation Loss: 180.8899438873581  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1241 training Loss: 789.7169725099393 validation Loss: 180.80099176402132  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1242 training Loss: 789.3060050174108 validation Loss: 180.7121409383746  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1243 training Loss: 788.8955181379589 validation Loss: 180.62339123685294  valid acc: 0.995  train Acc: 0.9967857142857143\n",
      "epoch: 1244 training Loss: 788.4855110316876 validation Loss: 180.5347424862799  valid acc: 0.9966666666666667  train Acc: 0.9967857142857143\n",
      "epoch: 1245 training Loss: 788.0759828606124 validation Loss: 180.44619451386666  valid acc: 0.9966666666666667  train Acc: 0.9967857142857143\n",
      "epoch: 1246 training Loss: 787.666932788656 validation Loss: 180.35774714721083  valid acc: 0.9966666666666667  train Acc: 0.9967857142857143\n",
      "epoch: 1247 training Loss: 787.2583599816418 validation Loss: 180.26940021429556  valid acc: 0.9966666666666667  train Acc: 0.9967857142857143\n",
      "epoch: 1248 training Loss: 786.8502636072892 validation Loss: 180.18115354348848  valid acc: 0.9966666666666667  train Acc: 0.9967857142857143\n",
      "epoch: 1249 training Loss: 786.4426428352092 validation Loss: 180.0930069635404  valid acc: 0.9966666666666667  train Acc: 0.9967857142857143\n",
      "epoch: 1250 training Loss: 786.035496836897 validation Loss: 180.00496030358474  valid acc: 0.9966666666666667  train Acc: 0.9967857142857143\n",
      "epoch: 1251 training Loss: 785.6288247857295 validation Loss: 179.91701339313596  valid acc: 0.9966666666666667  train Acc: 0.9967857142857143\n",
      "epoch: 1252 training Loss: 785.2226258569581 validation Loss: 179.82916606208892  valid acc: 0.9966666666666667  train Acc: 0.9967857142857143\n",
      "epoch: 1253 training Loss: 784.8168992277044 validation Loss: 179.74141814071777  valid acc: 0.9966666666666667  train Acc: 0.9967857142857143\n",
      "epoch: 1254 training Loss: 784.4116440769551 validation Loss: 179.65376945967472  valid acc: 0.9966666666666667  train Acc: 0.9967857142857143\n",
      "epoch: 1255 training Loss: 784.0068595855565 validation Loss: 179.56621984998935  valid acc: 0.9966666666666667  train Acc: 0.9971428571428571\n",
      "epoch: 1256 training Loss: 783.6025449362098 validation Loss: 179.4787691430673  valid acc: 0.9966666666666667  train Acc: 0.9971428571428571\n",
      "epoch: 1257 training Loss: 783.1986993134658 validation Loss: 179.39141717068935  valid acc: 0.9966666666666667  train Acc: 0.9971428571428571\n",
      "epoch: 1258 training Loss: 782.7953219037195 validation Loss: 179.30416376501046  valid acc: 0.9966666666666667  train Acc: 0.9971428571428571\n",
      "epoch: 1259 training Loss: 782.3924118952059 validation Loss: 179.2170087585587  valid acc: 0.9966666666666667  train Acc: 0.9971428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1260 training Loss: 781.989968477994 validation Loss: 179.12995198423434  valid acc: 0.9966666666666667  train Acc: 0.9971428571428571\n",
      "epoch: 1261 training Loss: 781.5879908439827 validation Loss: 179.04299327530862  valid acc: 0.9966666666666667  train Acc: 0.9971428571428571\n",
      "epoch: 1262 training Loss: 781.1864781868944 validation Loss: 178.95613246542297  valid acc: 0.9966666666666667  train Acc: 0.9971428571428571\n",
      "epoch: 1263 training Loss: 780.785429702272 validation Loss: 178.86936938858798  valid acc: 0.9966666666666667  train Acc: 0.9971428571428571\n",
      "epoch: 1264 training Loss: 780.3848445874723 validation Loss: 178.7827038791823  valid acc: 0.9966666666666667  train Acc: 0.9971428571428571\n",
      "epoch: 1265 training Loss: 779.9847220416616 validation Loss: 178.69613577195173  valid acc: 0.9966666666666667  train Acc: 0.9971428571428571\n",
      "epoch: 1266 training Loss: 779.5850612658107 validation Loss: 178.60966490200815  valid acc: 0.9966666666666667  train Acc: 0.9971428571428571\n",
      "epoch: 1267 training Loss: 779.1858614626898 validation Loss: 178.5232911048288  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1268 training Loss: 778.7871218368641 validation Loss: 178.4370142162548  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1269 training Loss: 778.3888415946888 validation Loss: 178.35083407249064  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1270 training Loss: 777.9910199443033 validation Loss: 178.26475051010306  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1271 training Loss: 777.5936560956275 validation Loss: 178.17876336601995  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1272 training Loss: 777.1967492603566 validation Loss: 178.0928724775295  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1273 training Loss: 776.8002986519555 validation Loss: 178.00707768227917  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1274 training Loss: 776.4043034856552 validation Loss: 177.92137881827483  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1275 training Loss: 776.0087629784471 validation Loss: 177.8357757238797  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1276 training Loss: 775.6136763490788 validation Loss: 177.7502682378134  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1277 training Loss: 775.2190428180487 validation Loss: 177.66485619915107  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1278 training Loss: 774.8248616076016 validation Loss: 177.57953944732225  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1279 training Loss: 774.4311319417241 validation Loss: 177.4943178221102  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1280 training Loss: 774.0378530461397 validation Loss: 177.40919116365063  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1281 training Loss: 773.6450241483038 validation Loss: 177.32415931243105  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1282 training Loss: 773.2526444773996 validation Loss: 177.23922210928973  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1283 training Loss: 772.8607132643328 validation Loss: 177.15437939541454  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1284 training Loss: 772.4692297417274 validation Loss: 177.0696310123424  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1285 training Loss: 772.0781931439208 validation Loss: 176.98497680195808  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1286 training Loss: 771.6876027069591 validation Loss: 176.9004166064934  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1287 training Loss: 771.2974576685924 validation Loss: 176.81595026852614  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1288 training Loss: 770.9077572682711 validation Loss: 176.73157763097936  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1289 training Loss: 770.5185007471396 validation Loss: 176.6472985371202  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1290 training Loss: 770.1296873480334 validation Loss: 176.56311283055933  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1291 training Loss: 769.7413163154733 validation Loss: 176.47902035524956  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1292 training Loss: 769.3533868956616 validation Loss: 176.3950209554854  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1293 training Loss: 768.9658983364774 validation Loss: 176.31111447590177  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1294 training Loss: 768.5788498874717 validation Loss: 176.22730076147337  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1295 training Loss: 768.1922407998634 validation Loss: 176.14357965751364  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1296 training Loss: 767.8060703265342 validation Loss: 176.05995100967385  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1297 training Loss: 767.4203377220249 validation Loss: 175.97641466394225  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1298 training Loss: 767.03504224253 validation Loss: 175.89297046664316  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1299 training Loss: 766.6501831458944 validation Loss: 175.80961826443612  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1300 training Loss: 766.2657596916076 validation Loss: 175.72635790431485  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1301 training Loss: 765.8817711408004 validation Loss: 175.64318923360662  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1302 training Loss: 765.4982167562401 validation Loss: 175.56011209997106  valid acc: 0.9966666666666667  train Acc: 0.9975\n",
      "epoch: 1303 training Loss: 765.1150958023255 validation Loss: 175.47712635139962  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1304 training Loss: 764.7324075450836 validation Loss: 175.3942318362143  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1305 training Loss: 764.3501512521646 validation Loss: 175.3114284030671  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1306 training Loss: 763.9683261928371 validation Loss: 175.22871590093908  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1307 training Loss: 763.5869316379851 validation Loss: 175.14609417913934  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1308 training Loss: 763.2059668601021 validation Loss: 175.06356308730423  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1309 training Loss: 762.825431133288 validation Loss: 174.98112247539655  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1310 training Loss: 762.4453237332439 validation Loss: 174.8987721937047  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1311 training Loss: 762.0656439372685 validation Loss: 174.81651209284166  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1312 training Loss: 761.6863910242535 validation Loss: 174.73434202374426  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1313 training Loss: 761.3075642746791 validation Loss: 174.6522618376723  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1314 training Loss: 760.9291629706103 validation Loss: 174.57027138620776  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1315 training Loss: 760.5511863956922 validation Loss: 174.48837052125378  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1316 training Loss: 760.173633835146 validation Loss: 174.40655909503394  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1317 training Loss: 759.7965045757647 validation Loss: 174.3248369600915  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1318 training Loss: 759.4197979059088 validation Loss: 174.24320396928832  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1319 training Loss: 759.0435131155025 validation Loss: 174.16165997580424  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1320 training Loss: 758.667649496029 validation Loss: 174.08020483313607  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1321 training Loss: 758.2922063405267 validation Loss: 173.99883839509698  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1322 training Loss: 757.9171829435847 validation Loss: 173.9175605158154  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1323 training Loss: 757.5425786013395 validation Loss: 173.8363710497344  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1324 training Loss: 757.1683926114696 validation Loss: 173.75526985161073  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1325 training Loss: 756.7946242731925 validation Loss: 173.67425677651408  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1326 training Loss: 756.4212728872599 validation Loss: 173.5933316798263  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1327 training Loss: 756.048337755954 validation Loss: 173.51249441724036  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1328 training Loss: 755.6758181830832 validation Loss: 173.43174484475975  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1329 training Loss: 755.3037134739784 validation Loss: 173.3510828186977  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1330 training Loss: 754.9320229354885 validation Loss: 173.27050819567614  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1331 training Loss: 754.5607458759764 validation Loss: 173.19002083262507  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1332 training Loss: 754.1898816053156 validation Loss: 173.10962058678172  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1333 training Loss: 753.8194294348853 validation Loss: 173.02930731568966  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1334 training Loss: 753.4493886775668 validation Loss: 172.94908087719818  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1335 training Loss: 753.0797586477402 validation Loss: 172.86894112946126  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1336 training Loss: 752.7105386612786 validation Loss: 172.788887930937  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1337 training Loss: 752.3417280355465 validation Loss: 172.70892114038668  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1338 training Loss: 751.9733260893938 validation Loss: 172.62904061687394  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1339 training Loss: 751.6053321431534 validation Loss: 172.54924621976411  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1340 training Loss: 751.2377455186358 validation Loss: 172.46953780872343  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1341 training Loss: 750.8705655391267 validation Loss: 172.3899152437181  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1342 training Loss: 750.5037915293819 validation Loss: 172.31037838501362  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1343 training Loss: 750.1374228156243 validation Loss: 172.23092709317407  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1344 training Loss: 749.7714587255391 validation Loss: 172.1515612290612  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1345 training Loss: 749.4058985882708 validation Loss: 172.07228065383373  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1346 training Loss: 749.0407417344189 validation Loss: 171.99308522894648  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1347 training Loss: 748.6759874960343 validation Loss: 171.91397481614973  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1348 training Loss: 748.3116352066149 validation Loss: 171.83494927748848  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1349 training Loss: 747.9476842011027 validation Loss: 171.75600847530143  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1350 training Loss: 747.584133815879 validation Loss: 171.67715227222058  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1351 training Loss: 747.2209833887616 validation Loss: 171.5983805311701  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1352 training Loss: 746.8582322590001 validation Loss: 171.51969311536584  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1353 training Loss: 746.4958797672729 validation Loss: 171.44108988831454  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1354 training Loss: 746.1339252556826 validation Loss: 171.36257071381283  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1355 training Loss: 745.7723680677532 validation Loss: 171.28413545594685  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1356 training Loss: 745.4112075484256 validation Loss: 171.20578397909122  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1357 training Loss: 745.0504430440544 validation Loss: 171.1275161479084  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1358 training Loss: 744.6900739024038 validation Loss: 171.04933182734803  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1359 training Loss: 744.3300994726442 validation Loss: 170.9712308826459  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1360 training Loss: 743.9705191053481 validation Loss: 170.89321317932348  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1361 training Loss: 743.6113321524867 validation Loss: 170.8152785831872  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1362 training Loss: 743.2525379674266 validation Loss: 170.7374269603275  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1363 training Loss: 742.8941359049256 validation Loss: 170.6596581771182  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1364 training Loss: 742.5361253211288 validation Loss: 170.58197210021586  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1365 training Loss: 742.1785055735658 validation Loss: 170.50436859655895  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1366 training Loss: 741.8212760211466 validation Loss: 170.42684753336704  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1367 training Loss: 741.4644360241577 validation Loss: 170.34940877814023  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1368 training Loss: 741.1079849442594 validation Loss: 170.27205219865849  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1369 training Loss: 740.7519221444813 validation Loss: 170.19477766298064  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1370 training Loss: 740.3962469892189 validation Loss: 170.1175850394439  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1371 training Loss: 740.0409588442303 validation Loss: 170.04047419666307  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1372 training Loss: 739.6860570766329 validation Loss: 169.96344500352978  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1373 training Loss: 739.331541054899 validation Loss: 169.88649732921198  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1374 training Loss: 738.9774101488529 validation Loss: 169.80963104315288  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1375 training Loss: 738.6236637296673 validation Loss: 169.73284601507058  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1376 training Loss: 738.2703011698596 validation Loss: 169.65614211495716  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1377 training Loss: 737.9173218432887 validation Loss: 169.57951921307802  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1378 training Loss: 737.5647251251509 validation Loss: 169.50297717997125  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1379 training Loss: 737.212510391977 validation Loss: 169.4265158864469  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1380 training Loss: 736.860677021629 validation Loss: 169.35013520358615  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1381 training Loss: 736.5092243932959 validation Loss: 169.27383500274078  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1382 training Loss: 736.1581518874906 validation Loss: 169.1976151555325  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1383 training Loss: 735.8074588860466 validation Loss: 169.12147553385205  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1384 training Loss: 735.4571447721144 validation Loss: 169.04541600985868  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1385 training Loss: 735.1072089301582 validation Loss: 168.96943645597952  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1386 training Loss: 734.7576507459526 validation Loss: 168.8935367449086  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1387 training Loss: 734.4084696065785 validation Loss: 168.8177167496066  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1388 training Loss: 734.0596649004209 validation Loss: 168.74197634329974  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1389 training Loss: 733.7112360171643 validation Loss: 168.66631539947937  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1390 training Loss: 733.3631823477901 validation Loss: 168.59073379190124  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1391 training Loss: 733.015503284573 validation Loss: 168.51523139458473  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1392 training Loss: 732.6681982210779 validation Loss: 168.43980808181232  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1393 training Loss: 732.3212665521559 validation Loss: 168.36446372812884  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1394 training Loss: 731.9747076739417 validation Loss: 168.2891982083408  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1395 training Loss: 731.62852098385 validation Loss: 168.2140113975156  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1396 training Loss: 731.2827058805718 validation Loss: 168.13890317098128  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1397 training Loss: 730.9372617640722 validation Loss: 168.0638734043253  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1398 training Loss: 730.5921880355857 validation Loss: 167.9889219733943  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1399 training Loss: 730.2474840976137 validation Loss: 167.91404875429325  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1400 training Loss: 729.9031493539214 validation Loss: 167.83925362338482  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1401 training Loss: 729.5591832095345 validation Loss: 167.76453645728876  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1402 training Loss: 729.2155850707352 validation Loss: 167.68989713288124  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1403 training Loss: 728.8723543450594 validation Loss: 167.61533552729418  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1404 training Loss: 728.5294904412941 validation Loss: 167.54085151791458  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1405 training Loss: 728.1869927694735 validation Loss: 167.4664449823839  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1406 training Loss: 727.8448607408757 validation Loss: 167.3921157985975  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1407 training Loss: 727.5030937680198 validation Loss: 167.31786384470374  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1408 training Loss: 727.1616912646627 validation Loss: 167.24368899910368  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1409 training Loss: 726.8206526457959 validation Loss: 167.16959114045017  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1410 training Loss: 726.479977327642 validation Loss: 167.09557014764732  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1411 training Loss: 726.1396647276522 validation Loss: 167.02162589984985  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1412 training Loss: 725.7997142645025 validation Loss: 166.94775827646254  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1413 training Loss: 725.4601253580911 validation Loss: 166.87396715713942  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1414 training Loss: 725.1208974295344 validation Loss: 166.80025242178326  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1415 training Loss: 724.7820299011648 validation Loss: 166.72661395054487  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1416 training Loss: 724.4435221965277 validation Loss: 166.6530516238227  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1417 training Loss: 724.1053737403774 validation Loss: 166.57956532226183  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1418 training Loss: 723.7675839586743 validation Loss: 166.50615492675362  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1419 training Loss: 723.4301522785827 validation Loss: 166.43282031843512  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1420 training Loss: 723.0930781284667 validation Loss: 166.3595613786882  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1421 training Loss: 722.7563609378872 validation Loss: 166.28637798913917  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1422 training Loss: 722.4200001375999 validation Loss: 166.21327003165808  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1423 training Loss: 722.083995159551 validation Loss: 166.1402373883581  valid acc: 0.9966666666666667  train Acc: 0.9978571428571429\n",
      "epoch: 1424 training Loss: 721.7483454368746 validation Loss: 166.06727994159485  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1425 training Loss: 721.4130504038899 validation Loss: 165.9943975739659  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1426 training Loss: 721.0781094960978 validation Loss: 165.92159016831022  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1427 training Loss: 720.7435221501783 validation Loss: 165.84885760770723  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1428 training Loss: 720.409287803987 validation Loss: 165.77619977547664  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1429 training Loss: 720.0754058965526 validation Loss: 165.70361655517755  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1430 training Loss: 719.7418758680736 validation Loss: 165.63110783060796  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1431 training Loss: 719.4086971599154 validation Loss: 165.55867348580415  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1432 training Loss: 719.0758692146071 validation Loss: 165.48631340504002  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1433 training Loss: 718.7433914758392 validation Loss: 165.41402747282666  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1434 training Loss: 718.4112633884597 validation Loss: 165.34181557391156  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1435 training Loss: 718.0794843984722 validation Loss: 165.26967759327815  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1436 training Loss: 717.7480539530322 validation Loss: 165.19761341614512  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1437 training Loss: 717.416971500444 validation Loss: 165.12562292796594  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1438 training Loss: 717.0862364901591 validation Loss: 165.0537060144282  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1439 training Loss: 716.7558483727712 validation Loss: 164.98186256145294  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1440 training Loss: 716.4258066000157 validation Loss: 164.9100924551943  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1441 training Loss: 716.0961106247647 validation Loss: 164.8383955820387  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1442 training Loss: 715.7667599010251 validation Loss: 164.76677182860436  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1443 training Loss: 715.4377538839362 validation Loss: 164.69522108174075  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1444 training Loss: 715.1090920297659 validation Loss: 164.62374322852796  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1445 training Loss: 714.7807737959076 validation Loss: 164.55233815627622  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1446 training Loss: 714.4527986408791 validation Loss: 164.48100575252514  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1447 training Loss: 714.1251660243177 validation Loss: 164.40974590504325  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1448 training Loss: 713.7978754069785 validation Loss: 164.3385585018275  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1449 training Loss: 713.4709262507317 validation Loss: 164.26744343110258  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1450 training Loss: 713.144318018559 validation Loss: 164.1964005813204  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1451 training Loss: 712.8180501745517 validation Loss: 164.12542984115956  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1452 training Loss: 712.4921221839066 validation Loss: 164.05453109952464  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1453 training Loss: 712.1665335129248 validation Loss: 163.98370424554577  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1454 training Loss: 711.8412836290081 validation Loss: 163.9129491685781  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1455 training Loss: 711.5163720006558 validation Loss: 163.84226575820117  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1456 training Loss: 711.1917980974624 validation Loss: 163.77165390421828  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1457 training Loss: 710.8675613901157 validation Loss: 163.7011134966561  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1458 training Loss: 710.5436613503923 validation Loss: 163.630644425764  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1459 training Loss: 710.220097451156 validation Loss: 163.5602465820135  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1460 training Loss: 709.896869166355 validation Loss: 163.4899198560978  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1461 training Loss: 709.5739759710189 validation Loss: 163.41966413893118  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1462 training Loss: 709.2514173412559 validation Loss: 163.3494793216484  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1463 training Loss: 708.9291927542506 validation Loss: 163.27936529560426  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1464 training Loss: 708.6073016882606 validation Loss: 163.20932195237296  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1465 training Loss: 708.2857436226144 validation Loss: 163.1393491837476  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1466 training Loss: 707.9645180377084 validation Loss: 163.0694468817397  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1467 training Loss: 707.6436244150043 validation Loss: 162.9996149385785  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1468 training Loss: 707.3230622370265 validation Loss: 162.9298532467106  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1469 training Loss: 707.0028309873594 validation Loss: 162.86016169879926  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1470 training Loss: 706.6829301506444 validation Loss: 162.79054018772405  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1471 training Loss: 706.363359212578 validation Loss: 162.72098860658008  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1472 training Loss: 706.0441176599082 validation Loss: 162.65150684867774  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1473 training Loss: 705.725204980433 validation Loss: 162.58209480754192  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1474 training Loss: 705.4066206629968 validation Loss: 162.51275237691164  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1475 training Loss: 705.0883641974881 validation Loss: 162.44347945073952  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1476 training Loss: 704.7704350748372 validation Loss: 162.37427592319105  valid acc: 0.9966666666666667  train Acc: 0.9982142857142857\n",
      "epoch: 1477 training Loss: 704.4528327870129 validation Loss: 162.30514168864443  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1478 training Loss: 704.135556827021 validation Loss: 162.23607664168964  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1479 training Loss: 703.8186066889004 validation Loss: 162.1670806771283  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1480 training Loss: 703.5019818677218 validation Loss: 162.0981536899728  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1481 training Loss: 703.1856818595838 validation Loss: 162.0292955754461  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1482 training Loss: 702.8697061616118 validation Loss: 161.96050622898093  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1483 training Loss: 702.554054271954 validation Loss: 161.8917855462195  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1484 training Loss: 702.2387256897799 validation Loss: 161.82313342301285  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1485 training Loss: 701.9237199152776 validation Loss: 161.7545497554204  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1486 training Loss: 701.6090364496505 validation Loss: 161.68603443970943  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1487 training Loss: 701.2946747951158 validation Loss: 161.6175873723545  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1488 training Loss: 700.9806344549014 validation Loss: 161.54920845003707  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1489 training Loss: 700.6669149332434 validation Loss: 161.48089756964487  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1490 training Loss: 700.3535157353839 validation Loss: 161.41265462827153  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1491 training Loss: 700.0404363675681 validation Loss: 161.34447952321582  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1492 training Loss: 699.7276763370419 validation Loss: 161.27637215198158  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1493 training Loss: 699.4152351520502 validation Loss: 161.20833241227677  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1494 training Loss: 699.1031123218327 validation Loss: 161.14036020201323  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1495 training Loss: 698.7913073566234 validation Loss: 161.07245541930607  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1496 training Loss: 698.4798197676469 validation Loss: 161.0046179624733  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1497 training Loss: 698.1686490671161 validation Loss: 160.93684773003514  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1498 training Loss: 697.8577947682302 validation Loss: 160.86914462071377  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1499 training Loss: 697.5472563851717 validation Loss: 160.80150853343264  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1500 training Loss: 697.2370334331047 validation Loss: 160.733939367316  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1501 training Loss: 696.9271254281714 validation Loss: 160.66643702168852  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1502 training Loss: 696.617531887491 validation Loss: 160.59900139607478  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1503 training Loss: 696.308252329156 validation Loss: 160.53163239019858  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1504 training Loss: 695.9992862722306 validation Loss: 160.46432990398284  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1505 training Loss: 695.6906332367482 validation Loss: 160.3970938375487  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1506 training Loss: 695.3822927437088 validation Loss: 160.32992409121528  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1507 training Loss: 695.0742643150768 validation Loss: 160.26282056549923  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1508 training Loss: 694.7665474737782 validation Loss: 160.19578316111404  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1509 training Loss: 694.4591417436992 validation Loss: 160.1288117789698  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1510 training Loss: 694.1520466496826 validation Loss: 160.06190632017262  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1511 training Loss: 693.8452617175265 validation Loss: 159.99506668602402  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1512 training Loss: 693.5387864739807 validation Loss: 159.92829277802065  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1513 training Loss: 693.2326204467465 validation Loss: 159.86158449785387  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1514 training Loss: 692.9267631644716 validation Loss: 159.7949417474089  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1515 training Loss: 692.6212141567503 validation Loss: 159.72836442876488  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1516 training Loss: 692.3159729541194 validation Loss: 159.66185244419395  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1517 training Loss: 692.0110390880567 validation Loss: 159.59540569616098  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1518 training Loss: 691.7064120909787 validation Loss: 159.5290240873232  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1519 training Loss: 691.4020914962377 validation Loss: 159.46270752052948  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1520 training Loss: 691.0980768381207 validation Loss: 159.39645589882005  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1521 training Loss: 690.7943676518458 validation Loss: 159.33026912542607  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1522 training Loss: 690.4909634735606 validation Loss: 159.26414710376895  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1523 training Loss: 690.1878638403393 validation Loss: 159.19808973746018  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1524 training Loss: 689.8850682901821 validation Loss: 159.13209693030058  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1525 training Loss: 689.5825763620103 validation Loss: 159.06616858628013  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1526 training Loss: 689.2803875956666 validation Loss: 159.00030460957723  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1527 training Loss: 688.9785015319115 validation Loss: 158.9345049045585  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1528 training Loss: 688.6769177124211 validation Loss: 158.8687693757782  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1529 training Loss: 688.3756356797849 validation Loss: 158.8030979279777  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1530 training Loss: 688.0746549775041 validation Loss: 158.73749046608518  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1531 training Loss: 687.7739751499889 validation Loss: 158.67194689521511  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1532 training Loss: 687.4735957425567 validation Loss: 158.60646712066787  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1533 training Loss: 687.1735163014285 validation Loss: 158.54105104792916  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1534 training Loss: 686.8737363737291 validation Loss: 158.47569858266965  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1535 training Loss: 686.5742555074828 validation Loss: 158.4104096307446  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1536 training Loss: 686.2750732516122 validation Loss: 158.34518409819322  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1537 training Loss: 685.9761891559357 validation Loss: 158.28002189123845  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1538 training Loss: 685.6776027711655 validation Loss: 158.2149229162864  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1539 training Loss: 685.3793136489053 validation Loss: 158.1498870799259  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1540 training Loss: 685.0813213416479 validation Loss: 158.08491428892808  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1541 training Loss: 684.7836254027742 validation Loss: 158.020004450246  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1542 training Loss: 684.486225386549 validation Loss: 157.95515747101416  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1543 training Loss: 684.1891208481205 validation Loss: 157.89037325854795  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1544 training Loss: 683.8923113435179 validation Loss: 157.82565172034344  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1545 training Loss: 683.5957964296485 validation Loss: 157.76099276407686  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1546 training Loss: 683.2995756642968 validation Loss: 157.69639629760405  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1547 training Loss: 683.0036486061208 validation Loss: 157.63186222896013  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1548 training Loss: 682.7080148146512 validation Loss: 157.5673904663592  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1549 training Loss: 682.4126738502889 validation Loss: 157.50298091819366  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1550 training Loss: 682.1176252743028 validation Loss: 157.43863349303388  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1551 training Loss: 681.8228686488274 validation Loss: 157.37434809962795  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1552 training Loss: 681.5284035368612 validation Loss: 157.31012464690093  valid acc: 0.9966666666666667  train Acc: 0.9985714285714286\n",
      "epoch: 1553 training Loss: 681.2342295022645 validation Loss: 157.24596304395476  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1554 training Loss: 680.9403461097575 validation Loss: 157.1818632000676  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1555 training Loss: 680.6467529249173 validation Loss: 157.11782502469345  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1556 training Loss: 680.353449514177 validation Loss: 157.05384842746193  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1557 training Loss: 680.060435444823 validation Loss: 156.9899333181775  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1558 training Loss: 679.7677102849932 validation Loss: 156.9260796068194  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1559 training Loss: 679.4752736036745 validation Loss: 156.8622872035411  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1560 training Loss: 679.1831249707013 validation Loss: 156.7985560186697  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1561 training Loss: 678.8912639567529 validation Loss: 156.73488596270585  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1562 training Loss: 678.5996901333526 validation Loss: 156.67127694632313  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1563 training Loss: 678.3084030728639 validation Loss: 156.60772888036763  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1564 training Loss: 678.0174023484901 validation Loss: 156.5442416758577  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1565 training Loss: 677.7266875342713 validation Loss: 156.48081524398333  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1566 training Loss: 677.4362582050825 validation Loss: 156.41744949610592  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1567 training Loss: 677.1461139366322 validation Loss: 156.3541443437577  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1568 training Loss: 676.85625430546 validation Loss: 156.2908996986416  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1569 training Loss: 676.566678888934 validation Loss: 156.2277154726305  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1570 training Loss: 676.2773872652501 validation Loss: 156.16459157776708  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1571 training Loss: 675.988379013429 validation Loss: 156.10152792626332  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1572 training Loss: 675.6996537133143 validation Loss: 156.0385244305002  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1573 training Loss: 675.4112109455713 validation Loss: 155.975581003027  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1574 training Loss: 675.123050291684 validation Loss: 155.9126975565614  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1575 training Loss: 674.8351713339538 validation Loss: 155.8498740039886  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1576 training Loss: 674.5475736554974 validation Loss: 155.7871102583612  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1577 training Loss: 674.2602568402448 validation Loss: 155.72440623289867  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1578 training Loss: 673.9732204729372 validation Loss: 155.66176184098714  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1579 training Loss: 673.6864641391255 validation Loss: 155.59917699617876  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1580 training Loss: 673.3999874251679 validation Loss: 155.53665161219146  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1581 training Loss: 673.113789918228 validation Loss: 155.47418560290862  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1582 training Loss: 672.8278712062735 validation Loss: 155.41177888237843  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1583 training Loss: 672.5422308780734 validation Loss: 155.34943136481382  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1584 training Loss: 672.2568685231965 validation Loss: 155.28714296459182  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1585 training Loss: 671.97178373201 validation Loss: 155.2249135962533  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1586 training Loss: 671.6869760956763 validation Loss: 155.1627431745025  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1587 training Loss: 671.4024452061525 validation Loss: 155.10063161420683  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1588 training Loss: 671.1181906561875 validation Loss: 155.03857883039623  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1589 training Loss: 670.8342120393208 validation Loss: 154.97658473826294  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1590 training Loss: 670.5505089498802 validation Loss: 154.91464925316112  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1591 training Loss: 670.2670809829798 validation Loss: 154.85277229060642  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1592 training Loss: 669.9839277345188 validation Loss: 154.79095376627566  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1593 training Loss: 669.701048801179 validation Loss: 154.72919359600633  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1594 training Loss: 669.4184437804232 validation Loss: 154.6674916957964  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1595 training Loss: 669.136112270493 validation Loss: 154.60584798180372  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1596 training Loss: 668.8540538704076 validation Loss: 154.5442623703459  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1597 training Loss: 668.5722681799613 validation Loss: 154.48273477789976  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1598 training Loss: 668.2907547997222 validation Loss: 154.42126512110093  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1599 training Loss: 668.0095133310299 validation Loss: 154.35985331674362  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1600 training Loss: 667.7285433759941 validation Loss: 154.29849928178018  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1601 training Loss: 667.4478445374924 validation Loss: 154.23720293332067  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1602 training Loss: 667.1674164191687 validation Loss: 154.17596418863263  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1603 training Loss: 666.8872586254312 validation Loss: 154.1147829651405  valid acc: 0.9966666666666667  train Acc: 0.9989285714285714\n",
      "epoch: 1604 training Loss: 666.6073707614506 validation Loss: 154.05365918042557  valid acc: 0.9983333333333333  train Acc: 0.9989285714285714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1605 training Loss: 666.327752433159 validation Loss: 153.99259275222528  valid acc: 0.9983333333333333  train Acc: 0.9989285714285714\n",
      "epoch: 1606 training Loss: 666.0484032472468 validation Loss: 153.93158359843306  valid acc: 0.9983333333333333  train Acc: 0.9989285714285714\n",
      "epoch: 1607 training Loss: 665.769322811162 validation Loss: 153.8706316370979  valid acc: 0.9983333333333333  train Acc: 0.9989285714285714\n",
      "epoch: 1608 training Loss: 665.4905107331078 validation Loss: 153.80973678642397  valid acc: 0.9983333333333333  train Acc: 0.9989285714285714\n",
      "epoch: 1609 training Loss: 665.2119666220412 validation Loss: 153.7488989647704  valid acc: 0.9983333333333333  train Acc: 0.9989285714285714\n",
      "epoch: 1610 training Loss: 664.933690087671 validation Loss: 153.68811809065065  valid acc: 0.9983333333333333  train Acc: 0.9989285714285714\n",
      "epoch: 1611 training Loss: 664.6556807404562 validation Loss: 153.6273940827324  valid acc: 0.9983333333333333  train Acc: 0.9989285714285714\n",
      "epoch: 1612 training Loss: 664.3779381916036 validation Loss: 153.56672685983702  valid acc: 0.9983333333333333  train Acc: 0.9989285714285714\n",
      "epoch: 1613 training Loss: 664.100462053067 validation Loss: 153.50611634093943  valid acc: 0.9983333333333333  train Acc: 0.9989285714285714\n",
      "epoch: 1614 training Loss: 663.8232519375449 validation Loss: 153.44556244516747  valid acc: 0.9983333333333333  train Acc: 0.9989285714285714\n",
      "epoch: 1615 training Loss: 663.5463074584791 validation Loss: 153.38506509180172  valid acc: 0.9983333333333333  train Acc: 0.9989285714285714\n",
      "epoch: 1616 training Loss: 663.2696282300519 validation Loss: 153.32462420027508  valid acc: 0.9983333333333333  train Acc: 0.9989285714285714\n",
      "epoch: 1617 training Loss: 662.9932138671859 validation Loss: 153.2642396901725  valid acc: 0.9983333333333333  train Acc: 0.9989285714285714\n",
      "epoch: 1618 training Loss: 662.717063985541 validation Loss: 153.20391148123048  valid acc: 0.9983333333333333  train Acc: 0.9989285714285714\n",
      "epoch: 1619 training Loss: 662.4411782015134 validation Loss: 153.14363949333693  valid acc: 0.9983333333333333  train Acc: 0.9989285714285714\n",
      "epoch: 1620 training Loss: 662.1655561322334 validation Loss: 153.0834236465305  valid acc: 0.9983333333333333  train Acc: 0.9989285714285714\n",
      "epoch: 1621 training Loss: 661.890197395564 validation Loss: 153.0232638610006  valid acc: 0.9983333333333333  train Acc: 0.9989285714285714\n",
      "epoch: 1622 training Loss: 661.6151016100993 validation Loss: 152.96316005708672  valid acc: 0.9983333333333333  train Acc: 0.9989285714285714\n",
      "epoch: 1623 training Loss: 661.340268395162 validation Loss: 152.9031121552784  valid acc: 0.9983333333333333  train Acc: 0.9992857142857143\n",
      "epoch: 1624 training Loss: 661.0656973708027 validation Loss: 152.8431200762146  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1625 training Loss: 660.7913881577978 validation Loss: 152.7831837406835  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1626 training Loss: 660.5173403776471 validation Loss: 152.7233030696222  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1627 training Loss: 660.2435536525734 validation Loss: 152.6634779841161  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1628 training Loss: 659.9700276055198 validation Loss: 152.60370840539892  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1629 training Loss: 659.6967618601482 validation Loss: 152.54399425485224  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1630 training Loss: 659.423756040838 validation Loss: 152.48433545400496  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1631 training Loss: 659.1510097726843 validation Loss: 152.42473192453326  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1632 training Loss: 658.8785226814957 validation Loss: 152.36518358825992  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1633 training Loss: 658.6062943937936 validation Loss: 152.30569036715445  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1634 training Loss: 658.3343245368092 validation Loss: 152.24625218333225  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1635 training Loss: 658.0626127384832 validation Loss: 152.18686895905455  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1636 training Loss: 657.7911586274631 validation Loss: 152.12754061672806  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1637 training Loss: 657.5199618331028 validation Loss: 152.06826707890463  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1638 training Loss: 657.2490219854592 validation Loss: 152.00904826828082  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1639 training Loss: 656.978338715292 validation Loss: 151.94988410769764  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1640 training Loss: 656.7079116540614 validation Loss: 151.8907745201402  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1641 training Loss: 656.4377404339266 validation Loss: 151.83171942873747  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1642 training Loss: 656.1678246877445 validation Loss: 151.77271875676172  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1643 training Loss: 655.8981640490674 validation Loss: 151.71377242762853  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1644 training Loss: 655.6287581521419 validation Loss: 151.6548803648961  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1645 training Loss: 655.3596066319074 validation Loss: 151.59604249226513  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1646 training Loss: 655.0907091239935 validation Loss: 151.5372587335785  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1647 training Loss: 654.82206526472 validation Loss: 151.4785290128209  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1648 training Loss: 654.5536746910938 validation Loss: 151.41985325411844  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1649 training Loss: 654.2855370408081 validation Loss: 151.36123138173843  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1650 training Loss: 654.0176519522405 validation Loss: 151.30266332008898  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1651 training Loss: 653.7500190644519 validation Loss: 151.24414899371877  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1652 training Loss: 653.4826380171842 validation Loss: 151.18568832731663  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1653 training Loss: 653.215508450859 validation Loss: 151.1272812457113  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1654 training Loss: 652.9486300065765 validation Loss: 151.06892767387092  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1655 training Loss: 652.6820023261131 validation Loss: 151.0106275369031  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1656 training Loss: 652.4156250519204 validation Loss: 150.95238076005415  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1657 training Loss: 652.1494978271232 validation Loss: 150.89418726870912  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1658 training Loss: 651.883620295519 validation Loss: 150.83604698839127  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1659 training Loss: 651.6179921015749 validation Loss: 150.77795984476174  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1660 training Loss: 651.3526128904269 validation Loss: 150.7199257636195  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1661 training Loss: 651.0874823078786 validation Loss: 150.66194467090077  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1662 training Loss: 650.822600000399 validation Loss: 150.60401649267862  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1663 training Loss: 650.5579656151215 validation Loss: 150.54614115516313  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1664 training Loss: 650.2935787998422 validation Loss: 150.48831858470058  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1665 training Loss: 650.0294392030178 validation Loss: 150.4305487077734  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1666 training Loss: 649.7655464737652 validation Loss: 150.37283145099974  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1667 training Loss: 649.5019002618592 validation Loss: 150.31516674113325  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1668 training Loss: 649.2385002177309 validation Loss: 150.25755450506279  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1669 training Loss: 648.9753459924668 validation Loss: 150.19999466981199  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1670 training Loss: 648.7124372378066 validation Loss: 150.142487162539  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1671 training Loss: 648.4497736061423 validation Loss: 150.08503191053634  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1672 training Loss: 648.1873547505161 validation Loss: 150.02762884123035  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1673 training Loss: 647.9251803246194 validation Loss: 149.97027788218105  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1674 training Loss: 647.6632499827913 validation Loss: 149.9129789610817  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1675 training Loss: 647.4015633800163 validation Loss: 149.8557320057587  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1676 training Loss: 647.1401201719245 validation Loss: 149.79853694417113  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1677 training Loss: 646.8789200147883 validation Loss: 149.7413937044105  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1678 training Loss: 646.6179625655216 validation Loss: 149.68430221470038  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1679 training Loss: 646.3572474816788 validation Loss: 149.6272624033962  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1680 training Loss: 646.0967744214529 validation Loss: 149.57027419898495  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1681 training Loss: 645.836543043674 validation Loss: 149.51333753008478  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1682 training Loss: 645.5765530078079 validation Loss: 149.45645232544481  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1683 training Loss: 645.3168039739546 validation Loss: 149.3996185139448  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1684 training Loss: 645.0572956028473 validation Loss: 149.3428360245948  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1685 training Loss: 644.7980275558499 validation Loss: 149.2861047865349  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1686 training Loss: 644.5389994949568 validation Loss: 149.22942472903506  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1687 training Loss: 644.2802110827906 validation Loss: 149.17279578149453  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1688 training Loss: 644.0216619826008 validation Loss: 149.11621787344183  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1689 training Loss: 643.7633518582631 validation Loss: 149.05969093453427  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1690 training Loss: 643.5052803742765 validation Loss: 149.00321489455789  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1691 training Loss: 643.2474471957636 validation Loss: 148.94678968342683  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1692 training Loss: 642.9898519884674 validation Loss: 148.89041523118334  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1693 training Loss: 642.7324944187519 validation Loss: 148.8340914679974  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1694 training Loss: 642.4753741535985 validation Loss: 148.7778183241664  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1695 training Loss: 642.2184908606064 validation Loss: 148.7215957301148  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1696 training Loss: 641.9618442079902 validation Loss: 148.66542361639404  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1697 training Loss: 641.7054338645789 validation Loss: 148.609301913682  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1698 training Loss: 641.4492594998142 validation Loss: 148.5532305527829  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1699 training Loss: 641.1933207837492 validation Loss: 148.49720946462702  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1700 training Loss: 640.9376173870473 validation Loss: 148.4412385802702  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1701 training Loss: 640.6821489809806 validation Loss: 148.38531783089394  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1702 training Loss: 640.4269152374284 validation Loss: 148.32944714780467  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1703 training Loss: 640.1719158288756 validation Loss: 148.27362646243387  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1704 training Loss: 639.9171504284122 validation Loss: 148.2178557063375  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1705 training Loss: 639.6626187097311 validation Loss: 148.16213481119587  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1706 training Loss: 639.4083203471268 validation Loss: 148.10646370881327  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1707 training Loss: 639.154255015495 validation Loss: 148.0508423311179  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1708 training Loss: 638.9004223903294 validation Loss: 147.99527061016127  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1709 training Loss: 638.646822147722 validation Loss: 147.93974847811825  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1710 training Loss: 638.3934539643612 validation Loss: 147.88427586728645  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1711 training Loss: 638.1403175175304 validation Loss: 147.82885271008627  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1712 training Loss: 637.8874124851063 validation Loss: 147.77347893906054  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1713 training Loss: 637.6347385455582 validation Loss: 147.71815448687403  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1714 training Loss: 637.3822953779463 validation Loss: 147.6628792863134  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1715 training Loss: 637.1300826619207 validation Loss: 147.60765327028696  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1716 training Loss: 636.8781000777192 validation Loss: 147.5524763718242  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1717 training Loss: 636.626347306167 validation Loss: 147.4973485240758  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1718 training Loss: 636.374824028675 validation Loss: 147.4422696603129  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1719 training Loss: 636.123529927238 validation Loss: 147.3872397139274  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1720 training Loss: 635.8724646844341 validation Loss: 147.33225861843124  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1721 training Loss: 635.6216279834234 validation Loss: 147.2773263074564  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1722 training Loss: 635.3710195079453 validation Loss: 147.22244271475446  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1723 training Loss: 635.1206389423196 validation Loss: 147.1676077741965  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1724 training Loss: 634.8704859714429 validation Loss: 147.11282141977273  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1725 training Loss: 634.6205602807886 validation Loss: 147.05808358559207  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1726 training Loss: 634.3708615564052 validation Loss: 147.0033942058823  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1727 training Loss: 634.121389484915 validation Loss: 146.9487532149895  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1728 training Loss: 633.872143753513 validation Loss: 146.89416054737762  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1729 training Loss: 633.6231240499651 validation Loss: 146.83961613762875  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1730 training Loss: 633.374330062608 validation Loss: 146.78511992044236  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1731 training Loss: 633.1257614803462 validation Loss: 146.73067183063523  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1732 training Loss: 632.877417992652 validation Loss: 146.67627180314125  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1733 training Loss: 632.629299289564 validation Loss: 146.6219197730111  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1734 training Loss: 632.3814050616854 validation Loss: 146.56761567541184  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1735 training Loss: 632.1337350001833 validation Loss: 146.51335944562706  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1736 training Loss: 631.8862887967869 validation Loss: 146.45915101905615  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1737 training Loss: 631.6390661437865 validation Loss: 146.4049903312143  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1738 training Loss: 631.3920667340324 validation Loss: 146.35087731773223  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1739 training Loss: 631.1452902609334 validation Loss: 146.2968119143559  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1740 training Loss: 630.8987364184555 validation Loss: 146.24279405694628  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1741 training Loss: 630.6524049011209 validation Loss: 146.18882368147902  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1742 training Loss: 630.4062954040069 validation Loss: 146.13490072404426  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1743 training Loss: 630.1604076227436 validation Loss: 146.08102512084642  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1744 training Loss: 629.9147412535141 validation Loss: 146.02719680820383  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1745 training Loss: 629.6692959930524 validation Loss: 145.9734157225486  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1746 training Loss: 629.4240715386427 validation Loss: 145.9196818004263  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1747 training Loss: 629.179067588117 validation Loss: 145.86599497849573  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1748 training Loss: 628.9342838398561 validation Loss: 145.8123551935286  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1749 training Loss: 628.6897199927855 validation Loss: 145.75876238240943  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1750 training Loss: 628.4453757463767 validation Loss: 145.70521648213517  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1751 training Loss: 628.2012508006446 validation Loss: 145.65171742981508  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1752 training Loss: 627.9573448561466 validation Loss: 145.59826516267026  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1753 training Loss: 627.7136576139817 validation Loss: 145.54485961803366  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1754 training Loss: 627.470188775789 validation Loss: 145.4915007333497  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1755 training Loss: 627.226938043746 validation Loss: 145.43818844617402  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1756 training Loss: 626.9839051205686 validation Loss: 145.38492269417327  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1757 training Loss: 626.7410897095092 validation Loss: 145.33170341512488  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1758 training Loss: 626.4984915143551 validation Loss: 145.27853054691678  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1759 training Loss: 626.256110239428 validation Loss: 145.2254040275472  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1760 training Loss: 626.0139455895828 validation Loss: 145.17232379512433  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1761 training Loss: 625.7719972702058 validation Loss: 145.11928978786625  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1762 training Loss: 625.530264987214 validation Loss: 145.06630194410047  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1763 training Loss: 625.288748447054 validation Loss: 145.01336020226398  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1764 training Loss: 625.0474473567006 validation Loss: 144.96046450090262  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1765 training Loss: 624.8063614236553 validation Loss: 144.90761477867127  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1766 training Loss: 624.5654903559462 validation Loss: 144.85481097433328  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1767 training Loss: 624.3248338621255 validation Loss: 144.80205302676035  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1768 training Loss: 624.0843916512695 validation Loss: 144.74934087493244  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1769 training Loss: 623.8441634329763 validation Loss: 144.69667445793718  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1770 training Loss: 623.6041489173659 validation Loss: 144.64405371497003  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1771 training Loss: 623.3643478150782 validation Loss: 144.5914785853338  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1772 training Loss: 623.1247598372718 validation Loss: 144.53894900843838  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1773 training Loss: 622.8853846956235 validation Loss: 144.48646492380078  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1774 training Loss: 622.6462221023264 validation Loss: 144.43402627104467  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1775 training Loss: 622.4072717700894 validation Loss: 144.38163298990003  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1776 training Loss: 622.1685334121357 validation Loss: 144.32928502020337  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1777 training Loss: 621.9300067422018 validation Loss: 144.27698230189696  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1778 training Loss: 621.6916914745362 validation Loss: 144.22472477502902  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1779 training Loss: 621.4535873238984 validation Loss: 144.17251237975324  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1780 training Loss: 621.2156940055578 validation Loss: 144.12034505632863  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1781 training Loss: 620.9780112352926 validation Loss: 144.06822274511939  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1782 training Loss: 620.7405387293886 validation Loss: 144.0161453865945  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1783 training Loss: 620.5032762046382 validation Loss: 143.96411292132757  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1784 training Loss: 620.2662233783386 validation Loss: 143.9121252899967  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1785 training Loss: 620.0293799682922 validation Loss: 143.8601824333841  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1786 training Loss: 619.7927456928039 validation Loss: 143.80828429237596  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1787 training Loss: 619.5563202706808 validation Loss: 143.75643080796223  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1788 training Loss: 619.3201034212311 validation Loss: 143.7046219212363  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1789 training Loss: 619.0840948642625 validation Loss: 143.652857573395  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1790 training Loss: 618.8482943200819 validation Loss: 143.60113770573804  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1791 training Loss: 618.6127015094937 validation Loss: 143.54946225966805  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1792 training Loss: 618.3773161537986 validation Loss: 143.49783117669026  valid acc: 0.9983333333333333  train Acc: 0.9996428571428572\n",
      "epoch: 1793 training Loss: 618.1421379747933 validation Loss: 143.44624439841226  valid acc: 0.9983333333333333  train Acc: 1.0\n",
      "epoch: 1794 training Loss: 617.9071666947682 validation Loss: 143.39470186654398  valid acc: 0.9983333333333333  train Acc: 1.0\n",
      "epoch: 1795 training Loss: 617.6724020365077 validation Loss: 143.34320352289703  valid acc: 0.9983333333333333  train Acc: 1.0\n",
      "epoch: 1796 training Loss: 617.437843723288 validation Loss: 143.29174930938498  valid acc: 0.9983333333333333  train Acc: 1.0\n",
      "epoch: 1797 training Loss: 617.2034914788765 validation Loss: 143.24033916802279  valid acc: 0.9983333333333333  train Acc: 1.0\n",
      "epoch: 1798 training Loss: 616.9693450275308 validation Loss: 143.18897304092667  valid acc: 0.9983333333333333  train Acc: 1.0\n",
      "epoch: 1799 training Loss: 616.7354040939972 validation Loss: 143.13765087031408  valid acc: 0.9983333333333333  train Acc: 1.0\n",
      "epoch: 1800 training Loss: 616.5016684035106 validation Loss: 143.0863725985032  valid acc: 0.9983333333333333  train Acc: 1.0\n",
      "epoch: 1801 training Loss: 616.2681376817919 validation Loss: 143.0351381679128  valid acc: 0.9983333333333333  train Acc: 1.0\n",
      "epoch: 1802 training Loss: 616.0348116550484 validation Loss: 142.98394752106222  valid acc: 0.9983333333333333  train Acc: 1.0\n",
      "epoch: 1803 training Loss: 615.801690049972 validation Loss: 142.93280060057086  valid acc: 0.9983333333333333  train Acc: 1.0\n",
      "epoch: 1804 training Loss: 615.568772593738 validation Loss: 142.88169734915823  valid acc: 0.9983333333333333  train Acc: 1.0\n",
      "epoch: 1805 training Loss: 615.3360590140046 validation Loss: 142.8306377096435  valid acc: 0.9983333333333333  train Acc: 1.0\n",
      "epoch: 1806 training Loss: 615.1035490389118 validation Loss: 142.7796216249455  valid acc: 0.9983333333333333  train Acc: 1.0\n",
      "epoch: 1807 training Loss: 614.8712423970794 validation Loss: 142.72864903808227  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1808 training Loss: 614.6391388176073 validation Loss: 142.67771989217113  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1809 training Loss: 614.4072380300736 validation Loss: 142.62683413042816  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1810 training Loss: 614.1755397645338 validation Loss: 142.57599169616836  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1811 training Loss: 613.9440437515199 validation Loss: 142.52519253280497  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1812 training Loss: 613.7127497220387 validation Loss: 142.47443658384964  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1813 training Loss: 613.4816574075719 validation Loss: 142.42372379291209  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1814 training Loss: 613.2507665400738 validation Loss: 142.3730541036999  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1815 training Loss: 613.0200768519716 validation Loss: 142.32242746001825  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1816 training Loss: 612.789588076163 validation Loss: 142.27184380576975  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1817 training Loss: 612.5592999460165 validation Loss: 142.22130308495431  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1818 training Loss: 612.3292121953693 validation Loss: 142.17080524166886  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1819 training Loss: 612.0993245585266 validation Loss: 142.120350220107  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1820 training Loss: 611.8696367702613 validation Loss: 142.0699379645592  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1821 training Loss: 611.6401485658118 validation Loss: 142.019568419412  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1822 training Loss: 611.4108596808819 validation Loss: 141.9692415291484  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1823 training Loss: 611.1817698516394 validation Loss: 141.91895723834728  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1824 training Loss: 610.9528788147147 validation Loss: 141.8687154916833  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1825 training Loss: 610.7241863072013 validation Loss: 141.8185162339267  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1826 training Loss: 610.4956920666527 validation Loss: 141.7683594099431  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1827 training Loss: 610.267395831083 validation Loss: 141.71824496469327  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1828 training Loss: 610.0392973389655 validation Loss: 141.66817284323298  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1829 training Loss: 609.8113963292308 validation Loss: 141.61814299071278  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1830 training Loss: 609.5836925412676 validation Loss: 141.5681553523777  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1831 training Loss: 609.35618571492 validation Loss: 141.51820987356717  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1832 training Loss: 609.1288755904874 validation Loss: 141.46830649971483  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1833 training Loss: 608.9017619087233 validation Loss: 141.4184451763482  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1834 training Loss: 608.6748444108346 validation Loss: 141.36862584908857  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1835 training Loss: 608.4481228384799 validation Loss: 141.31884846365082  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1836 training Loss: 608.2215969337693 validation Loss: 141.2691129658432  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1837 training Loss: 607.9952664392631 validation Loss: 141.2194193015671  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1838 training Loss: 607.7691310979708 validation Loss: 141.16976741681682  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1839 training Loss: 607.5431906533499 validation Loss: 141.12015725767947  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1840 training Loss: 607.3174448493061 validation Loss: 141.07058877033478  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1841 training Loss: 607.0918934301901 validation Loss: 141.02106190105476  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1842 training Loss: 606.8665361407993 validation Loss: 140.97157659620365  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1843 training Loss: 606.6413727263747 validation Loss: 140.92213280223766  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1844 training Loss: 606.4164029326012 validation Loss: 140.8727304657047  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1845 training Loss: 606.1916265056058 validation Loss: 140.82336953324443  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1846 training Loss: 605.9670431919574 validation Loss: 140.7740499515878  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1847 training Loss: 605.7426527386656 validation Loss: 140.72477166755695  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1848 training Loss: 605.5184548931794 validation Loss: 140.67553462806507  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1849 training Loss: 605.2944494033868 validation Loss: 140.62633878011616  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1850 training Loss: 605.0706360176134 validation Loss: 140.5771840708048  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1851 training Loss: 604.8470144846215 validation Loss: 140.5280704473161  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1852 training Loss: 604.6235845536098 validation Loss: 140.4789978569253  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1853 training Loss: 604.4003459742116 validation Loss: 140.42996624699776  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1854 training Loss: 604.1772984964945 validation Loss: 140.38097556498863  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1855 training Loss: 603.9544418709587 validation Loss: 140.3320257584428  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1856 training Loss: 603.7317758485373 validation Loss: 140.28311677499465  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1857 training Loss: 603.5093001805944 validation Loss: 140.23424856236778  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1858 training Loss: 603.2870146189242 validation Loss: 140.18542106837492  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1859 training Loss: 603.0649189157507 validation Loss: 140.13663424091774  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1860 training Loss: 602.843012823726 validation Loss: 140.0878880279866  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1861 training Loss: 602.6212960959301 validation Loss: 140.03918237766044  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1862 training Loss: 602.3997684858698 validation Loss: 139.9905172381065  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1863 training Loss: 602.1784297474771 validation Loss: 139.94189255758022  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1864 training Loss: 601.9572796351097 validation Loss: 139.89330828442505  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1865 training Loss: 601.7363179035485 validation Loss: 139.84476436707223  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1866 training Loss: 601.5155443079979 validation Loss: 139.79626075404047  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1867 training Loss: 601.2949586040841 validation Loss: 139.74779739393617  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1868 training Loss: 601.074560547855 validation Loss: 139.69937423545275  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1869 training Loss: 600.8543498957782 validation Loss: 139.6509912273708  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1870 training Loss: 600.6343264047416 validation Loss: 139.60264831855773  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1871 training Loss: 600.414489832051 validation Loss: 139.55434545796777  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1872 training Loss: 600.1948399354303 validation Loss: 139.5060825946414  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1873 training Loss: 599.9753764730199 validation Loss: 139.45785967770576  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1874 training Loss: 599.7560992033759 validation Loss: 139.40967665637385  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1875 training Loss: 599.5370078854701 validation Loss: 139.36153347994485  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1876 training Loss: 599.318102278688 validation Loss: 139.31343009780363  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1877 training Loss: 599.0993821428282 validation Loss: 139.26536645942073  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1878 training Loss: 598.880847238102 validation Loss: 139.21734251435203  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1879 training Loss: 598.6624973251322 validation Loss: 139.1693582122387  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1880 training Loss: 598.4443321649519 validation Loss: 139.12141350280706  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1881 training Loss: 598.2263515190044 validation Loss: 139.07350833586824  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1882 training Loss: 598.0085551491417 validation Loss: 139.02564266131813  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1883 training Loss: 597.7909428176237 validation Loss: 138.97781642913708  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1884 training Loss: 597.5735142871175 validation Loss: 138.93002958938993  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1885 training Loss: 597.3562693206968 validation Loss: 138.88228209222567  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1886 training Loss: 597.1392076818404 validation Loss: 138.83457388787724  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1887 training Loss: 596.922329134432 validation Loss: 138.78690492666146  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1888 training Loss: 596.7056334427585 validation Loss: 138.7392751589788  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1889 training Loss: 596.4891203715102 validation Loss: 138.6916845353133  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1890 training Loss: 596.2727896857795 validation Loss: 138.64413300623218  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1891 training Loss: 596.0566411510592 validation Loss: 138.59662052238588  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1892 training Loss: 595.8406745332434 validation Loss: 138.54914703450777  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1893 training Loss: 595.6248895986251 validation Loss: 138.50171249341412  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1894 training Loss: 595.4092861138961 validation Loss: 138.4543168500037  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1895 training Loss: 595.1938638461459 validation Loss: 138.4069600552577  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1896 training Loss: 594.9786225628611 validation Loss: 138.35964206023976  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1897 training Loss: 594.7635620319244 validation Loss: 138.31236281609551  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1898 training Loss: 594.5486820216138 validation Loss: 138.2651222740525  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1899 training Loss: 594.3339823006017 validation Loss: 138.2179203854201  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1900 training Loss: 594.1194626379543 validation Loss: 138.17075710158926  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1901 training Loss: 593.9051228031303 validation Loss: 138.1236323740324  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1902 training Loss: 593.6909625659807 validation Loss: 138.07654615430312  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1903 training Loss: 593.4769816967474 validation Loss: 138.02949839403618  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1904 training Loss: 593.2631799660628 validation Loss: 137.98248904494722  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1905 training Loss: 593.0495571449488 validation Loss: 137.93551805883266  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1906 training Loss: 592.8361130048158 validation Loss: 137.88858538756944  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1907 training Loss: 592.6228473174625 validation Loss: 137.8416909831151  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1908 training Loss: 592.4097598550742 validation Loss: 137.79483479750718  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1909 training Loss: 592.1968503902226 validation Loss: 137.74801678286354  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1910 training Loss: 591.984118695865 validation Loss: 137.7012368913818  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1911 training Loss: 591.7715645453433 validation Loss: 137.65449507533947  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1912 training Loss: 591.5591877123829 validation Loss: 137.60779128709348  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1913 training Loss: 591.3469879710929 validation Loss: 137.56112547908037  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1914 training Loss: 591.1349650959637 validation Loss: 137.5144976038159  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1915 training Loss: 590.9231188618681 validation Loss: 137.46790761389485  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1916 training Loss: 590.711449044059 validation Loss: 137.42135546199097  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1917 training Loss: 590.4999554181691 validation Loss: 137.37484110085683  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1918 training Loss: 590.2886377602101 validation Loss: 137.32836448332358  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1919 training Loss: 590.0774958465724 validation Loss: 137.2819255623008  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1920 training Loss: 589.8665294540235 validation Loss: 137.23552429077643  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1921 training Loss: 589.6557383597074 validation Loss: 137.18916062181643  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1922 training Loss: 589.4451223411444 validation Loss: 137.14283450856482  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1923 training Loss: 589.2346811762296 validation Loss: 137.09654590424336  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1924 training Loss: 589.0244146432326 validation Loss: 137.05029476215154  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1925 training Loss: 588.8143225207964 validation Loss: 137.00408103566622  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1926 training Loss: 588.6044045879369 validation Loss: 136.95790467824173  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1927 training Loss: 588.3946606240418 validation Loss: 136.9117656434094  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1928 training Loss: 588.1850904088701 validation Loss: 136.8656638847777  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1929 training Loss: 587.9756937225513 validation Loss: 136.8195993560319  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1930 training Loss: 587.7664703455846 validation Loss: 136.77357201093395  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1931 training Loss: 587.5574200588378 validation Loss: 136.72758180332244  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1932 training Loss: 587.3485426435473 validation Loss: 136.68162868711215  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1933 training Loss: 587.1398378813165 validation Loss: 136.63571261629426  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1934 training Loss: 586.9313055541154 validation Loss: 136.5898335449359  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1935 training Loss: 586.72294544428 validation Loss: 136.54399142718023  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1936 training Loss: 586.5147573345114 validation Loss: 136.49818621724603  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1937 training Loss: 586.306741007875 validation Loss: 136.45241786942776  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1938 training Loss: 586.0988962477995 validation Loss: 136.4066863380953  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1939 training Loss: 585.8912228380768 validation Loss: 136.36099157769388  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1940 training Loss: 585.6837205628609 validation Loss: 136.31533354274376  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1941 training Loss: 585.4763892066666 validation Loss: 136.2697121878402  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1942 training Loss: 585.2692285543696 validation Loss: 136.2241274676534  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1943 training Loss: 585.0622383912056 validation Loss: 136.17857933692818  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1944 training Loss: 584.855418502769 validation Loss: 136.1330677504838  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1945 training Loss: 584.6487686750128 validation Loss: 136.08759266321402  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1946 training Loss: 584.4422886942475 validation Loss: 136.04215403008675  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1947 training Loss: 584.2359783471403 validation Loss: 135.996751806144  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1948 training Loss: 584.0298374207149 validation Loss: 135.95138594650163  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1949 training Loss: 583.8238657023498 validation Loss: 135.90605640634936  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1950 training Loss: 583.618062979779 validation Loss: 135.86076314095052  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1951 training Loss: 583.4124290410896 validation Loss: 135.81550610564182  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1952 training Loss: 583.2069636747221 validation Loss: 135.77028525583341  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1953 training Loss: 583.0016666694698 validation Loss: 135.72510054700848  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1954 training Loss: 582.7965378144772 validation Loss: 135.67995193472336  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1955 training Loss: 582.5915768992404 validation Loss: 135.63483937460717  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1956 training Loss: 582.3867837136052 validation Loss: 135.58976282236176  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1957 training Loss: 582.1821580477671 validation Loss: 135.5447222337616  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1958 training Loss: 581.9776996922709 validation Loss: 135.49971756465362  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1959 training Loss: 581.7734084380089 validation Loss: 135.45474877095688  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1960 training Loss: 581.5692840762213 validation Loss: 135.40981580866273  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1961 training Loss: 581.3653263984947 validation Loss: 135.36491863383446  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1962 training Loss: 581.1615351967616 validation Loss: 135.32005720260713  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1963 training Loss: 580.9579102633004 validation Loss: 135.27523147118765  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1964 training Loss: 580.754451390733 validation Loss: 135.2304413958543  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1965 training Loss: 580.5511583720263 validation Loss: 135.1856869329569  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1966 training Loss: 580.3480310004893 validation Loss: 135.14096803891655  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1967 training Loss: 580.1450690697742 validation Loss: 135.09628467022532  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1968 training Loss: 579.9422723738746 validation Loss: 135.05163678344636  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1969 training Loss: 579.7396407071253 validation Loss: 135.00702433521366  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1970 training Loss: 579.5371738642013 validation Loss: 134.96244728223186  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1971 training Loss: 579.3348716401174 validation Loss: 134.91790558127616  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1972 training Loss: 579.1327338302272 validation Loss: 134.87339918919213  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1973 training Loss: 578.9307602302224 validation Loss: 134.82892806289567  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1974 training Loss: 578.728950636133 validation Loss: 134.78449215937272  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1975 training Loss: 578.5273048443248 validation Loss: 134.74009143567918  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1976 training Loss: 578.3258226515006 validation Loss: 134.69572584894098  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1977 training Loss: 578.1245038546987 validation Loss: 134.65139535635348  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1978 training Loss: 577.9233482512914 validation Loss: 134.60709991518172  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1979 training Loss: 577.722355638986 validation Loss: 134.56283948276018  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1980 training Loss: 577.521525815823 validation Loss: 134.51861401649256  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1981 training Loss: 577.3208585801752 validation Loss: 134.47442347385177  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1982 training Loss: 577.1203537307485 validation Loss: 134.4302678123796  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1983 training Loss: 576.9200110665793 validation Loss: 134.38614698968678  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1984 training Loss: 576.7198303870349 validation Loss: 134.34206096345272  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1985 training Loss: 576.5198114918129 validation Loss: 134.29800969142545  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1986 training Loss: 576.3199541809404 validation Loss: 134.25399313142134  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1987 training Loss: 576.1202582547727 validation Loss: 134.21001124132528  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1988 training Loss: 575.9207235139935 validation Loss: 134.16606397909007  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1989 training Loss: 575.7213497596141 validation Loss: 134.1221513027367  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1990 training Loss: 575.5221367929721 validation Loss: 134.078273170354  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1991 training Loss: 575.3230844157313 validation Loss: 134.03442954009859  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1992 training Loss: 575.1241924298808 validation Loss: 133.99062037019468  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1993 training Loss: 574.9254606377347 validation Loss: 133.946845618934  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1994 training Loss: 574.726888841931 validation Loss: 133.90310524467554  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1995 training Loss: 574.528476845431 validation Loss: 133.85939920584565  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1996 training Loss: 574.3302244515191 validation Loss: 133.81572746093767  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1997 training Loss: 574.1321314638016 validation Loss: 133.77208996851186  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1998 training Loss: 573.9341976862058 validation Loss: 133.72848668719539  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 1999 training Loss: 573.7364229229808 validation Loss: 133.68491757568205  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2000 training Loss: 573.5388069786949 validation Loss: 133.64138259273216  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2001 training Loss: 573.3413496582364 validation Loss: 133.5978816971725  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2002 training Loss: 573.1440507668124 validation Loss: 133.5544148478961  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2003 training Loss: 572.946910109948 validation Loss: 133.51098200386215  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2004 training Loss: 572.7499274934862 validation Loss: 133.46758312409585  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2005 training Loss: 572.5531027235866 validation Loss: 133.4242181676883  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2006 training Loss: 572.3564356067252 validation Loss: 133.3808870937963  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2007 training Loss: 572.1599259496936 validation Loss: 133.33758986164236  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2008 training Loss: 571.9635735595987 validation Loss: 133.2943264305144  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2009 training Loss: 571.7673782438612 validation Loss: 133.25109675976572  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2010 training Loss: 571.5713398102162 validation Loss: 133.20790080881494  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2011 training Loss: 571.3754580667112 validation Loss: 133.16473853714564  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2012 training Loss: 571.1797328217067 validation Loss: 133.12160990430644  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2013 training Loss: 570.9841638838752 validation Loss: 133.0785148699108  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2014 training Loss: 570.7887510621997 validation Loss: 133.03545339363697  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2015 training Loss: 570.5934941659741 validation Loss: 132.9924254352276  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2016 training Loss: 570.3983930048028 validation Loss: 132.94943095448994  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2017 training Loss: 570.2034473885985 validation Loss: 132.90646991129557  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2018 training Loss: 570.0086571275838 validation Loss: 132.86354226558018  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2019 training Loss: 569.8140220322882 validation Loss: 132.82064797734358  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2020 training Loss: 569.6195419135496 validation Loss: 132.77778700664962  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2021 training Loss: 569.4252165825123 validation Loss: 132.73495931362572  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2022 training Loss: 569.231045850627 validation Loss: 132.6921648584633  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2023 training Loss: 569.0370295296498 validation Loss: 132.64940360141708  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2024 training Loss: 568.8431674316421 validation Loss: 132.6066755028054  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2025 training Loss: 568.6494593689697 validation Loss: 132.56398052300983  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2026 training Loss: 568.4559051543017 validation Loss: 132.52131862247515  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2027 training Loss: 568.262504600611 validation Loss: 132.4786897617092  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2028 training Loss: 568.0692575211729 validation Loss: 132.4360939012828  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2029 training Loss: 567.8761637295644 validation Loss: 132.39353100182947  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2030 training Loss: 567.683223039664 validation Loss: 132.35100102404553  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2031 training Loss: 567.4904352656511 validation Loss: 132.30850392868987  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2032 training Loss: 567.2978002220051 validation Loss: 132.26603967658372  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2033 training Loss: 567.1053177235049 validation Loss: 132.22360822861074  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2034 training Loss: 566.9129875852286 validation Loss: 132.18120954571674  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2035 training Loss: 566.7208096225525 validation Loss: 132.1388435889096  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2036 training Loss: 566.5287836511504 validation Loss: 132.09651031925912  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2037 training Loss: 566.3369094869939 validation Loss: 132.05420969789702  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2038 training Loss: 566.1451869463506 validation Loss: 132.01194168601666  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2039 training Loss: 565.9536158457842 validation Loss: 131.96970624487295  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2040 training Loss: 565.7621960021538 validation Loss: 131.92750333578235  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2041 training Loss: 565.5709272326137 validation Loss: 131.88533292012266  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2042 training Loss: 565.3798093546119 validation Loss: 131.8431949593328  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2043 training Loss: 565.1888421858902 validation Loss: 131.80108941491292  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2044 training Loss: 564.9980255444832 validation Loss: 131.75901624842408  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2045 training Loss: 564.8073592487185 validation Loss: 131.71697542148817  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2046 training Loss: 564.616843117215 validation Loss: 131.6749668957879  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2047 training Loss: 564.4264769688835 validation Loss: 131.63299063306658  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2048 training Loss: 564.236260622925 validation Loss: 131.591046595128  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2049 training Loss: 564.0461938988307 validation Loss: 131.54913474383636  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2050 training Loss: 563.8562766163815 validation Loss: 131.50725504111614  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2051 training Loss: 563.6665085956472 validation Loss: 131.46540744895185  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2052 training Loss: 563.4768896569863 validation Loss: 131.42359192938824  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2053 training Loss: 563.2874196210446 validation Loss: 131.38180844452978  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2054 training Loss: 563.0980983087555 validation Loss: 131.34005695654088  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2055 training Loss: 562.9089255413393 validation Loss: 131.2983374276455  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2056 training Loss: 562.7199011403018 validation Loss: 131.25664982012722  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2057 training Loss: 562.5310249274348 validation Loss: 131.21499409632915  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2058 training Loss: 562.342296724815 validation Loss: 131.1733702186536  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2059 training Loss: 562.1537163548038 validation Loss: 131.13177814956208  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2060 training Loss: 561.965283640046 validation Loss: 131.0902178515753  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2061 training Loss: 561.7769984034696 validation Loss: 131.04868928727294  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2062 training Loss: 561.5888604682862 validation Loss: 131.0071924192935  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2063 training Loss: 561.4008696579888 validation Loss: 130.96572721033422  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2064 training Loss: 561.2130257963519 validation Loss: 130.92429362315096  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2065 training Loss: 561.025328707432 validation Loss: 130.88289162055824  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2066 training Loss: 560.8377782155649 validation Loss: 130.84152116542884  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2067 training Loss: 560.6503741453673 validation Loss: 130.80018222069384  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2068 training Loss: 560.4631163217349 validation Loss: 130.75887474934262  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2069 training Loss: 560.2760045698424 validation Loss: 130.71759871442254  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2070 training Loss: 560.0890387151426 validation Loss: 130.67635407903887  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2071 training Loss: 559.9022185833662 validation Loss: 130.63514080635483  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2072 training Loss: 559.7155440005212 validation Loss: 130.59395885959123  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2073 training Loss: 559.529014792892 validation Loss: 130.5528082020267  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2074 training Loss: 559.3426307870391 validation Loss: 130.51168879699713  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2075 training Loss: 559.1563918097987 validation Loss: 130.470600607896  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2076 training Loss: 558.9702976882824 validation Loss: 130.42954359817398  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2077 training Loss: 558.7843482498756 validation Loss: 130.38851773133888  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2078 training Loss: 558.5985433222381 validation Loss: 130.34752297095565  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2079 training Loss: 558.4128827333029 validation Loss: 130.30655928064616  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2080 training Loss: 558.2273663112758 validation Loss: 130.26562662408907  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2081 training Loss: 558.0419938846353 validation Loss: 130.22472496501985  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2082 training Loss: 557.8567652821312 validation Loss: 130.18385426723046  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2083 training Loss: 557.671680332785 validation Loss: 130.14301449456946  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2084 training Loss: 557.4867388658885 validation Loss: 130.10220561094184  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2085 training Loss: 557.301940711004 validation Loss: 130.0614275803088  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2086 training Loss: 557.1172856979636 validation Loss: 130.0206803666877  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2087 training Loss: 556.9327736568682 validation Loss: 129.97996393415212  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2088 training Loss: 556.7484044180874 validation Loss: 129.9392782468314  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2089 training Loss: 556.5641778122588 validation Loss: 129.89862326891088  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2090 training Loss: 556.380093670288 validation Loss: 129.85799896463158  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2091 training Loss: 556.1961518233472 validation Loss: 129.81740529829017  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2092 training Loss: 556.0123521028752 validation Loss: 129.77684223423887  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2093 training Loss: 555.8286943405767 validation Loss: 129.73630973688523  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2094 training Loss: 555.6451783684222 validation Loss: 129.6958077706923  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2095 training Loss: 555.4618040186468 validation Loss: 129.6553363001782  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2096 training Loss: 555.2785711237501 validation Loss: 129.61489528991612  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2097 training Loss: 555.0954795164959 validation Loss: 129.57448470453434  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2098 training Loss: 554.912529029911 validation Loss: 129.534104508716  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2099 training Loss: 554.7297194972851 validation Loss: 129.49375466719903  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2100 training Loss: 554.5470507521709 validation Loss: 129.45343514477605  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2101 training Loss: 554.3645226283822 validation Loss: 129.41314590629412  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2102 training Loss: 554.1821349599944 validation Loss: 129.37288691665503  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2103 training Loss: 553.9998875813438 validation Loss: 129.33265814081466  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2104 training Loss: 553.8177803270272 validation Loss: 129.29245954378337  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2105 training Loss: 553.6358130319009 validation Loss: 129.25229109062548  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2106 training Loss: 553.4539855310807 validation Loss: 129.2121527464595  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2107 training Loss: 553.2722976599409 validation Loss: 129.17204447645787  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2108 training Loss: 553.0907492541148 validation Loss: 129.1319662458468  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2109 training Loss: 552.9093401494929 validation Loss: 129.0919180199063  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2110 training Loss: 552.7280701822231 validation Loss: 129.05189976396997  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2111 training Loss: 552.5469391887102 validation Loss: 129.011911443425  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2112 training Loss: 552.3659470056155 validation Loss: 128.971953023712  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2113 training Loss: 552.1850934698555 validation Loss: 128.93202447032485  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2114 training Loss: 552.0043784186028 validation Loss: 128.8921257488107  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2115 training Loss: 551.8238016892842 validation Loss: 128.8522568247698  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2116 training Loss: 551.6433631195811 validation Loss: 128.8124176638555  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2117 training Loss: 551.4630625474288 validation Loss: 128.77260823177397  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2118 training Loss: 551.2828998110153 validation Loss: 128.73282849428423  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2119 training Loss: 551.1028747487827 validation Loss: 128.69307841719802  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2120 training Loss: 550.9229871994243 validation Loss: 128.65335796637976  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2121 training Loss: 550.7432370018855 validation Loss: 128.61366710774627  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2122 training Loss: 550.5636239953634 validation Loss: 128.57400580726684  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2123 training Loss: 550.3841480193059 validation Loss: 128.53437403096314  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2124 training Loss: 550.2048089134112 validation Loss: 128.49477174490895  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2125 training Loss: 550.0256065176272 validation Loss: 128.4551989152302  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2126 training Loss: 549.8465406721518 validation Loss: 128.41565550810486  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2127 training Loss: 549.6676112174314 validation Loss: 128.3761414897628  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2128 training Loss: 549.4888179941609 validation Loss: 128.33665682648575  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2129 training Loss: 549.3101608432837 validation Loss: 128.29720148460711  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2130 training Loss: 549.1316396059899 validation Loss: 128.25777543051188  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2131 training Loss: 548.9532541237171 validation Loss: 128.21837863063664  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2132 training Loss: 548.7750042381497 validation Loss: 128.1790110514694  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2133 training Loss: 548.5968897912176 validation Loss: 128.1396726595494  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2134 training Loss: 548.4189106250967 validation Loss: 128.1003634214672  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2135 training Loss: 548.2410665822078 validation Loss: 128.0610833038645  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2136 training Loss: 548.0633575052168 validation Loss: 128.02183227343394  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2137 training Loss: 547.8857832370333 validation Loss: 127.98261029691925  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2138 training Loss: 547.7083436208109 validation Loss: 127.94341734111481  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2139 training Loss: 547.5310384999461 validation Loss: 127.90425337286588  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2140 training Loss: 547.3538677180788 validation Loss: 127.86511835906835  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2141 training Loss: 547.1768311190908 validation Loss: 127.82601226666863  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2142 training Loss: 546.9999285471058 validation Loss: 127.78693506266356  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2143 training Loss: 546.8231598464887 validation Loss: 127.74788671410039  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2144 training Loss: 546.6465248618458 validation Loss: 127.70886718807661  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2145 training Loss: 546.4700234380235 validation Loss: 127.66987645173991  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2146 training Loss: 546.2936554201082 validation Loss: 127.63091447228803  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2147 training Loss: 546.1174206534263 validation Loss: 127.59198121696865  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2148 training Loss: 545.9413189835423 validation Loss: 127.5530766530794  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2149 training Loss: 545.7653502562604 validation Loss: 127.51420074796766  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2150 training Loss: 545.5895143176224 validation Loss: 127.4753534690305  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2151 training Loss: 545.4138110139077 validation Loss: 127.4365347837146  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2152 training Loss: 545.2382401916333 validation Loss: 127.3977446595162  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2153 training Loss: 545.0628016975528 validation Loss: 127.3589830639809  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2154 training Loss: 544.8874953786562 validation Loss: 127.32024996470355  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2155 training Loss: 544.7123210821695 validation Loss: 127.2815453293284  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2156 training Loss: 544.5372786555538 validation Loss: 127.2428691255487  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2157 training Loss: 544.3623679465055 validation Loss: 127.20422132110674  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2158 training Loss: 544.1875888029554 validation Loss: 127.16560188379385  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2159 training Loss: 544.0129410730686 validation Loss: 127.12701078145015  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2160 training Loss: 543.8384246052434 validation Loss: 127.08844798196455  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2161 training Loss: 543.6640392481119 validation Loss: 127.0499134532746  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2162 training Loss: 543.4897848505384 validation Loss: 127.01140716336644  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2163 training Loss: 543.3156612616199 validation Loss: 126.97292908027475  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2164 training Loss: 543.1416683306848 validation Loss: 126.93447917208258  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2165 training Loss: 542.9678059072936 validation Loss: 126.89605740692122  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2166 training Loss: 542.7940738412373 validation Loss: 126.85766375297024  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2167 training Loss: 542.6204719825373 validation Loss: 126.81929817845736  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2168 training Loss: 542.4470001814457 validation Loss: 126.7809606516583  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2169 training Loss: 542.2736582884438 validation Loss: 126.7426511408967  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2170 training Loss: 542.1004461542423 validation Loss: 126.70436961454409  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2171 training Loss: 541.9273636297804 validation Loss: 126.66611604101975  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2172 training Loss: 541.7544105662262 validation Loss: 126.62789038879062  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2173 training Loss: 541.5815868149751 validation Loss: 126.5896926263713  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2174 training Loss: 541.4088922276507 validation Loss: 126.55152272232381  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2175 training Loss: 541.236326656103 validation Loss: 126.51338064525754  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2176 training Loss: 541.063889952409 validation Loss: 126.47526636382933  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2177 training Loss: 540.8915819688717 validation Loss: 126.43717984674313  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2178 training Loss: 540.7194025580201 validation Loss: 126.39912106275013  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2179 training Loss: 540.5473515726081 validation Loss: 126.3610899806484  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2180 training Loss: 540.3754288656152 validation Loss: 126.32308656928319  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2181 training Loss: 540.2036342902445 validation Loss: 126.28511079754651  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2182 training Loss: 540.031967699924 validation Loss: 126.24716263437716  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2183 training Loss: 539.8604289483051 validation Loss: 126.20924204876064  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2184 training Loss: 539.6890178892619 validation Loss: 126.17134900972914  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2185 training Loss: 539.5177343768919 validation Loss: 126.13348348636127  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2186 training Loss: 539.3465782655146 validation Loss: 126.09564544778215  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2187 training Loss: 539.1755494096717 validation Loss: 126.05783486316318  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2188 training Loss: 539.0046476641264 validation Loss: 126.02005170172212  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2189 training Loss: 538.8338728838626 validation Loss: 125.98229593272285  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2190 training Loss: 538.6632249240854 validation Loss: 125.9445675254754  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2191 training Loss: 538.4927036402199 validation Loss: 125.90686644933574  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2192 training Loss: 538.3223088879114 validation Loss: 125.86919267370575  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2193 training Loss: 538.1520405230241 validation Loss: 125.83154616803326  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2194 training Loss: 537.9818984016415 validation Loss: 125.79392690181172  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2195 training Loss: 537.8118823800658 validation Loss: 125.75633484458032  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2196 training Loss: 537.6419923148172 validation Loss: 125.71876996592383  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2197 training Loss: 537.4722280626339 validation Loss: 125.68123223547249  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2198 training Loss: 537.3025894804715 validation Loss: 125.64372162290198  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2199 training Loss: 537.1330764255023 validation Loss: 125.60623809793327  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2200 training Loss: 536.9636887551152 validation Loss: 125.5687816303326  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2201 training Loss: 536.7944263269155 validation Loss: 125.53135218991136  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2202 training Loss: 536.6252889987243 validation Loss: 125.49394974652598  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2203 training Loss: 536.4562766285775 validation Loss: 125.45657427007797  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2204 training Loss: 536.2873890747264 validation Loss: 125.41922573051367  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2205 training Loss: 536.1186261956367 validation Loss: 125.38190409782429  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2206 training Loss: 535.9499878499884 validation Loss: 125.3446093420457  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2207 training Loss: 535.7814738966749 validation Loss: 125.30734143325854  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2208 training Loss: 535.6130841948033 validation Loss: 125.27010034158796  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2209 training Loss: 535.4448186036932 validation Loss: 125.2328860372036  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2210 training Loss: 535.276676982877 validation Loss: 125.19569849031953  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2211 training Loss: 535.1086591920991 validation Loss: 125.15853767119413  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2212 training Loss: 534.9407650913158 validation Loss: 125.12140355013005  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2213 training Loss: 534.7729945406948 validation Loss: 125.08429609747412  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2214 training Loss: 534.6053474006143 validation Loss: 125.04721528361719  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2215 training Loss: 534.4378235316634 validation Loss: 125.01016107899414  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2216 training Loss: 534.2704227946413 validation Loss: 124.97313345408382  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2217 training Loss: 534.1031450505568 validation Loss: 124.9361323794088  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2218 training Loss: 533.9359901606281 validation Loss: 124.89915782553551  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2219 training Loss: 533.7689579862827 validation Loss: 124.86220976307405  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2220 training Loss: 533.6020483891562 validation Loss: 124.82528816267808  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2221 training Loss: 533.4352612310926 validation Loss: 124.78839299504475  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2222 training Loss: 533.2685963741437 validation Loss: 124.75152423091473  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2223 training Loss: 533.102053680569 validation Loss: 124.71468184107198  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2224 training Loss: 532.9356330128344 validation Loss: 124.67786579634372  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2225 training Loss: 532.7693342336128 validation Loss: 124.64107606760042  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2226 training Loss: 532.6031572057832 validation Loss: 124.60431262575565  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2227 training Loss: 532.4371017924309 validation Loss: 124.56757544176601  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2228 training Loss: 532.2711678568463 validation Loss: 124.53086448663102  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2229 training Loss: 532.1053552625247 validation Loss: 124.49417973139315  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2230 training Loss: 531.9396638731668 validation Loss: 124.45752114713758  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2231 training Loss: 531.7740935526767 validation Loss: 124.42088870499235  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2232 training Loss: 531.6086441651635 validation Loss: 124.38428237612797  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2233 training Loss: 531.4433155749389 validation Loss: 124.34770213175761  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2234 training Loss: 531.2781076465185 validation Loss: 124.31114794313697  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2235 training Loss: 531.1130202446204 validation Loss: 124.27461978156403  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2236 training Loss: 530.9480532341654 validation Loss: 124.23811761837919  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2237 training Loss: 530.7832064802759 validation Loss: 124.20164142496509  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2238 training Loss: 530.6184798482764 validation Loss: 124.16519117274652  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2239 training Loss: 530.4538732036924 validation Loss: 124.12876683319041  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2240 training Loss: 530.2893864122507 validation Loss: 124.09236837780564  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2241 training Loss: 530.1250193398786 validation Loss: 124.05599577814307  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2242 training Loss: 529.9607718527031 validation Loss: 124.01964900579546  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2243 training Loss: 529.7966438170516 validation Loss: 123.98332803239731  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2244 training Loss: 529.6326350994511 validation Loss: 123.94703282962486  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2245 training Loss: 529.4687455666269 validation Loss: 123.91076336919596  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2246 training Loss: 529.3049750855035 validation Loss: 123.87451962287001  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2247 training Loss: 529.1413235232039 validation Loss: 123.83830156244798  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2248 training Loss: 528.9777907470489 validation Loss: 123.80210915977216  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2249 training Loss: 528.8143766245568 validation Loss: 123.7659423867262  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2250 training Loss: 528.651081023443 validation Loss: 123.72980121523497  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2251 training Loss: 528.4879038116205 validation Loss: 123.6936856172646  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2252 training Loss: 528.3248448571982 validation Loss: 123.65759556482226  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2253 training Loss: 528.1619040284811 validation Loss: 123.62153102995624  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2254 training Loss: 527.9990811939706 validation Loss: 123.58549198475565  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2255 training Loss: 527.8363762223628 validation Loss: 123.54947840135056  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2256 training Loss: 527.6737889825495 validation Loss: 123.51349025191186  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2257 training Loss: 527.5113193436166 validation Loss: 123.47752750865118  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2258 training Loss: 527.3489671748449 validation Loss: 123.44159014382075  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2259 training Loss: 527.186732345709 validation Loss: 123.40567812971341  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2260 training Loss: 527.0246147258769 validation Loss: 123.36979143866253  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2261 training Loss: 526.8626141852102 validation Loss: 123.33393004304193  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2262 training Loss: 526.7007305937632 validation Loss: 123.29809391526572  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2263 training Loss: 526.5389638217832 validation Loss: 123.26228302778838  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2264 training Loss: 526.377313739709 validation Loss: 123.22649735310459  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2265 training Loss: 526.2157802181719 validation Loss: 123.19073686374914  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2266 training Loss: 526.0543631279943 validation Loss: 123.1550015322969  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2267 training Loss: 525.8930623401898 validation Loss: 123.11929133136275  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2268 training Loss: 525.7318777259632 validation Loss: 123.0836062336015  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2269 training Loss: 525.5708091567092 validation Loss: 123.0479462117078  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2270 training Loss: 525.4098565040129 validation Loss: 123.0123112384161  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2271 training Loss: 525.2490196396493 validation Loss: 122.97670128650057  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2272 training Loss: 525.0882984355826 validation Loss: 122.94111632877497  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2273 training Loss: 524.9276927639659 validation Loss: 122.90555633809257  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2274 training Loss: 524.7672024971412 validation Loss: 122.8700212873463  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2275 training Loss: 524.6068275076389 validation Loss: 122.83451114946841  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2276 training Loss: 524.4465676681772 validation Loss: 122.79902589743051  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2277 training Loss: 524.2864228516623 validation Loss: 122.76356550424345  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2278 training Loss: 524.1263929311875 validation Loss: 122.72812994295734  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2279 training Loss: 523.9664777800333 validation Loss: 122.69271918666146  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2280 training Loss: 523.8066772716664 validation Loss: 122.65733320848406  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2281 training Loss: 523.6469912797404 validation Loss: 122.62197198159244  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2282 training Loss: 523.4874196780944 validation Loss: 122.58663547919286  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2283 training Loss: 523.3279623407532 validation Loss: 122.55132367453034  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2284 training Loss: 523.1686191419271 validation Loss: 122.51603654088878  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2285 training Loss: 523.0093899560109 validation Loss: 122.48077405159071  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2286 training Loss: 522.8502746575848 validation Loss: 122.4455361799974  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2287 training Loss: 522.6912731214123 validation Loss: 122.4103228995086  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2288 training Loss: 522.5323852224417 validation Loss: 122.37513418356264  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2289 training Loss: 522.3736108358041 validation Loss: 122.33997000563625  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2290 training Loss: 522.2149498368146 validation Loss: 122.3048303392445  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2291 training Loss: 522.0564021009707 validation Loss: 122.26971515794085  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2292 training Loss: 521.8979675039527 validation Loss: 122.23462443531687  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2293 training Loss: 521.7396459216232 validation Loss: 122.19955814500237  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2294 training Loss: 521.5814372300265 validation Loss: 122.16451626066524  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2295 training Loss: 521.4233413053888 validation Loss: 122.12949875601136  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2296 training Loss: 521.2653580241174 validation Loss: 122.09450560478459  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2297 training Loss: 521.1074872628006 validation Loss: 122.05953678076668  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2298 training Loss: 520.9497288982072 validation Loss: 122.02459225777719  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2299 training Loss: 520.7920828072865 validation Loss: 121.98967200967343  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2300 training Loss: 520.6345488671673 validation Loss: 121.95477601035037  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2301 training Loss: 520.4771269551585 validation Loss: 121.91990423374065  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2302 training Loss: 520.319816948748 validation Loss: 121.88505665381439  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2303 training Loss: 520.1626187256027 validation Loss: 121.85023324457924  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2304 training Loss: 520.0055321635687 validation Loss: 121.81543398008024  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2305 training Loss: 519.8485571406693 validation Loss: 121.7806588343998  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2306 training Loss: 519.6916935351069 validation Loss: 121.74590778165758  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2307 training Loss: 519.534941225261 validation Loss: 121.71118079601044  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2308 training Loss: 519.3783000896885 validation Loss: 121.67647785165244  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2309 training Loss: 519.2217700071236 validation Loss: 121.6417989228147  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2310 training Loss: 519.0653508564768 validation Loss: 121.60714398376527  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2311 training Loss: 518.9090425168351 validation Loss: 121.57251300880927  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2312 training Loss: 518.752844867462 validation Loss: 121.5379059722886  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2313 training Loss: 518.5967577877964 validation Loss: 121.50332284858209  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2314 training Loss: 518.4407811574524 validation Loss: 121.46876361210519  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2315 training Loss: 518.2849148562195 validation Loss: 121.4342282373101  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2316 training Loss: 518.1291587640621 validation Loss: 121.39971669868561  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2317 training Loss: 517.9735127611191 validation Loss: 121.3652289707571  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2318 training Loss: 517.8179767277029 validation Loss: 121.3307650280864  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2319 training Loss: 517.662550544301 validation Loss: 121.29632484527181  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2320 training Loss: 517.5072340915731 validation Loss: 121.26190839694794  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2321 training Loss: 517.3520272503531 validation Loss: 121.22751565778566  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2322 training Loss: 517.1969299016473 validation Loss: 121.19314660249216  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2323 training Loss: 517.0419419266348 validation Loss: 121.15880120581072  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2324 training Loss: 516.8870632066669 validation Loss: 121.12447944252074  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2325 training Loss: 516.7322936232671 validation Loss: 121.09018128743772  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2326 training Loss: 516.5776330581303 validation Loss: 121.05590671541296  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2327 training Loss: 516.4230813931229 validation Loss: 121.02165570133383  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2328 training Loss: 516.268638510282 validation Loss: 120.98742822012348  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2329 training Loss: 516.1143042918161 validation Loss: 120.95322424674083  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2330 training Loss: 515.9600786201038 validation Loss: 120.91904375618051  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2331 training Loss: 515.8059613776936 validation Loss: 120.88488672347283  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2332 training Loss: 515.6519524473042 validation Loss: 120.85075312368372  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2333 training Loss: 515.4980517118236 validation Loss: 120.81664293191452  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2334 training Loss: 515.3442590543091 validation Loss: 120.78255612330213  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2335 training Loss: 515.1905743579869 validation Loss: 120.74849267301883  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2336 training Loss: 515.0369975062519 validation Loss: 120.7144525562722  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2337 training Loss: 514.883528382667 validation Loss: 120.68043574830514  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2338 training Loss: 514.7301668709633 validation Loss: 120.64644222439571  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2339 training Loss: 514.5769128550398 validation Loss: 120.61247195985717  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2340 training Loss: 514.4237662189626 validation Loss: 120.57852493003784  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2341 training Loss: 514.2707268469649 validation Loss: 120.54460111032104  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2342 training Loss: 514.1177946234468 validation Loss: 120.51070047612512  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2343 training Loss: 513.9649694329751 validation Loss: 120.47682300290325  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2344 training Loss: 513.8122511602824 validation Loss: 120.44296866614349  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2345 training Loss: 513.6596396902675 validation Loss: 120.40913744136864  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2346 training Loss: 513.5071349079946 validation Loss: 120.37532930413627  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2347 training Loss: 513.3547366986934 validation Loss: 120.3415442300386  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2348 training Loss: 513.2024449477585 validation Loss: 120.30778219470233  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2349 training Loss: 513.0502595407494 validation Loss: 120.27404317378884  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2350 training Loss: 512.8981803633897 validation Loss: 120.24032714299388  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2351 training Loss: 512.7462073015674 validation Loss: 120.20663407804767  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2352 training Loss: 512.5943402413343 validation Loss: 120.17296395471473  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2353 training Loss: 512.4425790689057 validation Loss: 120.13931674879392  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2354 training Loss: 512.29092367066 validation Loss: 120.10569243611829  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2355 training Loss: 512.1393739331389 validation Loss: 120.07209099255506  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2356 training Loss: 511.9879297430466 validation Loss: 120.03851239400558  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2357 training Loss: 511.83659098724956 validation Loss: 120.00495661640522  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2358 training Loss: 511.68535755277657 validation Loss: 119.97142363572337  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2359 training Loss: 511.53422932681804 validation Loss: 119.93791342796331  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2360 training Loss: 511.38320619672584 validation Loss: 119.9044259691622  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2361 training Loss: 511.2322880500133 validation Loss: 119.87096123539102  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2362 training Loss: 511.0814747743547 validation Loss: 119.83751920275449  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2363 training Loss: 510.93076625758465 validation Loss: 119.80409984739106  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2364 training Loss: 510.7801623876985 validation Loss: 119.77070314547274  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2365 training Loss: 510.6296630528514 validation Loss: 119.73732907320515  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2366 training Loss: 510.4792681413585 validation Loss: 119.70397760682741  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2367 training Loss: 510.32897754169426 validation Loss: 119.6706487226121  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2368 training Loss: 510.1787911424926 validation Loss: 119.63734239686522  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2369 training Loss: 510.0287088325463 validation Loss: 119.60405860592608  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2370 training Loss: 509.87873050080674 validation Loss: 119.57079732616725  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2371 training Loss: 509.72885603638366 validation Loss: 119.53755853399454  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2372 training Loss: 509.57908532854503 validation Loss: 119.50434220584691  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2373 training Loss: 509.4294182667167 validation Loss: 119.47114831819647  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2374 training Loss: 509.27985474048194 validation Loss: 119.43797684754833  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2375 training Loss: 509.130394639581 validation Loss: 119.40482777044056  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2376 training Loss: 508.9810378539118 validation Loss: 119.37170106344425  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2377 training Loss: 508.83178427352846 validation Loss: 119.33859670316325  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2378 training Loss: 508.68263378864145 validation Loss: 119.30551466623434  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2379 training Loss: 508.53358628961786 validation Loss: 119.27245492932698  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2380 training Loss: 508.3846416669804 validation Loss: 119.2394174691433  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2381 training Loss: 508.2357998114071 validation Loss: 119.20640226241821  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2382 training Loss: 508.08706061373175 validation Loss: 119.17340928591906  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2383 training Loss: 507.93842396494307 validation Loss: 119.14043851644581  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2384 training Loss: 507.7898897561845 validation Loss: 119.10748993083088  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2385 training Loss: 507.641457878754 validation Loss: 119.07456350593908  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2386 training Loss: 507.49312822410377 validation Loss: 119.04165921866758  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2387 training Loss: 507.34490068383985 validation Loss: 119.00877704594586  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2388 training Loss: 507.19677514972227 validation Loss: 118.97591696473573  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2389 training Loss: 507.04875151366423 validation Loss: 118.94307895203102  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2390 training Loss: 506.90082966773195 validation Loss: 118.91026298485784  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2391 training Loss: 506.753009504145 validation Loss: 118.87746904027429  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2392 training Loss: 506.60529091527496 validation Loss: 118.84469709537058  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2393 training Loss: 506.45767379364634 validation Loss: 118.81194712726882  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2394 training Loss: 506.3101580319352 validation Loss: 118.77921911312299  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2395 training Loss: 506.1627435229698 validation Loss: 118.74651303011903  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2396 training Loss: 506.0154301597297 validation Loss: 118.71382885547465  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2397 training Loss: 505.8682178353458 validation Loss: 118.68116656643926  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2398 training Loss: 505.7211064431 validation Loss: 118.64852614029397  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2399 training Loss: 505.57409587642496 validation Loss: 118.61590755435157  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2400 training Loss: 505.42718602890363 validation Loss: 118.5833107859564  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2401 training Loss: 505.2803767942695 validation Loss: 118.5507358124843  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2402 training Loss: 505.1336680664059 validation Loss: 118.51818261134264  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2403 training Loss: 504.98705973934545 validation Loss: 118.48565115997013  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2404 training Loss: 504.84055170727066 validation Loss: 118.4531414358369  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2405 training Loss: 504.69414386451314 validation Loss: 118.42065341644437  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2406 training Loss: 504.5478361055532 validation Loss: 118.3881870793252  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2407 training Loss: 504.4016283250197 validation Loss: 118.3557424020433  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2408 training Loss: 504.25552041769026 validation Loss: 118.32331936219364  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2409 training Loss: 504.1095122784903 validation Loss: 118.29091793740237  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2410 training Loss: 503.96360380249325 validation Loss: 118.25853810532661  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2411 training Loss: 503.81779488492003 validation Loss: 118.22617984365445  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2412 training Loss: 503.67208542113895 validation Loss: 118.193843130105  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2413 training Loss: 503.52647530666525 validation Loss: 118.16152794242817  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2414 training Loss: 503.3809644371612 validation Loss: 118.1292342584047  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2415 training Loss: 503.23555270843565 validation Loss: 118.09696205584613  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2416 training Loss: 503.09024001644354 validation Loss: 118.0647113125947  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2417 training Loss: 502.9450262572859 validation Loss: 118.03248200652331  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2418 training Loss: 502.7999113272098 validation Loss: 118.00027411553549  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2419 training Loss: 502.6548951226076 validation Loss: 117.96808761756529  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2420 training Loss: 502.5099775400172 validation Loss: 117.9359224905773  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2421 training Loss: 502.3651584761211 validation Loss: 117.90377871256652  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2422 training Loss: 502.2204378277469 validation Loss: 117.8716562615584  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2423 training Loss: 502.07581549186676 validation Loss: 117.83955511560873  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2424 training Loss: 501.93129136559696 validation Loss: 117.80747525280358  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2425 training Loss: 501.78686534619794 validation Loss: 117.77541665125925  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2426 training Loss: 501.64253733107387 validation Loss: 117.74337928912226  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2427 training Loss: 501.49830721777226 validation Loss: 117.71136314456923  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2428 training Loss: 501.35417490398413 validation Loss: 117.67936819580694  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2429 training Loss: 501.2101402875436 validation Loss: 117.6473944210721  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2430 training Loss: 501.0662032664272 validation Loss: 117.61544179863151  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2431 training Loss: 500.92236373875426 validation Loss: 117.58351030678182  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2432 training Loss: 500.7786216027863 validation Loss: 117.55159992384961  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2433 training Loss: 500.6349767569269 validation Loss: 117.51971062819129  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2434 training Loss: 500.4914290997215 validation Loss: 117.48784239819298  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2435 training Loss: 500.34797852985685 validation Loss: 117.45599521227066  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2436 training Loss: 500.20462494616123 validation Loss: 117.42416904886986  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2437 training Loss: 500.06136824760387 validation Loss: 117.39236388646577  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2438 training Loss: 499.91820833329484 validation Loss: 117.36057970356322  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2439 training Loss: 499.77514510248466 validation Loss: 117.32881647869647  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2440 training Loss: 499.6321784545642 validation Loss: 117.2970741904293  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2441 training Loss: 499.48930828906464 validation Loss: 117.26535281735494  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2442 training Loss: 499.3465345056567 validation Loss: 117.23365233809595  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2443 training Loss: 499.2038570041507 validation Loss: 117.20197273130421  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2444 training Loss: 499.0612756844965 validation Loss: 117.17031397566089  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2445 training Loss: 498.9187904467828 validation Loss: 117.13867604987638  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2446 training Loss: 498.77640119123737 validation Loss: 117.10705893269025  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2447 training Loss: 498.63410781822654 validation Loss: 117.07546260287118  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2448 training Loss: 498.491910228255 validation Loss: 117.04388703921693  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2449 training Loss: 498.34980832196567 validation Loss: 117.01233222055426  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2450 training Loss: 498.2078020001392 validation Loss: 116.98079812573894  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2451 training Loss: 498.0658911636941 validation Loss: 116.94928473365563  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2452 training Loss: 497.92407571368614 validation Loss: 116.91779202321791  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2453 training Loss: 497.78235555130834 validation Loss: 116.8863199733681  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2454 training Loss: 497.64073057789074 validation Loss: 116.85486856307745  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2455 training Loss: 497.49920069490014 validation Loss: 116.82343777134577  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2456 training Loss: 497.3577658039396 validation Loss: 116.79202757720162  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2457 training Loss: 497.2164258067486 validation Loss: 116.76063795970222  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2458 training Loss: 497.0751806052026 validation Loss: 116.72926889793334  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2459 training Loss: 496.9340301013128 validation Loss: 116.69792037100922  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2460 training Loss: 496.792974197226 validation Loss: 116.66659235807268  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2461 training Loss: 496.6520127952242 validation Loss: 116.63528483829498  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2462 training Loss: 496.51114579772457 validation Loss: 116.60399779087567  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2463 training Loss: 496.37037310727914 validation Loss: 116.57273119504272  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2464 training Loss: 496.2296946265745 validation Loss: 116.54148503005237  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2465 training Loss: 496.0891102584317 validation Loss: 116.51025927518909  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2466 training Loss: 495.9486199058058 validation Loss: 116.47905390976553  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2467 training Loss: 495.80822347178594 validation Loss: 116.44786891312255  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2468 training Loss: 495.6679208595948 validation Loss: 116.41670426462903  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2469 training Loss: 495.52771197258846 validation Loss: 116.38555994368194  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2470 training Loss: 495.38759671425663 validation Loss: 116.35443592970628  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2471 training Loss: 495.24757498822163 validation Loss: 116.32333220215493  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2472 training Loss: 495.10764669823857 validation Loss: 116.29224874050877  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2473 training Loss: 494.96781174819523 validation Loss: 116.26118552427647  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2474 training Loss: 494.8280700421118 validation Loss: 116.23014253299455  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2475 training Loss: 494.68842148414024 validation Loss: 116.19911974622727  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2476 training Loss: 494.5488659785648 validation Loss: 116.16811714356666  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2477 training Loss: 494.4094034298011 validation Loss: 116.13713470463237  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2478 training Loss: 494.27003374239615 validation Loss: 116.10617240907169  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2479 training Loss: 494.13075682102817 validation Loss: 116.07523023655949  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2480 training Loss: 493.99157257050655 validation Loss: 116.0443081667982  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2481 training Loss: 493.85248089577107 validation Loss: 116.01340617951772  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2482 training Loss: 493.7134817018924 validation Loss: 115.98252425447534  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2483 training Loss: 493.57457489407113 validation Loss: 115.95166237145581  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2484 training Loss: 493.4357603776382 validation Loss: 115.92082051027121  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2485 training Loss: 493.29703805805434 validation Loss: 115.88999865076092  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2486 training Loss: 493.1584078409097 validation Loss: 115.85919677279155  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2487 training Loss: 493.0198696319242 validation Loss: 115.82841485625696  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2488 training Loss: 492.8814233369467 validation Loss: 115.79765288107816  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2489 training Loss: 492.74306886195507 validation Loss: 115.76691082720323  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2490 training Loss: 492.6048061130559 validation Loss: 115.73618867460739  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2491 training Loss: 492.4666349964843 validation Loss: 115.70548640329287  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2492 training Loss: 492.3285554186038 validation Loss: 115.67480399328886  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2493 training Loss: 492.1905672859059 validation Loss: 115.64414142465148  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2494 training Loss: 492.0526705050099 validation Loss: 115.61349867746378  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2495 training Loss: 491.914864982663 validation Loss: 115.5828757318356  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2496 training Loss: 491.77715062573964 validation Loss: 115.55227256790363  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2497 training Loss: 491.6395273412414 validation Loss: 115.52168916583126  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2498 training Loss: 491.5019950362971 validation Loss: 115.49112550580865  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2499 training Loss: 491.3645536181622 validation Loss: 115.46058156805259  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2500 training Loss: 491.22720299421866 validation Loss: 115.43005733280651  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2501 training Loss: 491.08994307197503 validation Loss: 115.39955278034037  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2502 training Loss: 490.9527737590657 validation Loss: 115.36906789095072  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2503 training Loss: 490.8156949632512 validation Loss: 115.33860264496053  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2504 training Loss: 490.6787065924176 validation Loss: 115.3081570227193  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2505 training Loss: 490.54180855457673 validation Loss: 115.27773100460283  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2506 training Loss: 490.40500075786554 validation Loss: 115.24732457101334  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2507 training Loss: 490.2682831105458 validation Loss: 115.21693770237937  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2508 training Loss: 490.13165552100463 validation Loss: 115.18657037915561  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2509 training Loss: 489.9951178977534 validation Loss: 115.15622258182313  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2510 training Loss: 489.85867014942824 validation Loss: 115.12589429088905  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2511 training Loss: 489.7223121847891 validation Loss: 115.0955854868867  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2512 training Loss: 489.58604391272047 validation Loss: 115.06529615037546  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2513 training Loss: 489.4498652422301 validation Loss: 115.0350262619408  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2514 training Loss: 489.31377608244975 validation Loss: 115.0047758021941  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2515 training Loss: 489.1777763426343 validation Loss: 114.97454475177281  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2516 training Loss: 489.04186593216195 validation Loss: 114.94433309134021  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2517 training Loss: 488.90604476053375 validation Loss: 114.91414080158555  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2518 training Loss: 488.77031273737384 validation Loss: 114.88396786322375  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2519 training Loss: 488.6346697724284 validation Loss: 114.85381425699569  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2520 training Loss: 488.49911577556645 validation Loss: 114.82367996366784  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2521 training Loss: 488.3636506567787 validation Loss: 114.7935649640325  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2522 training Loss: 488.22827432617805 validation Loss: 114.76346923890753  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2523 training Loss: 488.09298669399925 validation Loss: 114.73339276913649  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2524 training Loss: 487.9577876705982 validation Loss: 114.70333553558842  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2525 training Loss: 487.8226771664524 validation Loss: 114.67329751915796  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2526 training Loss: 487.6876550921605 validation Loss: 114.64327870076522  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2527 training Loss: 487.5527213584419 validation Loss: 114.6132790613557  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2528 training Loss: 487.4178758761367 validation Loss: 114.58329858190038  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2529 training Loss: 487.2831185562057 validation Loss: 114.55333724339556  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2530 training Loss: 487.14844930972976 validation Loss: 114.52339502686284  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2531 training Loss: 487.0138680479098 validation Loss: 114.49347191334915  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2532 training Loss: 486.87937468206707 validation Loss: 114.46356788392657  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2533 training Loss: 486.7449691236419 validation Loss: 114.43368291969247  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2534 training Loss: 486.6106512841946 validation Loss: 114.40381700176928  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2535 training Loss: 486.4764210754045 validation Loss: 114.37397011130463  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2536 training Loss: 486.3422784090703 validation Loss: 114.34414222947109  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2537 training Loss: 486.2082231971092 validation Loss: 114.3143333374664  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2538 training Loss: 486.0742553515574 validation Loss: 114.28454341651315  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2539 training Loss: 485.94037478456937 validation Loss: 114.25477244785897  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2540 training Loss: 485.80658140841814 validation Loss: 114.22502041277632  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2541 training Loss: 485.67287513549456 validation Loss: 114.19528729256255  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2542 training Loss: 485.53925587830764 validation Loss: 114.16557306853986  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2543 training Loss: 485.40572354948375 validation Loss: 114.13587772205514  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2544 training Loss: 485.2722780617671 validation Loss: 114.1062012344801  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2545 training Loss: 485.1389193280189 validation Loss: 114.07654358721112  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2546 training Loss: 485.00564726121786 validation Loss: 114.04690476166923  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2547 training Loss: 484.8724617744591 validation Loss: 114.0172847393  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2548 training Loss: 484.73936278095493 validation Loss: 113.9876835015737  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2549 training Loss: 484.6063501940338 validation Loss: 113.95810102998504  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2550 training Loss: 484.47342392714074 validation Loss: 113.9285373060533  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2551 training Loss: 484.340583893837 validation Loss: 113.89899231132212  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2552 training Loss: 484.2078300077992 validation Loss: 113.86946602735959  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2553 training Loss: 484.07516218282035 validation Loss: 113.8399584357582  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2554 training Loss: 483.9425803328088 validation Loss: 113.81046951813468  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2555 training Loss: 483.8100843717881 validation Loss: 113.78099925613017  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2556 training Loss: 483.677674213897 validation Loss: 113.75154763140998  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2557 training Loss: 483.5453497733893 validation Loss: 113.72211462566364  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2558 training Loss: 483.4131109646338 validation Loss: 113.69270022060486  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2559 training Loss: 483.2809577021135 validation Loss: 113.66330439797147  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2560 training Loss: 483.14888990042584 validation Loss: 113.63392713952541  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2561 training Loss: 483.01690747428273 validation Loss: 113.60456842705264  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2562 training Loss: 482.8850103385099 validation Loss: 113.57522824236315  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2563 training Loss: 482.75319840804707 validation Loss: 113.5459065672909  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2564 training Loss: 482.62147159794733 validation Loss: 113.51660338369379  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2565 training Loss: 482.48982982337736 validation Loss: 113.48731867345359  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2566 training Loss: 482.3582729996172 validation Loss: 113.45805241847592  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2567 training Loss: 482.22680104205983 validation Loss: 113.42880460069026  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2568 training Loss: 482.0954138662111 validation Loss: 113.39957520204976  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2569 training Loss: 481.9641113876896 validation Loss: 113.37036420453148  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2570 training Loss: 481.8328935222265 validation Loss: 113.34117159013599  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2571 training Loss: 481.70176018566525 validation Loss: 113.31199734088764  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2572 training Loss: 481.5707112939612 validation Loss: 113.28284143883435  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2573 training Loss: 481.43974676318203 validation Loss: 113.25370386604763  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2574 training Loss: 481.30886650950697 validation Loss: 113.22458460462254  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2575 training Loss: 481.17807044922677 validation Loss: 113.19548363667761  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2576 training Loss: 481.04735849874373 validation Loss: 113.16640094435488  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2577 training Loss: 480.9167305745712 validation Loss: 113.13733650981976  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2578 training Loss: 480.7861865933337 validation Loss: 113.10829031526112  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2579 training Loss: 480.6557264717665 validation Loss: 113.07926234289113  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2580 training Loss: 480.5253501267156 validation Loss: 113.05025257494526  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2581 training Loss: 480.39505747513715 validation Loss: 113.02126099368229  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2582 training Loss: 480.26484843409816 validation Loss: 112.99228758138422  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2583 training Loss: 480.1347229207753 validation Loss: 112.96333232035622  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2584 training Loss: 480.00468085245524 validation Loss: 112.93439519292667  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2585 training Loss: 479.8747221465345 validation Loss: 112.90547618144704  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2586 training Loss: 479.74484672051926 validation Loss: 112.8765752682919  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2587 training Loss: 479.6150544920247 validation Loss: 112.84769243585879  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2588 training Loss: 479.4853453787755 validation Loss: 112.8188276665684  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2589 training Loss: 479.3557192986052 validation Loss: 112.78998094286425  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2590 training Loss: 479.2261761694564 validation Loss: 112.76115224721289  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2591 training Loss: 479.09671590937995 validation Loss: 112.73234156210371  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2592 training Loss: 478.9673384365358 validation Loss: 112.70354887004899  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2593 training Loss: 478.83804366919156 validation Loss: 112.67477415358378  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2594 training Loss: 478.7088315257234 validation Loss: 112.646017395266  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2595 training Loss: 478.57970192461516 validation Loss: 112.61727857767625  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2596 training Loss: 478.45065478445866 validation Loss: 112.58855768341786  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2597 training Loss: 478.3216900239531 validation Loss: 112.5598546951168  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2598 training Loss: 478.1928075619053 validation Loss: 112.53116959542172  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2599 training Loss: 478.0640073172289 validation Loss: 112.50250236700384  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2600 training Loss: 477.9352892089453 validation Loss: 112.473852992557  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2601 training Loss: 477.80665315618205 validation Loss: 112.44522145479746  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2602 training Loss: 477.6780990781739 validation Loss: 112.41660773646406  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2603 training Loss: 477.5496268942619 validation Loss: 112.38801182031806  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2604 training Loss: 477.4212365238935 validation Loss: 112.35943368914315  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2605 training Loss: 477.29292788662246 validation Loss: 112.33087332574536  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2606 training Loss: 477.1647009021083 validation Loss: 112.30233071295308  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2607 training Loss: 477.0365554901165 validation Loss: 112.27380583361705  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2608 training Loss: 476.9084915705181 validation Loss: 112.2452986706102  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2609 training Loss: 476.78050906328986 validation Loss: 112.21680920682778  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2610 training Loss: 476.65260788851367 validation Loss: 112.18833742518717  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2611 training Loss: 476.52478796637655 validation Loss: 112.15988330862797  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2612 training Loss: 476.39704921717055 validation Loss: 112.13144684011185  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2613 training Loss: 476.2693915612924 validation Loss: 112.10302800262258  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2614 training Loss: 476.14181491924353 validation Loss: 112.07462677916601  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2615 training Loss: 476.01431921162987 validation Loss: 112.04624315277006  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2616 training Loss: 475.8869043591617 validation Loss: 112.01787710648449  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2617 training Loss: 475.75957028265293 validation Loss: 111.98952862338112  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2618 training Loss: 475.63231690302206 validation Loss: 111.96119768655367  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2619 training Loss: 475.5051441412909 validation Loss: 111.93288427911769  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2620 training Loss: 475.378051918585 validation Loss: 111.90458838421064  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2621 training Loss: 475.2510401561335 validation Loss: 111.87630998499172  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2622 training Loss: 475.12410877526855 validation Loss: 111.84804906464194  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2623 training Loss: 474.9972576974255 validation Loss: 111.81980560636401  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2624 training Loss: 474.8704868441424 validation Loss: 111.7915795933824  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2625 training Loss: 474.7437961370606 validation Loss: 111.76337100894318  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2626 training Loss: 474.6171854979234 validation Loss: 111.73517983631413  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2627 training Loss: 474.490654848577 validation Loss: 111.7070060587845  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2628 training Loss: 474.3642041109696 validation Loss: 111.67884965966526  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2629 training Loss: 474.2378332071514 validation Loss: 111.65071062228878  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2630 training Loss: 474.1115420592749 validation Loss: 111.62258893000899  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2631 training Loss: 473.9853305895939 validation Loss: 111.5944845662012  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2632 training Loss: 473.85919872046406 validation Loss: 111.56639751426226  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2633 training Loss: 473.7331463743425 validation Loss: 111.53832775761032  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2634 training Loss: 473.60717347378716 validation Loss: 111.5102752796849  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2635 training Loss: 473.48127994145773 validation Loss: 111.48224006394685  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2636 training Loss: 473.35546570011434 validation Loss: 111.45422209387831  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2637 training Loss: 473.22973067261813 validation Loss: 111.42622135298265  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2638 training Loss: 473.10407478193054 validation Loss: 111.39823782478447  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2639 training Loss: 472.9784979511138 validation Loss: 111.37027149282957  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2640 training Loss: 472.8530001033302 validation Loss: 111.34232234068486  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2641 training Loss: 472.7275811618421 validation Loss: 111.31439035193841  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2642 training Loss: 472.60224105001214 validation Loss: 111.28647551019932  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2643 training Loss: 472.47697969130223 validation Loss: 111.25857779909776  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2644 training Loss: 472.3517970092743 validation Loss: 111.23069720228493  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2645 training Loss: 472.2266929275895 validation Loss: 111.20283370343297  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2646 training Loss: 472.10166737000844 validation Loss: 111.17498728623504  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2647 training Loss: 471.9767202603907 validation Loss: 111.14715793440509  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2648 training Loss: 471.85185152269503 validation Loss: 111.11934563167806  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2649 training Loss: 471.7270610809788 validation Loss: 111.0915503618097  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2650 training Loss: 471.60234885939826 validation Loss: 111.06377210857656  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2651 training Loss: 471.47771478220784 validation Loss: 111.03601085577597  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2652 training Loss: 471.3531587737608 validation Loss: 111.00826658722602  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2653 training Loss: 471.2286807585079 validation Loss: 110.98053928676549  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2654 training Loss: 471.1042806609985 validation Loss: 110.95282893825387  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2655 training Loss: 470.9799584058796 validation Loss: 110.92513552557128  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2656 training Loss: 470.85571391789586 validation Loss: 110.89745903261843  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2657 training Loss: 470.73154712188955 validation Loss: 110.86979944331662  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2658 training Loss: 470.6074579428001 validation Loss: 110.84215674160775  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2659 training Loss: 470.48344630566464 validation Loss: 110.81453091145416  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2660 training Loss: 470.3595121356169 validation Loss: 110.7869219368387  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2661 training Loss: 470.2356553578878 validation Loss: 110.7593298017647  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2662 training Loss: 470.11187589780485 validation Loss: 110.73175449025582  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2663 training Loss: 469.9881736807924 validation Loss: 110.70419598635621  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2664 training Loss: 469.86454863237077 validation Loss: 110.67665427413036  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2665 training Loss: 469.7410006781573 validation Loss: 110.64912933766294  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2666 training Loss: 469.6175297438649 validation Loss: 110.6216211610591  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2667 training Loss: 469.4941357553023 validation Loss: 110.59412972844409  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2668 training Loss: 469.3708186383748 validation Loss: 110.56665502396348  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2669 training Loss: 469.24757831908266 validation Loss: 110.53919703178298  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2670 training Loss: 469.12441472352197 validation Loss: 110.51175573608847  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2671 training Loss: 469.0013277778841 validation Loss: 110.48433112108594  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2672 training Loss: 468.87831740845576 validation Loss: 110.4569231710015  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2673 training Loss: 468.7553835416186 validation Loss: 110.42953187008135  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2674 training Loss: 468.6325261038491 validation Loss: 110.40215720259161  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2675 training Loss: 468.50974502171863 validation Loss: 110.37479915281853  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2676 training Loss: 468.387040221893 validation Loss: 110.3474577050682  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2677 training Loss: 468.2644116311328 validation Loss: 110.32013284366676  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2678 training Loss: 468.1418591762925 validation Loss: 110.29282455296018  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2679 training Loss: 468.019382784321 validation Loss: 110.26553281731434  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2680 training Loss: 467.896982382261 validation Loss: 110.23825762111493  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2681 training Loss: 467.7746578972492 validation Loss: 110.21099894876744  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2682 training Loss: 467.6524092565158 validation Loss: 110.18375678469718  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2683 training Loss: 467.53023638738483 validation Loss: 110.15653111334917  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2684 training Loss: 467.4081392172734 validation Loss: 110.12932191918819  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2685 training Loss: 467.286117673692 validation Loss: 110.1021291866986  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2686 training Loss: 467.16417168424425 validation Loss: 110.07495290038452  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2687 training Loss: 467.04230117662667 validation Loss: 110.04779304476965  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2688 training Loss: 466.92050607862836 validation Loss: 110.02064960439728  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2689 training Loss: 466.79878631813153 validation Loss: 109.99352256383025  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2690 training Loss: 466.67714182311056 validation Loss: 109.96641190765092  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2691 training Loss: 466.55557252163214 validation Loss: 109.9393176204612  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2692 training Loss: 466.43407834185535 validation Loss: 109.91223968688239  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2693 training Loss: 466.3126592120312 validation Loss: 109.8851780915553  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2694 training Loss: 466.19131506050263 validation Loss: 109.85813281914011  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2695 training Loss: 466.0700458157045 validation Loss: 109.83110385431632  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2696 training Loss: 465.9488514061628 validation Loss: 109.80409118178284  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2697 training Loss: 465.82773176049545 validation Loss: 109.77709478625793  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2698 training Loss: 465.7066868074117 validation Loss: 109.75011465247904  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2699 training Loss: 465.58571647571165 validation Loss: 109.72315076520292  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2700 training Loss: 465.4648206942867 validation Loss: 109.69620310920547  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2701 training Loss: 465.3439993921188 validation Loss: 109.66927166928194  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2702 training Loss: 465.22325249828117 validation Loss: 109.6423564302466  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2703 training Loss: 465.10257994193717 validation Loss: 109.61545737693288  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2704 training Loss: 464.9819816523408 validation Loss: 109.58857449419338  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2705 training Loss: 464.8614575588364 validation Loss: 109.56170776689966  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2706 training Loss: 464.74100759085815 validation Loss: 109.5348571799424  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2707 training Loss: 464.62063167793065 validation Loss: 109.50802271823127  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2708 training Loss: 464.50032974966825 validation Loss: 109.48120436669495  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2709 training Loss: 464.38010173577504 validation Loss: 109.45440211028097  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2710 training Loss: 464.2599475660446 validation Loss: 109.42761593395588  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2711 training Loss: 464.1398671703599 validation Loss: 109.4008458227051  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2712 training Loss: 464.0198604786933 validation Loss: 109.3740917615329  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2713 training Loss: 463.89992742110655 validation Loss: 109.34735373546239  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2714 training Loss: 463.7800679277502 validation Loss: 109.32063172953546  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2715 training Loss: 463.6602819288635 validation Loss: 109.29392572881282  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2716 training Loss: 463.5405693547748 validation Loss: 109.26723571837388  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2717 training Loss: 463.42093013590073 validation Loss: 109.24056168331676  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2718 training Loss: 463.30136420274664 validation Loss: 109.2139036087583  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2719 training Loss: 463.181871485906 validation Loss: 109.18726147983398  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2720 training Loss: 463.0624519160607 validation Loss: 109.1606352816979  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2721 training Loss: 462.9431054239802 validation Loss: 109.13402499952278  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2722 training Loss: 462.82383194052255 validation Loss: 109.10743061849986  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2723 training Loss: 462.70463139663286 validation Loss: 109.08085212383897  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2724 training Loss: 462.58550372334423 validation Loss: 109.05428950076842  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2725 training Loss: 462.46644885177716 validation Loss: 109.027742734535  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2726 training Loss: 462.3474667131395 validation Loss: 109.00121181040396  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2727 training Loss: 462.22855723872635 validation Loss: 108.97469671365901  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2728 training Loss: 462.10972035991995 validation Loss: 108.9481974296022  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2729 training Loss: 461.9909560081892 validation Loss: 108.92171394355397  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2730 training Loss: 461.8722641150898 validation Loss: 108.89524624085306  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2731 training Loss: 461.7536446122644 validation Loss: 108.86879430685657  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2732 training Loss: 461.635097431442 validation Loss: 108.84235812693987  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2733 training Loss: 461.51662250443803 validation Loss: 108.81593768649657  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2734 training Loss: 461.398219763154 validation Loss: 108.7895329709385  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2735 training Loss: 461.2798891395777 validation Loss: 108.76314396569568  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2736 training Loss: 461.16163056578273 validation Loss: 108.7367706562163  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2737 training Loss: 461.04344397392873 validation Loss: 108.7104130279667  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2738 training Loss: 460.9253292962609 validation Loss: 108.68407106643131  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2739 training Loss: 460.8072864651101 validation Loss: 108.65774475711267  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2740 training Loss: 460.6893154128925 validation Loss: 108.63143408553137  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2741 training Loss: 460.5714160721096 validation Loss: 108.60513903722597  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2742 training Loss: 460.4535883753482 validation Loss: 108.57885959775308  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2743 training Loss: 460.3358322552799 validation Loss: 108.5525957526873  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2744 training Loss: 460.2181476446613 validation Loss: 108.52634748762105  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2745 training Loss: 460.1005344763338 validation Loss: 108.50011478816484  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2746 training Loss: 459.98299268322336 validation Loss: 108.47389763994693  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2747 training Loss: 459.8655221983405 validation Loss: 108.4476960286135  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2748 training Loss: 459.7481229547801 validation Loss: 108.42150993982852  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2749 training Loss: 459.6307948857211 validation Loss: 108.3953393592738  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2750 training Loss: 459.5135379244267 validation Loss: 108.36918427264891  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2751 training Loss: 459.39635200424414 validation Loss: 108.34304466567116  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2752 training Loss: 459.2792370586043 validation Loss: 108.31692052407556  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2753 training Loss: 459.1621930210217 validation Loss: 108.29081183361487  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2754 training Loss: 459.0452198250948 validation Loss: 108.26471858005942  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2755 training Loss: 458.92831740450504 validation Loss: 108.23864074919733  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2756 training Loss: 458.8114856930176 validation Loss: 108.21257832683419  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2757 training Loss: 458.6947246244805 validation Loss: 108.18653129879318  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2758 training Loss: 458.5780341328249 validation Loss: 108.16049965091511  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2759 training Loss: 458.46141415206495 validation Loss: 108.1344833690583  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2760 training Loss: 458.3448646162975 validation Loss: 108.10848243909851  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2761 training Loss: 458.2283854597021 validation Loss: 108.08249684692902  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2762 training Loss: 458.1119766165408 validation Loss: 108.05652657846058  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2763 training Loss: 457.99563802115824 validation Loss: 108.0305716196213  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2764 training Loss: 457.87936960798083 validation Loss: 108.00463195635672  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2765 training Loss: 457.7631713115177 validation Loss: 107.97870757462977  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2766 training Loss: 457.64704306635963 validation Loss: 107.95279846042064  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2767 training Loss: 457.5309848071794 validation Loss: 107.92690459972691  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2768 training Loss: 457.4149964687315 validation Loss: 107.90102597856341  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2769 training Loss: 457.29907798585214 validation Loss: 107.87516258296225  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2770 training Loss: 457.18322929345896 validation Loss: 107.84931439897271  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2771 training Loss: 457.06745032655084 validation Loss: 107.82348141266135  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2772 training Loss: 456.95174102020826 validation Loss: 107.7976636101119  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2773 training Loss: 456.8361013095923 validation Loss: 107.7718609774252  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2774 training Loss: 456.7205311299456 validation Loss: 107.74607350071923  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2775 training Loss: 456.6050304165914 validation Loss: 107.72030116612913  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2776 training Loss: 456.4895991049335 validation Loss: 107.69454395980699  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2777 training Loss: 456.37423713045666 validation Loss: 107.66880186792208  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2778 training Loss: 456.2589444287257 validation Loss: 107.64307487666059  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2779 training Loss: 456.14372093538645 validation Loss: 107.61736297222578  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2780 training Loss: 456.0285665861643 validation Loss: 107.59166614083782  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2781 training Loss: 455.9134813168651 validation Loss: 107.56598436873384  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2782 training Loss: 455.79846506337464 validation Loss: 107.54031764216789  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2783 training Loss: 455.6835177616586 validation Loss: 107.51466594741095  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2784 training Loss: 455.5686393477624 validation Loss: 107.4890292707508  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2785 training Loss: 455.4538297578109 validation Loss: 107.46340759849207  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2786 training Loss: 455.3390889280087 validation Loss: 107.43780091695623  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2787 training Loss: 455.22441679463964 validation Loss: 107.41220921248156  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2788 training Loss: 455.1098132940667 validation Loss: 107.38663247142301  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2789 training Loss: 454.9952783627324 validation Loss: 107.36107068015235  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2790 training Loss: 454.88081193715766 validation Loss: 107.335523825058  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2791 training Loss: 454.7664139539427 validation Loss: 107.30999189254514  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2792 training Loss: 454.65208434976637 validation Loss: 107.28447486903553  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2793 training Loss: 454.53782306138623 validation Loss: 107.25897274096764  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2794 training Loss: 454.42363002563826 validation Loss: 107.23348549479645  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2795 training Loss: 454.30950517943677 validation Loss: 107.20801311699358  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2796 training Loss: 454.1954484597743 validation Loss: 107.18255559404722  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2797 training Loss: 454.08145980372194 validation Loss: 107.15711291246208  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2798 training Loss: 453.9675391484284 validation Loss: 107.13168505875933  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2799 training Loss: 453.8536864311202 validation Loss: 107.1062720194767  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2800 training Loss: 453.73990158910215 validation Loss: 107.08087378116834  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2801 training Loss: 453.6261845597563 validation Loss: 107.05549033040478  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2802 training Loss: 453.5125352805423 validation Loss: 107.03012165377305  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2803 training Loss: 453.39895368899744 validation Loss: 107.00476773787646  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2804 training Loss: 453.285439722736 validation Loss: 106.97942856933474  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2805 training Loss: 453.1719933194497 validation Loss: 106.95410413478399  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2806 training Loss: 453.05861441690723 validation Loss: 106.92879442087649  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2807 training Loss: 452.94530295295425 validation Loss: 106.90349941428089  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2808 training Loss: 452.8320588655132 validation Loss: 106.87821910168209  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2809 training Loss: 452.7188820925831 validation Loss: 106.85295346978121  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2810 training Loss: 452.60577257223997 validation Loss: 106.82770250529558  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2811 training Loss: 452.49273024263596 validation Loss: 106.80246619495865  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2812 training Loss: 452.3797550419996 validation Loss: 106.77724452552015  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2813 training Loss: 452.26684690863567 validation Loss: 106.75203748374581  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2814 training Loss: 452.1540057809252 validation Loss: 106.72684505641759  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2815 training Loss: 452.0412315973251 validation Loss: 106.70166723033344  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2816 training Loss: 451.9285242963683 validation Loss: 106.6765039923074  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2817 training Loss: 451.81588381666324 validation Loss: 106.65135532916956  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2818 training Loss: 451.70331009689426 validation Loss: 106.626221227766  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2819 training Loss: 451.5908030758211 validation Loss: 106.60110167495877  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2820 training Loss: 451.478362692279 validation Loss: 106.57599665762595  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2821 training Loss: 451.36598888517847 validation Loss: 106.55090616266145  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2822 training Loss: 451.2536815935051 validation Loss: 106.5258301769752  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2823 training Loss: 451.1414407563199 validation Loss: 106.50076868749295  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2824 training Loss: 451.02926631275835 validation Loss: 106.47572168115632  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2825 training Loss: 450.91715820203103 validation Loss: 106.45068914492279  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2826 training Loss: 450.8051163634232 validation Loss: 106.42567106576567  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2827 training Loss: 450.6931407362949 validation Loss: 106.400667430674  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2828 training Loss: 450.58123126008036 validation Loss: 106.37567822665267  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2829 training Loss: 450.46938787428843 validation Loss: 106.35070344072226  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2830 training Loss: 450.35761051850216 validation Loss: 106.32574305991906  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2831 training Loss: 450.2458991323788 validation Loss: 106.30079707129512  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2832 training Loss: 450.13425365564945 validation Loss: 106.27586546191812  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2833 training Loss: 450.02267402811935 validation Loss: 106.25094821887134  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2834 training Loss: 449.91116018966744 validation Loss: 106.2260453292538  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2835 training Loss: 449.7997120802464 validation Loss: 106.20115678018001  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2836 training Loss: 449.68832963988257 validation Loss: 106.17628255878017  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2837 training Loss: 449.5770128086756 validation Loss: 106.15142265219991  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2838 training Loss: 449.4657615267987 validation Loss: 106.12657704760048  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2839 training Loss: 449.3545757344981 validation Loss: 106.1017457321586  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2840 training Loss: 449.2434553720933 validation Loss: 106.07692869306649  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2841 training Loss: 449.1324003799769 validation Loss: 106.05212591753181  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2842 training Loss: 449.0214106986142 validation Loss: 106.0273373927777  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2843 training Loss: 448.91048626854354 validation Loss: 106.00256310604266  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2844 training Loss: 448.7996270303758 validation Loss: 105.97780304458064  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2845 training Loss: 448.68883292479467 validation Loss: 105.95305719566089  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2846 training Loss: 448.5781038925558 validation Loss: 105.92832554656805  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2847 training Loss: 448.4674398744878 validation Loss: 105.90360808460207  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2848 training Loss: 448.3568408114912 validation Loss: 105.87890479707822  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2849 training Loss: 448.24630664453866 validation Loss: 105.854215671327  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2850 training Loss: 448.13583731467503 validation Loss: 105.82954069469417  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2851 training Loss: 448.02543276301697 validation Loss: 105.80487985454077  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2852 training Loss: 447.91509293075296 validation Loss: 105.780233138243  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2853 training Loss: 447.8048177591432 validation Loss: 105.75560053319225  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2854 training Loss: 447.69460718951945 validation Loss: 105.73098202679506  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2855 training Loss: 447.5844611632851 validation Loss: 105.70637760647315  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2856 training Loss: 447.4743796219146 validation Loss: 105.68178725966332  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2857 training Loss: 447.36436250695397 validation Loss: 105.65721097381746  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2858 training Loss: 447.2544097600202 validation Loss: 105.63264873640252  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2859 training Loss: 447.1445213228015 validation Loss: 105.60810053490053  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2860 training Loss: 447.0346971370569 validation Loss: 105.58356635680853  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2861 training Loss: 446.9249371446162 validation Loss: 105.5590461896386  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2862 training Loss: 446.81524128738016 validation Loss: 105.53454002091769  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2863 training Loss: 446.7056095073199 validation Loss: 105.51004783818783  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2864 training Loss: 446.5960417464773 validation Loss: 105.48556962900591  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2865 training Loss: 446.48653794696435 validation Loss: 105.46110538094375  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2866 training Loss: 446.3770980509635 validation Loss: 105.43665508158807  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2867 training Loss: 446.26772200072753 validation Loss: 105.41221871854044  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2868 training Loss: 446.15840973857894 validation Loss: 105.38779627941729  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2869 training Loss: 446.0491612069107 validation Loss: 105.36338775184988  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2870 training Loss: 445.93997634818516 validation Loss: 105.33899312348422  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2871 training Loss: 445.8308551049347 validation Loss: 105.31461238198116  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2872 training Loss: 445.7217974197613 validation Loss: 105.29024551501628  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2873 training Loss: 445.61280323533657 validation Loss: 105.2658925102799  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2874 training Loss: 445.50387249440155 validation Loss: 105.24155335547704  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2875 training Loss: 445.39500513976634 validation Loss: 105.2172280383274  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2876 training Loss: 445.28620111431087 validation Loss: 105.1929165465654  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2877 training Loss: 445.1774603609836 validation Loss: 105.16861886794004  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2878 training Loss: 445.0687828228024 validation Loss: 105.14433499021497  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2879 training Loss: 444.9601684428537 validation Loss: 105.12006490116846  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2880 training Loss: 444.8516171642933 validation Loss: 105.09580858859337  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2881 training Loss: 444.74312893034534 validation Loss: 105.07156604029707  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2882 training Loss: 444.63470368430256 validation Loss: 105.04733724410153  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2883 training Loss: 444.5263413695264 validation Loss: 105.02312218784313  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2884 training Loss: 444.4180419294464 validation Loss: 104.99892085937287  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2885 training Loss: 444.30980530756085 validation Loss: 104.97473324655614  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2886 training Loss: 444.20163144743583 validation Loss: 104.95055933727281  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2887 training Loss: 444.0935202927057 validation Loss: 104.9263991194172  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2888 training Loss: 443.9854717870729 validation Loss: 104.90225258089797  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2889 training Loss: 443.87748587430764 validation Loss: 104.8781197096382  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2890 training Loss: 443.7695624982478 validation Loss: 104.85400049357538  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2891 training Loss: 443.66170160279916 validation Loss: 104.8298949206613  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2892 training Loss: 443.55390313193504 validation Loss: 104.80580297886206  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2893 training Loss: 443.44616702969637 validation Loss: 104.7817246561581  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2894 training Loss: 443.3384932401911 validation Loss: 104.7576599405441  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2895 training Loss: 443.2308817075947 validation Loss: 104.733608820029  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2896 training Loss: 443.12333237614985 validation Loss: 104.70957128263603  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2897 training Loss: 443.01584519016626 validation Loss: 104.68554731640259  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2898 training Loss: 442.9084200940206 validation Loss: 104.66153690938023  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2899 training Loss: 442.80105703215645 validation Loss: 104.6375400496348  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2900 training Loss: 442.69375594908405 validation Loss: 104.61355672524618  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2901 training Loss: 442.58651678938065 validation Loss: 104.58958692430843  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2902 training Loss: 442.4793394976895 validation Loss: 104.56563063492973  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2903 training Loss: 442.37222401872106 validation Loss: 104.54168784523239  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2904 training Loss: 442.2651702972515 validation Loss: 104.51775854335263  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2905 training Loss: 442.1581782781236 validation Loss: 104.4938427174409  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2906 training Loss: 442.0512479062462 validation Loss: 104.4699403556616  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2907 training Loss: 441.9443791265945 validation Loss: 104.44605144619311  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2908 training Loss: 441.83757188420935 validation Loss: 104.42217597722785  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2909 training Loss: 441.7308261241976 validation Loss: 104.39831393697219  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2910 training Loss: 441.62414179173186 validation Loss: 104.37446531364637  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2911 training Loss: 441.51751883205037 validation Loss: 104.35063009548469  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2912 training Loss: 441.41095719045717 validation Loss: 104.32680827073524  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2913 training Loss: 441.3044568123217 validation Loss: 104.30299982766005  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2914 training Loss: 441.1980176430784 validation Loss: 104.27920475453497  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2915 training Loss: 441.09163962822777 validation Loss: 104.25542303964974  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2916 training Loss: 440.98532271333477 validation Loss: 104.2316546713079  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2917 training Loss: 440.8790668440299 validation Loss: 104.20789963782676  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2918 training Loss: 440.77287196600844 validation Loss: 104.18415792753741  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2919 training Loss: 440.6667380250308 validation Loss: 104.1604295287848  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2920 training Loss: 440.5606649669219 validation Loss: 104.13671442992751  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2921 training Loss: 440.45465273757145 validation Loss: 104.11301261933787  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2922 training Loss: 440.34870128293414 validation Loss: 104.0893240854019  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2923 training Loss: 440.2428105490286 validation Loss: 104.06564881651934  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2924 training Loss: 440.1369804819383 validation Loss: 104.04198680110355  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2925 training Loss: 440.0312110278108 validation Loss: 104.01833802758152  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2926 training Loss: 439.925502132858 validation Loss: 103.9947024843939  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2927 training Loss: 439.819853743356 validation Loss: 103.9710801599949  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2928 training Loss: 439.7142658056447 validation Loss: 103.94747104285233  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2929 training Loss: 439.6087382661281 validation Loss: 103.92387512144752  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2930 training Loss: 439.5032710712741 validation Loss: 103.9002923842754  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2931 training Loss: 439.3978641676143 validation Loss: 103.87672281984442  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2932 training Loss: 439.2925175017439 validation Loss: 103.85316641667643  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2933 training Loss: 439.1872310203217 validation Loss: 103.82962316330688  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2934 training Loss: 439.0820046700699 validation Loss: 103.80609304828458  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2935 training Loss: 438.9768383977744 validation Loss: 103.78257606017185  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2936 training Loss: 438.87173215028383 validation Loss: 103.75907218754439  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2937 training Loss: 438.7666858745105 validation Loss: 103.73558141899133  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2938 training Loss: 438.66169951742944 validation Loss: 103.71210374311514  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2939 training Loss: 438.5567730260789 validation Loss: 103.68863914853162  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2940 training Loss: 438.45190634755994 validation Loss: 103.66518762387005  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2941 training Loss: 438.3470994290365 validation Loss: 103.64174915777294  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2942 training Loss: 438.24235221773523 validation Loss: 103.61832373889604  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2943 training Loss: 438.13766466094535 validation Loss: 103.59491135590848  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2944 training Loss: 438.0330367060186 validation Loss: 103.57151199749262  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2945 training Loss: 437.9284683003692 validation Loss: 103.54812565234404  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2946 training Loss: 437.82395939147375 validation Loss: 103.52475230917159  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2947 training Loss: 437.71950992687096 validation Loss: 103.50139195669729  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2948 training Loss: 437.61511985416183 validation Loss: 103.47804458365636  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2949 training Loss: 437.51078912100945 validation Loss: 103.45471017879717  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2950 training Loss: 437.40651767513873 validation Loss: 103.43138873088124  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2951 training Loss: 437.3023054643367 validation Loss: 103.40808022868326  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2952 training Loss: 437.1981524364521 validation Loss: 103.38478466099096  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2953 training Loss: 437.0940585393952 validation Loss: 103.3615020166052  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2954 training Loss: 436.9900237211381 validation Loss: 103.33823228433988  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2955 training Loss: 436.88604792971404 validation Loss: 103.314975453022  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2956 training Loss: 436.7821311132183 validation Loss: 103.29173151149155  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2957 training Loss: 436.678273219807 validation Loss: 103.26850044860154  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2958 training Loss: 436.5744741976979 validation Loss: 103.245282253218  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2959 training Loss: 436.47073399516944 validation Loss: 103.22207691421988  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2960 training Loss: 436.36705256056155 validation Loss: 103.19888442049913  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2961 training Loss: 436.2634298422748 validation Loss: 103.17570476096063  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2962 training Loss: 436.15986578877096 validation Loss: 103.15253792452216  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2963 training Loss: 436.05636034857247 validation Loss: 103.12938390011445  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2964 training Loss: 435.95291347026244 validation Loss: 103.10624267668103  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2965 training Loss: 435.84952510248445 validation Loss: 103.08311424317834  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2966 training Loss: 435.7461951939431 validation Loss: 103.0599985885757  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2967 training Loss: 435.64292369340285 validation Loss: 103.03689570185516  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2968 training Loss: 435.5397105496887 validation Loss: 103.01380557201162  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2969 training Loss: 435.4365557116863 validation Loss: 102.99072818805283  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2970 training Loss: 435.3334591283409 validation Loss: 102.96766353899919  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2971 training Loss: 435.2304207486582 validation Loss: 102.94461161388395  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2972 training Loss: 435.12744052170376 validation Loss: 102.92157240175302  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2973 training Loss: 435.024518396603 validation Loss: 102.89854589166507  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2974 training Loss: 434.9216543225414 validation Loss: 102.87553207269147  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2975 training Loss: 434.8188482487637 validation Loss: 102.8525309339162  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2976 training Loss: 434.71610012457484 validation Loss: 102.82954246443595  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2977 training Loss: 434.61340989933893 validation Loss: 102.80656665336005  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2978 training Loss: 434.5107775224798 validation Loss: 102.78360348981045  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2979 training Loss: 434.4082029434805 validation Loss: 102.76065296292168  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2980 training Loss: 434.30568611188335 validation Loss: 102.73771506184085  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2981 training Loss: 434.20322697728994 validation Loss: 102.71478977572768  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2982 training Loss: 434.100825489361 validation Loss: 102.69187709375441  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2983 training Loss: 433.9984815978164 validation Loss: 102.66897700510576  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2984 training Loss: 433.89619525243467 validation Loss: 102.64608949897905  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2985 training Loss: 433.79396640305356 validation Loss: 102.62321456458405  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2986 training Loss: 433.6917949995691 validation Loss: 102.600352191143  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2987 training Loss: 433.5896809919366 validation Loss: 102.57750236789059  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2988 training Loss: 433.4876243301694 validation Loss: 102.554665084074  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2989 training Loss: 433.38562496433997 validation Loss: 102.53184032895277  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2990 training Loss: 433.28368284457866 validation Loss: 102.50902809179883  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2991 training Loss: 433.18179792107435 validation Loss: 102.48622836189656  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2992 training Loss: 433.07997014407437 validation Loss: 102.46344112854273  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2993 training Loss: 432.9781994638839 validation Loss: 102.44066638104631  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2994 training Loss: 432.87648583086656 validation Loss: 102.41790410872878  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2995 training Loss: 432.77482919544366 validation Loss: 102.39515430092379  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2996 training Loss: 432.67322950809466 validation Loss: 102.37241694697737  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2997 training Loss: 432.57168671935665 validation Loss: 102.34969203624782  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2998 training Loss: 432.4702007798246 validation Loss: 102.32697955810568  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 2999 training Loss: 432.36877164015135 validation Loss: 102.3042795019337  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3000 training Loss: 432.26739925104687 validation Loss: 102.28159185712693  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3001 training Loss: 432.1660835632788 validation Loss: 102.25891661309261  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3002 training Loss: 432.0648245276725 validation Loss: 102.23625375925008  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3003 training Loss: 431.9636220951103 validation Loss: 102.21360328503097  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3004 training Loss: 431.86247621653183 validation Loss: 102.19096517987902  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3005 training Loss: 431.76138684293403 validation Loss: 102.16833943325008  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3006 training Loss: 431.6603539253708 validation Loss: 102.14572603461212  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3007 training Loss: 431.55937741495313 validation Loss: 102.1231249734453  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3008 training Loss: 431.4584572628489 validation Loss: 102.10053623924176  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3009 training Loss: 431.35759342028274 validation Loss: 102.07795982150571  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3010 training Loss: 431.2567858385361 validation Loss: 102.05539570975353  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3011 training Loss: 431.1560344689471 validation Loss: 102.03284389351347  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3012 training Loss: 431.0553392629104 validation Loss: 102.0103043623259  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3013 training Loss: 430.95470017187716 validation Loss: 101.98777710574316  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3014 training Loss: 430.85411714735505 validation Loss: 101.9652621133296  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3015 training Loss: 430.75359014090805 validation Loss: 101.94275937466145  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3016 training Loss: 430.65311910415625 validation Loss: 101.92026887932701  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3017 training Loss: 430.5527039887761 validation Loss: 101.89779061692639  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3018 training Loss: 430.4523447465001 validation Loss: 101.87532457707167  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3019 training Loss: 430.3520413291167 validation Loss: 101.85287074938682  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3020 training Loss: 430.2517936884704 validation Loss: 101.83042912350773  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3021 training Loss: 430.15160177646135 validation Loss: 101.80799968908204  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3022 training Loss: 430.05146554504563 validation Loss: 101.78558243576933  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3023 training Loss: 429.9513849462349 validation Loss: 101.76317735324099  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3024 training Loss: 429.85135993209656 validation Loss: 101.7407844311802  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3025 training Loss: 429.7513904547535 validation Loss: 101.71840365928195  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3026 training Loss: 429.65147646638405 validation Loss: 101.69603502725298  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3027 training Loss: 429.5516179192217 validation Loss: 101.67367852481182  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3028 training Loss: 429.4518147655555 validation Loss: 101.65133414168874  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3029 training Loss: 429.3520669577297 validation Loss: 101.62900186762566  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3030 training Loss: 429.2523744481434 validation Loss: 101.60668169237634  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3031 training Loss: 429.15273718925107 validation Loss: 101.58437360570616  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3032 training Loss: 429.05315513356186 validation Loss: 101.56207759739218  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3033 training Loss: 428.9536282336401 validation Loss: 101.53979365722307  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3034 training Loss: 428.8541564421046 validation Loss: 101.51752177499927  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3035 training Loss: 428.7547397116293 validation Loss: 101.49526194053271  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3036 training Loss: 428.65537799494246 validation Loss: 101.47301414364702  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3037 training Loss: 428.55607124482697 validation Loss: 101.45077837417736  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3038 training Loss: 428.4568194141203 validation Loss: 101.42855462197053  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3039 training Loss: 428.3576224557142 validation Loss: 101.40634287688484  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3040 training Loss: 428.258480322555 validation Loss: 101.38414312879014  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3041 training Loss: 428.1593929676431 validation Loss: 101.36195536756787  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3042 training Loss: 428.06036034403314 validation Loss: 101.3397795831109  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3043 training Loss: 427.96138240483367 validation Loss: 101.31761576532362  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3044 training Loss: 427.86245910320764 validation Loss: 101.29546390412193  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3045 training Loss: 427.76359039237144 validation Loss: 101.27332398943315  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3046 training Loss: 427.6647762255958 validation Loss: 101.25119601119607  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3047 training Loss: 427.5660165562051 validation Loss: 101.22907995936089  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3048 training Loss: 427.46731133757726 validation Loss: 101.20697582388922  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3049 training Loss: 427.36866052314394 validation Loss: 101.1848835947541  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3050 training Loss: 427.27006406639043 validation Loss: 101.16280326193993  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3051 training Loss: 427.17152192085547 validation Loss: 101.14073481544241  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3052 training Loss: 427.07303404013106 validation Loss: 101.11867824526871  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3053 training Loss: 426.9746003778627 validation Loss: 101.09663354143724  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3054 training Loss: 426.8762208877491 validation Loss: 101.07460069397776  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3055 training Loss: 426.77789552354216 validation Loss: 101.05257969293132  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3056 training Loss: 426.6796242390467 validation Loss: 101.03057052835024  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3057 training Loss: 426.58140698812076 validation Loss: 101.00857319029816  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3058 training Loss: 426.4832437246754 validation Loss: 100.9865876688499  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3059 training Loss: 426.3851344026742 validation Loss: 100.96461395409153  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3060 training Loss: 426.28707897613396 validation Loss: 100.94265203612039  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3061 training Loss: 426.1890773991238 validation Loss: 100.92070190504501  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3062 training Loss: 426.09112962576575 validation Loss: 100.898763550985  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3063 training Loss: 425.99323561023425 validation Loss: 100.87683696407132  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3064 training Loss: 425.8953953067563 validation Loss: 100.85492213444593  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3065 training Loss: 425.79760866961146 validation Loss: 100.83301905226203  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3066 training Loss: 425.6998756531313 validation Loss: 100.81112770768388  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3067 training Loss: 425.6021962116999 validation Loss: 100.78924809088687  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3068 training Loss: 425.5045702997535 validation Loss: 100.76738019205754  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3069 training Loss: 425.40699787178033 validation Loss: 100.74552400139339  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3070 training Loss: 425.30947888232083 validation Loss: 100.72367950910308  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3071 training Loss: 425.21201328596743 validation Loss: 100.70184670540625  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3072 training Loss: 425.11460103736414 validation Loss: 100.68002558053365  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3073 training Loss: 425.01724209120715 validation Loss: 100.65821612472698  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3074 training Loss: 424.9199364022443 validation Loss: 100.6364183282389  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3075 training Loss: 424.8226839252748 validation Loss: 100.61463218133318  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3076 training Loss: 424.7254846151499 validation Loss: 100.59285767428443  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3077 training Loss: 424.6283384267722 validation Loss: 100.5710947973783  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3078 training Loss: 424.5312453150956 validation Loss: 100.54934354091134  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3079 training Loss: 424.43420523512555 validation Loss: 100.52760389519099  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3080 training Loss: 424.3372181419188 validation Loss: 100.50587585053566  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3081 training Loss: 424.2402839905832 validation Loss: 100.48415939727462  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3082 training Loss: 424.1434027362779 validation Loss: 100.46245452574803  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3083 training Loss: 424.0465743342131 validation Loss: 100.4407612263069  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3084 training Loss: 423.94979873964996 validation Loss: 100.41907948931302  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3085 training Loss: 423.85307590790046 validation Loss: 100.39740930513912  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3086 training Loss: 423.75640579432786 validation Loss: 100.37575066416869  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3087 training Loss: 423.6597883543457 validation Loss: 100.35410355679602  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3088 training Loss: 423.56322354341853 validation Loss: 100.33246797342619  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3089 training Loss: 423.4667113170616 validation Loss: 100.31084390447504  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3090 training Loss: 423.3702516308406 validation Loss: 100.28923134036916  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3091 training Loss: 423.2738444403717 validation Loss: 100.2676302715459  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3092 training Loss: 423.17748970132163 validation Loss: 100.24604068845335  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3093 training Loss: 423.0811873694074 validation Loss: 100.2244625815502  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3094 training Loss: 422.9849374003963 validation Loss: 100.20289594130598  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3095 training Loss: 422.88873975010586 validation Loss: 100.1813407582008  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3096 training Loss: 422.79259437440385 validation Loss: 100.15979702272548  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3097 training Loss: 422.69650122920785 validation Loss: 100.13826472538145  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3098 training Loss: 422.6004602704857 validation Loss: 100.1167438566808  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3099 training Loss: 422.5044714542551 validation Loss: 100.09523440714624  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3100 training Loss: 422.40853473658365 validation Loss: 100.07373636731106  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3101 training Loss: 422.3126500735887 validation Loss: 100.05224972771916  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3102 training Loss: 422.21681742143727 validation Loss: 100.03077447892503  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3103 training Loss: 422.12103673634607 validation Loss: 100.00931061149363  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3104 training Loss: 422.02530797458144 validation Loss: 99.98785811600062  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3105 training Loss: 421.9296310924592 validation Loss: 99.96641698303202  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3106 training Loss: 421.8340060463445 validation Loss: 99.94498720318451  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3107 training Loss: 421.73843279265213 validation Loss: 99.92356876706516  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3108 training Loss: 421.642911287846 validation Loss: 99.90216166529157  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3109 training Loss: 421.54744148843906 validation Loss: 99.88076588849184  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3110 training Loss: 421.4520233509938 validation Loss: 99.85938142730447  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3111 training Loss: 421.35665683212164 validation Loss: 99.83800827237842  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3112 training Loss: 421.2613418884828 validation Loss: 99.81664641437311  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3113 training Loss: 421.1660784767869 validation Loss: 99.79529584395829  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3114 training Loss: 421.07086655379203 validation Loss: 99.77395655181421  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3115 training Loss: 420.9757060763054 validation Loss: 99.75262852863143  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3116 training Loss: 420.8805970011829 validation Loss: 99.73131176511096  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3117 training Loss: 420.78553928532875 validation Loss: 99.71000625196403  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3118 training Loss: 420.69053288569626 validation Loss: 99.68871197991233  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3119 training Loss: 420.5955777592871 validation Loss: 99.66742893968782  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3120 training Loss: 420.5006738631513 validation Loss: 99.64615712203278  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3121 training Loss: 420.40582115438747 validation Loss: 99.62489651769981  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3122 training Loss: 420.31101959014234 validation Loss: 99.60364711745174  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3123 training Loss: 420.2162691276112 validation Loss: 99.58240891206171  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3124 training Loss: 420.1215697240372 validation Loss: 99.5611818923131  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3125 training Loss: 420.0269213367119 validation Loss: 99.53996604899956  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3126 training Loss: 419.9323239229748 validation Loss: 99.51876137292489  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3127 training Loss: 419.8377774402135 validation Loss: 99.49756785490318  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3128 training Loss: 419.7432818458632 validation Loss: 99.47638548575867  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3129 training Loss: 419.64883709740735 validation Loss: 99.45521425632579  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3130 training Loss: 419.5544431523771 validation Loss: 99.43405415744914  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3131 training Loss: 419.46009996835113 validation Loss: 99.41290517998351  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3132 training Loss: 419.36580750295605 validation Loss: 99.39176731479375  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3133 training Loss: 419.2715657138658 validation Loss: 99.3706405527549  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3134 training Loss: 419.17737455880206 validation Loss: 99.34952488475211  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3135 training Loss: 419.08323399553376 validation Loss: 99.3284203016806  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3136 training Loss: 418.9891439818775 validation Loss: 99.30732679444569  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3137 training Loss: 418.8951044756969 validation Loss: 99.28624435396273  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3138 training Loss: 418.80111543490307 validation Loss: 99.26517297115721  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3139 training Loss: 418.70717681745424 validation Loss: 99.24411263696459  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3140 training Loss: 418.6132885813557 validation Loss: 99.22306334233039  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3141 training Loss: 418.5194506846599 validation Loss: 99.20202507821011  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3142 training Loss: 418.42566308546617 validation Loss: 99.18099783556931  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3143 training Loss: 418.3319257419208 validation Loss: 99.15998160538345  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3144 training Loss: 418.23823861221706 validation Loss: 99.13897637863806  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3145 training Loss: 418.1446016545949 validation Loss: 99.11798214632856  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3146 training Loss: 418.0510148273411 validation Loss: 99.09699889946032  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3147 training Loss: 417.9574780887889 validation Loss: 99.07602662904867  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3148 training Loss: 417.86399139731833 validation Loss: 99.05506532611881  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3149 training Loss: 417.7705547113558 validation Loss: 99.03411498170594  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3150 training Loss: 417.6771679893744 validation Loss: 99.01317558685506  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3151 training Loss: 417.58383118989366 validation Loss: 98.99224713262103  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3152 training Loss: 417.490544271479 validation Loss: 98.97132961006866  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3153 training Loss: 417.3973071927426 validation Loss: 98.95042301027257  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3154 training Loss: 417.3041199123427 validation Loss: 98.92952732431718  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3155 training Loss: 417.21098238898344 validation Loss: 98.90864254329676  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3156 training Loss: 417.1178945814155 validation Loss: 98.8877686583154  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3157 training Loss: 417.02485644843523 validation Loss: 98.86690566048696  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3158 training Loss: 416.9318679488849 validation Loss: 98.84605354093512  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3159 training Loss: 416.83892904165316 validation Loss: 98.82521229079326  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3160 training Loss: 416.7460396856738 validation Loss: 98.8043819012046  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3161 training Loss: 416.6531998399268 validation Loss: 98.783562363322  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3162 training Loss: 416.56040946343774 validation Loss: 98.76275366830816  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3163 training Loss: 416.46766851527786 validation Loss: 98.7419558073354  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3164 training Loss: 416.374976954564 validation Loss: 98.72116877158578  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3165 training Loss: 416.2823347404583 validation Loss: 98.70039255225106  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3166 training Loss: 416.1897418321686 validation Loss: 98.67962714053262  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3167 training Loss: 416.09719818894786 validation Loss: 98.6588725276416  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3168 training Loss: 416.0047037700947 validation Loss: 98.63812870479865  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3169 training Loss: 415.9122585349526 validation Loss: 98.61739566323418  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3170 training Loss: 415.81986244291056 validation Loss: 98.59667339418814  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3171 training Loss: 415.72751545340253 validation Loss: 98.57596188891014  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3172 training Loss: 415.6352175259076 validation Loss: 98.55526113865932  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3173 training Loss: 415.5429686199499 validation Loss: 98.53457113470449  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3174 training Loss: 415.45076869509825 validation Loss: 98.51389186832395  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3175 training Loss: 415.3586177109666 validation Loss: 98.49322333080559  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3176 training Loss: 415.26651562721366 validation Loss: 98.4725655134468  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3177 training Loss: 415.17446240354286 validation Loss: 98.45191840755459  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3178 training Loss: 415.08245799970246 validation Loss: 98.4312820044454  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3179 training Loss: 414.9905023754851 validation Loss: 98.4106562954452  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3180 training Loss: 414.8985954907281 validation Loss: 98.39004127188943  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3181 training Loss: 414.8067373053133 validation Loss: 98.36943692512304  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3182 training Loss: 414.71492777916694 validation Loss: 98.34884324650045  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3183 training Loss: 414.6231668722599 validation Loss: 98.3282602273855  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3184 training Loss: 414.53145454460696 validation Loss: 98.30768785915146  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3185 training Loss: 414.43979075626737 validation Loss: 98.28712613318102  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3186 training Loss: 414.3481754673445 validation Loss: 98.26657504086633  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3187 training Loss: 414.25660863798606 validation Loss: 98.24603457360891  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3188 training Loss: 414.1650902283837 validation Loss: 98.22550472281966  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3189 training Loss: 414.073620198773 validation Loss: 98.20498547991886  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3190 training Loss: 413.9821985094336 validation Loss: 98.18447683633615  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3191 training Loss: 413.89082512068904 validation Loss: 98.16397878351052  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3192 training Loss: 413.7994999929066 validation Loss: 98.14349131289026  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3193 training Loss: 413.7082230864973 validation Loss: 98.12301441593306  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3194 training Loss: 413.6169943619161 validation Loss: 98.10254808410582  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3195 training Loss: 413.52581377966146 validation Loss: 98.08209230888481  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3196 training Loss: 413.4346813002753 validation Loss: 98.06164708175555  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3197 training Loss: 413.34359688434336 validation Loss: 98.04121239421286  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3198 training Loss: 413.2525604924946 validation Loss: 98.02078823776078  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3199 training Loss: 413.16157208540153 validation Loss: 98.00037460391263  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3200 training Loss: 413.07063162377995 validation Loss: 97.97997148419094  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3201 training Loss: 412.9797390683889 validation Loss: 97.95957887012746  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3202 training Loss: 412.8888943800307 validation Loss: 97.93919675326316  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3203 training Loss: 412.798097519551 validation Loss: 97.91882512514819  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3204 training Loss: 412.70734844783834 validation Loss: 97.89846397734192  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3205 training Loss: 412.61664712582444 validation Loss: 97.87811330141287  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3206 training Loss: 412.5259935144838 validation Loss: 97.85777308893867  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3207 training Loss: 412.4353875748342 validation Loss: 97.83744333150614  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3208 training Loss: 412.3448292679359 validation Loss: 97.81712402071126  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3209 training Loss: 412.2543185548922 validation Loss: 97.79681514815908  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3210 training Loss: 412.1638553968494 validation Loss: 97.7765167054638  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3211 training Loss: 412.0734397549959 validation Loss: 97.75622868424864  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3212 training Loss: 411.9830715905632 validation Loss: 97.73595107614601  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3213 training Loss: 411.8927508648253 validation Loss: 97.71568387279731  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3214 training Loss: 411.80247753909856 validation Loss: 97.69542706585301  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3215 training Loss: 411.71225157474197 validation Loss: 97.67518064697268  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3216 training Loss: 411.6220729331569 validation Loss: 97.65494460782485  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3217 training Loss: 411.53194157578696 validation Loss: 97.63471894008711  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3218 training Loss: 411.44185746411813 validation Loss: 97.61450363544608  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3219 training Loss: 411.3518205596787 validation Loss: 97.59429868559735  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3220 training Loss: 411.2618308240389 validation Loss: 97.57410408224547  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3221 training Loss: 411.1718882188113 validation Loss: 97.55391981710402  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3222 training Loss: 411.08199270565046 validation Loss: 97.53374588189551  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3223 training Loss: 410.99214424625285 validation Loss: 97.51358226835137  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3224 training Loss: 410.902342802357 validation Loss: 97.49342896821203  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3225 training Loss: 410.81258833574327 validation Loss: 97.47328597322678  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3226 training Loss: 410.72288080823387 validation Loss: 97.45315327515387  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3227 training Loss: 410.6332201816927 validation Loss: 97.43303086576043  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3228 training Loss: 410.5436064180254 validation Loss: 97.41291873682248  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3229 training Loss: 410.4540394791794 validation Loss: 97.39281688012491  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3230 training Loss: 410.36451932714357 validation Loss: 97.37272528746146  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3231 training Loss: 410.2750459239483 validation Loss: 97.35264395063477  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3232 training Loss: 410.1856192316656 validation Loss: 97.3325728614563  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3233 training Loss: 410.096239212409 validation Loss: 97.31251201174632  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3234 training Loss: 410.00690582833295 validation Loss: 97.29246139333392  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3235 training Loss: 409.91761904163377 validation Loss: 97.272420998057  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3236 training Loss: 409.8283788145486 validation Loss: 97.25239081776225  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3237 training Loss: 409.73918510935596 validation Loss: 97.23237084430514  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3238 training Loss: 409.65003788837555 validation Loss: 97.2123610695499  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3239 training Loss: 409.5609371139682 validation Loss: 97.19236148536957  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3240 training Loss: 409.4718827485356 validation Loss: 97.17237208364585  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3241 training Loss: 409.38287475452057 validation Loss: 97.15239285626922  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3242 training Loss: 409.29391309440666 validation Loss: 97.13242379513889  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3243 training Loss: 409.20499773071856 validation Loss: 97.11246489216276  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3244 training Loss: 409.1161286260215 validation Loss: 97.09251613925746  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3245 training Loss: 409.0273057429217 validation Loss: 97.07257752834823  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3246 training Loss: 408.938529044066 validation Loss: 97.05264905136909  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3247 training Loss: 408.84979849214164 validation Loss: 97.03273070026262  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3248 training Loss: 408.7611140498768 validation Loss: 97.0128224669801  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3249 training Loss: 408.67247568003995 validation Loss: 96.99292434348148  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3250 training Loss: 408.58388334544026 validation Loss: 96.97303632173526  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3251 training Loss: 408.4953370089271 validation Loss: 96.95315839371867  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3252 training Loss: 408.40683663339036 validation Loss: 96.9332905514174  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3253 training Loss: 408.31838218176006 validation Loss: 96.91343278682585  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3254 training Loss: 408.22997361700675 validation Loss: 96.89358509194696  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3255 training Loss: 408.1416109021409 validation Loss: 96.87374745879222  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3256 training Loss: 408.0532940002133 validation Loss: 96.85391987938172  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3257 training Loss: 407.96502287431497 validation Loss: 96.83410234574407  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3258 training Loss: 407.8767974875765 validation Loss: 96.8142948499164  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3259 training Loss: 407.78861780316885 validation Loss: 96.79449738394439  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3260 training Loss: 407.7004837843029 validation Loss: 96.77470993988226  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3261 training Loss: 407.6123953942292 validation Loss: 96.75493250979264  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3262 training Loss: 407.5243525962384 validation Loss: 96.73516508574676  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3263 training Loss: 407.43635535366053 validation Loss: 96.71540765982425  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3264 training Loss: 407.3484036298658 validation Loss: 96.69566022411323  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3265 training Loss: 407.2604973882637 validation Loss: 96.6759227707103  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3266 training Loss: 407.1726365923034 validation Loss: 96.65619529172045  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3267 training Loss: 407.08482120547376 validation Loss: 96.63647777925715  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3268 training Loss: 406.99705119130306 validation Loss: 96.61677022544228  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3269 training Loss: 406.90932651335913 validation Loss: 96.59707262240613  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3270 training Loss: 406.82164713524907 validation Loss: 96.57738496228735  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3271 training Loss: 406.73401302061916 validation Loss: 96.55770723723307  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3272 training Loss: 406.6464241331553 validation Loss: 96.53803943939872  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3273 training Loss: 406.5588804365824 validation Loss: 96.51838156094809  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3274 training Loss: 406.4713818946648 validation Loss: 96.49873359405339  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3275 training Loss: 406.38392847120554 validation Loss: 96.47909553089511  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3276 training Loss: 406.29652013004704 validation Loss: 96.45946736366207  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3277 training Loss: 406.2091568350708 validation Loss: 96.43984908455147  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3278 training Loss: 406.121838550197 validation Loss: 96.42024068576876  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3279 training Loss: 406.03456523938496 validation Loss: 96.40064215952773  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3280 training Loss: 405.9473368666329 validation Loss: 96.38105349805045  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3281 training Loss: 405.8601533959776 validation Loss: 96.36147469356723  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3282 training Loss: 405.77301479149475 validation Loss: 96.3419057383167  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3283 training Loss: 405.68592101729894 validation Loss: 96.32234662454567  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3284 training Loss: 405.59887203754295 validation Loss: 96.3027973445093  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3285 training Loss: 405.51186781641866 validation Loss: 96.28325789047088  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3286 training Loss: 405.4249083181561 validation Loss: 96.26372825470199  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3287 training Loss: 405.33799350702407 validation Loss: 96.24420842948237  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3288 training Loss: 405.25112334732967 validation Loss: 96.2246984071  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3289 training Loss: 405.16429780341844 validation Loss: 96.20519817985102  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3290 training Loss: 405.07751683967433 validation Loss: 96.18570774003976  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3291 training Loss: 404.9907804205192 validation Loss: 96.16622707997871  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3292 training Loss: 404.90408851041377 validation Loss: 96.14675619198854  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3293 training Loss: 404.81744107385646 validation Loss: 96.12729506839804  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3294 training Loss: 404.73083807538416 validation Loss: 96.10784370154414  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3295 training Loss: 404.64427947957154 validation Loss: 96.08840208377191  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3296 training Loss: 404.55776525103147 validation Loss: 96.06897020743445  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3297 training Loss: 404.4712953544147 validation Loss: 96.04954806489309  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3298 training Loss: 404.38486975441015 validation Loss: 96.03013564851719  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3299 training Loss: 404.29848841574426 validation Loss: 96.01073295068416  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3300 training Loss: 404.2121513031817 validation Loss: 95.99133996377952  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3301 training Loss: 404.1258583815245 validation Loss: 95.97195668019685  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3302 training Loss: 404.0396096156128 validation Loss: 95.95258309233775  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3303 training Loss: 403.9534049703242 validation Loss: 95.93321919261186  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3304 training Loss: 403.86724441057385 validation Loss: 95.91386497343692  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3305 training Loss: 403.78112790131473 validation Loss: 95.89452042723856  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3306 training Loss: 403.6950554075372 validation Loss: 95.8751855464505  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3307 training Loss: 403.6090268942691 validation Loss: 95.85586032351443  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3308 training Loss: 403.52304232657565 validation Loss: 95.83654475088005  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3309 training Loss: 403.4371016695594 validation Loss: 95.81723882100499  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3310 training Loss: 403.35120488836054 validation Loss: 95.79794252635489  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3311 training Loss: 403.2653519481562 validation Loss: 95.77865585940333  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3312 training Loss: 403.17954281416087 validation Loss: 95.75937881263177  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3313 training Loss: 403.0937774516261 validation Loss: 95.74011137852969  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3314 training Loss: 403.00805582584087 validation Loss: 95.72085354959445  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3315 training Loss: 402.92237790213085 validation Loss: 95.7016053183313  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3316 training Loss: 402.836743645859 validation Loss: 95.68236667725348  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3317 training Loss: 402.751153022425 validation Loss: 95.66313761888199  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3318 training Loss: 402.66560599726597 validation Loss: 95.64391813574579  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3319 training Loss: 402.5801025358552 validation Loss: 95.6247082203817  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3320 training Loss: 402.49464260370326 validation Loss: 95.60550786533437  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3321 training Loss: 402.4092261663576 validation Loss: 95.58631706315636  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3322 training Loss: 402.32385318940203 validation Loss: 95.56713580640799  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3323 training Loss: 402.2385236384573 validation Loss: 95.54796408765745  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3324 training Loss: 402.1532374791807 validation Loss: 95.52880189948073  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3325 training Loss: 402.0679946772659 validation Loss: 95.50964923446165  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3326 training Loss: 401.9827951984436 validation Loss: 95.49050608519183  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3327 training Loss: 401.89763900848067 validation Loss: 95.47137244427064  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3328 training Loss: 401.8125260731804 validation Loss: 95.45224830430524  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3329 training Loss: 401.72745635838254 validation Loss: 95.43313365791056  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3330 training Loss: 401.642429829963 validation Loss: 95.41402849770931  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3331 training Loss: 401.55744645383436 validation Loss: 95.3949328163319  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3332 training Loss: 401.47250619594524 validation Loss: 95.3758466064165  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3333 training Loss: 401.38760902228023 validation Loss: 95.356769860609  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3334 training Loss: 401.30275489886037 validation Loss: 95.337702571563  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3335 training Loss: 401.2179437917428 validation Loss: 95.31864473193983  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3336 training Loss: 401.13317566702045 validation Loss: 95.29959633440848  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3337 training Loss: 401.0484504908226 validation Loss: 95.28055737164561  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3338 training Loss: 400.96376822931416 validation Loss: 95.26152783633562  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3339 training Loss: 400.87912884869604 validation Loss: 95.24250772117051  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3340 training Loss: 400.79453231520523 validation Loss: 95.22349701885001  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3341 training Loss: 400.7099785951143 validation Loss: 95.20449572208136  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3342 training Loss: 400.62546765473155 validation Loss: 95.18550382357961  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3343 training Loss: 400.5409994604013 validation Loss: 95.16652131606725  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3344 training Loss: 400.4565739785031 validation Loss: 95.14754819227454  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3345 training Loss: 400.3721911754524 validation Loss: 95.12858444493928  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3346 training Loss: 400.28785101770035 validation Loss: 95.1096300668068  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3347 training Loss: 400.20355347173324 validation Loss: 95.09068505063014  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3348 training Loss: 400.11929850407313 validation Loss: 95.07174938916981  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3349 training Loss: 400.0350860812774 validation Loss: 95.05282307519397  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3350 training Loss: 399.950916169939 validation Loss: 95.03390610147824  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3351 training Loss: 399.866788736686 validation Loss: 95.01499846080586  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3352 training Loss: 399.7827037481817 validation Loss: 94.99610014596755  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3353 training Loss: 399.698661171125 validation Loss: 94.97721114976159  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3354 training Loss: 399.6146609722495 validation Loss: 94.95833146499379  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3355 training Loss: 399.53070311832454 validation Loss: 94.93946108447739  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3356 training Loss: 399.44678757615407 validation Loss: 94.92060000103322  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3357 training Loss: 399.3629143125773 validation Loss: 94.90174820748953  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3358 training Loss: 399.2790832944684 validation Loss: 94.88290569668209  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3359 training Loss: 399.19529448873664 validation Loss: 94.86407246145407  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3360 training Loss: 399.1115478623262 validation Loss: 94.84524849465616  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3361 training Loss: 399.0278433822158 validation Loss: 94.82643378914645  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3362 training Loss: 398.9441810154194 validation Loss: 94.80762833779052  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3363 training Loss: 398.8605607289855 validation Loss: 94.78883213346131  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3364 training Loss: 398.7769824899975 validation Loss: 94.77004516903924  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3365 training Loss: 398.69344626557336 validation Loss: 94.75126743741208  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3366 training Loss: 398.60995202286574 validation Loss: 94.73249893147506  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3367 training Loss: 398.5264997290619 validation Loss: 94.71373964413075  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3368 training Loss: 398.44308935138366 validation Loss: 94.69498956828909  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3369 training Loss: 398.35972085708727 validation Loss: 94.67624869686743  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3370 training Loss: 398.2763942134636 validation Loss: 94.65751702279049  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3371 training Loss: 398.1931093878378 validation Loss: 94.63879453899025  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3372 training Loss: 398.1098663475694 validation Loss: 94.62008123840613  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3373 training Loss: 398.0266650600523 validation Loss: 94.6013771139848  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3374 training Loss: 397.9435054927147 validation Loss: 94.58268215868034  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3375 training Loss: 397.86038761301904 validation Loss: 94.56399636545406  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3376 training Loss: 397.7773113884617 validation Loss: 94.5453197272746  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3377 training Loss: 397.6942767865737 validation Loss: 94.5266522371179  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3378 training Loss: 397.61128377491974 validation Loss: 94.50799388796716  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3379 training Loss: 397.52833232109873 validation Loss: 94.4893446728129  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3380 training Loss: 397.4454223927436 validation Loss: 94.47070458465288  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3381 training Loss: 397.3625539575212 validation Loss: 94.45207361649207  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3382 training Loss: 397.2797269831324 validation Loss: 94.43345176134272  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3383 training Loss: 397.1969414373117 validation Loss: 94.41483901222435  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3384 training Loss: 397.1141972878278 validation Loss: 94.39623536216368  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3385 training Loss: 397.0314945024828 validation Loss: 94.37764080419458  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3386 training Loss: 396.948833049113 validation Loss: 94.35905533135828  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3387 training Loss: 396.866212895588 validation Loss: 94.34047893670302  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3388 training Loss: 396.783634009811 validation Loss: 94.32191161328439  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3389 training Loss: 396.70109635971926 validation Loss: 94.30335335416504  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3390 training Loss: 396.61859991328333 validation Loss: 94.28480415241489  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3391 training Loss: 396.53614463850715 validation Loss: 94.26626400111098  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3392 training Loss: 396.45373050342846 validation Loss: 94.24773289333747  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3393 training Loss: 396.3713574761183 validation Loss: 94.22921082218568  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3394 training Loss: 396.28902552468094 validation Loss: 94.21069778075409  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3395 training Loss: 396.20673461725414 validation Loss: 94.19219376214826  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3396 training Loss: 396.124484722009 validation Loss: 94.17369875948089  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3397 training Loss: 396.04227580714974 validation Loss: 94.15521276587181  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3398 training Loss: 395.960107840914 validation Loss: 94.13673577444789  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3399 training Loss: 395.8779807915723 validation Loss: 94.11826777834315  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3400 training Loss: 395.7958946274288 validation Loss: 94.0998087706986  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3401 training Loss: 395.71384931682013 validation Loss: 94.08135874466242  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3402 training Loss: 395.6318448281164 validation Loss: 94.06291769338978  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3403 training Loss: 395.5498811297206 validation Loss: 94.04448561004293  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3404 training Loss: 395.46795819006843 validation Loss: 94.02606248779114  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3405 training Loss: 395.38607597762893 validation Loss: 94.00764831981073  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3406 training Loss: 395.30423446090373 validation Loss: 93.989243099285  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3407 training Loss: 395.2224336084274 validation Loss: 93.97084681940439  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3408 training Loss: 395.1406733887672 validation Loss: 93.95245947336615  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3409 training Loss: 395.0589537705232 validation Loss: 93.93408105437467  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3410 training Loss: 394.97727472232816 validation Loss: 93.91571155564134  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3411 training Loss: 394.8956362128475 validation Loss: 93.8973509703844  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3412 training Loss: 394.81403821077924 validation Loss: 93.87899929182917  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3413 training Loss: 394.73248068485407 validation Loss: 93.86065651320789  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3414 training Loss: 394.650963603835 validation Loss: 93.84232262775976  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3415 training Loss: 394.5694869365177 validation Loss: 93.8239976287309  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3416 training Loss: 394.4880506517303 validation Loss: 93.80568150937435  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3417 training Loss: 394.40665471833313 validation Loss: 93.78737426295015  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3418 training Loss: 394.3252991052191 validation Loss: 93.7690758827252  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3419 training Loss: 394.24398378131326 validation Loss: 93.7507863619733  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3420 training Loss: 394.1627087155732 validation Loss: 93.73250569397516  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3421 training Loss: 394.0814738769884 validation Loss: 93.71423387201838  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3422 training Loss: 394.00027923458083 validation Loss: 93.69597088939744  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3423 training Loss: 393.9191247574044 validation Loss: 93.67771673941368  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3424 training Loss: 393.8380104145453 validation Loss: 93.65947141537534  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3425 training Loss: 393.7569361751216 validation Loss: 93.64123491059743  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3426 training Loss: 393.6759020082836 validation Loss: 93.6230072184019  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3427 training Loss: 393.5949078832134 validation Loss: 93.60478833211747  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3428 training Loss: 393.5139537691251 validation Loss: 93.58657824507969  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3429 training Loss: 393.4330396352648 validation Loss: 93.56837695063098  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3430 training Loss: 393.35216545091026 validation Loss: 93.55018444212052  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3431 training Loss: 393.27133118537137 validation Loss: 93.53200071290433  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3432 training Loss: 393.1905368079895 validation Loss: 93.5138257563452  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3433 training Loss: 393.10978228813775 validation Loss: 93.49565956581264  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3434 training Loss: 393.02906759522125 validation Loss: 93.47750213468301  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3435 training Loss: 392.94839269867623 validation Loss: 93.45935345633944  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3436 training Loss: 392.8677575679712 validation Loss: 93.44121352417181  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3437 training Loss: 392.7871621726057 validation Loss: 93.42308233157672  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3438 training Loss: 392.70660648211117 validation Loss: 93.40495987195752  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3439 training Loss: 392.6260904660501 validation Loss: 93.38684613872428  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3440 training Loss: 392.54561409401686 validation Loss: 93.36874112529382  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3441 training Loss: 392.46517733563707 validation Loss: 93.35064482508967  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3442 training Loss: 392.38478016056763 validation Loss: 93.33255723154204  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3443 training Loss: 392.30442253849696 validation Loss: 93.31447833808784  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3444 training Loss: 392.2241044391445 validation Loss: 93.2964081381707  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3445 training Loss: 392.1438258322613 validation Loss: 93.27834662524089  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3446 training Loss: 392.06358668762914 validation Loss: 93.26029379275536  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3447 training Loss: 391.98338697506136 validation Loss: 93.24224963417777  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3448 training Loss: 391.90322666440227 validation Loss: 93.22421414297835  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3449 training Loss: 391.82310572552734 validation Loss: 93.20618731263406  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3450 training Loss: 391.743024128343 validation Loss: 93.18816913662843  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3451 training Loss: 391.6629818427867 validation Loss: 93.17015960845166  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3452 training Loss: 391.5829788388268 validation Loss: 93.15215872160054  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3453 training Loss: 391.50301508646277 validation Loss: 93.1341664695785  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3454 training Loss: 391.4230905557248 validation Loss: 93.11618284589554  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3455 training Loss: 391.343205216674 validation Loss: 93.09820784406828  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3456 training Loss: 391.2633590394021 validation Loss: 93.08024145761993  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3457 training Loss: 391.18355199403186 validation Loss: 93.06228368008027  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3458 training Loss: 391.10378405071674 validation Loss: 93.04433450498561  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3459 training Loss: 391.02405517964064 validation Loss: 93.0263939258789  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3460 training Loss: 390.9443653510183 validation Loss: 93.00846193630957  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3461 training Loss: 390.8647145350951 validation Loss: 92.99053852983366  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3462 training Loss: 390.7851027021469 validation Loss: 92.97262370001364  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3463 training Loss: 390.7055298224801 validation Loss: 92.95471744041862  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3464 training Loss: 390.6259958664315 validation Loss: 92.93681974462422  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3465 training Loss: 390.5465008043685 validation Loss: 92.91893060621246  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3466 training Loss: 390.46704460668894 validation Loss: 92.90105001877201  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3467 training Loss: 390.3876272438208 validation Loss: 92.88317797589794  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3468 training Loss: 390.30824868622267 validation Loss: 92.8653144711918  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3469 training Loss: 390.22890890438316 validation Loss: 92.84745949826166  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3470 training Loss: 390.1496078688213 validation Loss: 92.82961305072207  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3471 training Loss: 390.07034555008636 validation Loss: 92.811775122194  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3472 training Loss: 389.9911219187577 validation Loss: 92.79394570630492  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3473 training Loss: 389.91193694544495 validation Loss: 92.77612479668869  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3474 training Loss: 389.8327906007876 validation Loss: 92.75831238698562  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3475 training Loss: 389.7536828554554 validation Loss: 92.7405084708425  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3476 training Loss: 389.674613680148 validation Loss: 92.72271304191247  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3477 training Loss: 389.5955830455952 validation Loss: 92.70492609385514  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3478 training Loss: 389.5165909225566 validation Loss: 92.68714762033649  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3479 training Loss: 389.4376372818217 validation Loss: 92.66937761502889  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3480 training Loss: 389.3587220942099 validation Loss: 92.65161607161113  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3481 training Loss: 389.27984533057054 validation Loss: 92.63386298376838  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3482 training Loss: 389.2010069617826 validation Loss: 92.61611834519215  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3483 training Loss: 389.1222069587548 validation Loss: 92.59838214958029  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3484 training Loss: 389.04344529242576 validation Loss: 92.58065439063711  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3485 training Loss: 388.96472193376354 validation Loss: 92.56293506207317  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3486 training Loss: 388.8860368537661 validation Loss: 92.54522415760539  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3487 training Loss: 388.8073900234608 validation Loss: 92.52752167095704  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3488 training Loss: 388.7287814139046 validation Loss: 92.50982759585773  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3489 training Loss: 388.6502109961839 validation Loss: 92.49214192604333  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3490 training Loss: 388.57167874141487 validation Loss: 92.47446465525609  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3491 training Loss: 388.4931846207429 validation Loss: 92.45679577724447  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3492 training Loss: 388.4147286053428 validation Loss: 92.43913528576331  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3493 training Loss: 388.3363106664189 validation Loss: 92.42148317457368  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3494 training Loss: 388.2579307752047 validation Loss: 92.40383943744294  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3495 training Loss: 388.1795889029631 validation Loss: 92.3862040681447  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3496 training Loss: 388.10128502098615 validation Loss: 92.36857706045889  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3497 training Loss: 388.0230191005953 validation Loss: 92.35095840817164  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3498 training Loss: 387.94479111314115 validation Loss: 92.33334810507532  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3499 training Loss: 387.86660103000327 validation Loss: 92.31574614496854  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3500 training Loss: 387.78844882259057 validation Loss: 92.29815252165615  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3501 training Loss: 387.71033446234077 validation Loss: 92.28056722894927  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3502 training Loss: 387.63225792072103 validation Loss: 92.26299026066512  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3503 training Loss: 387.55421916922705 validation Loss: 92.24542161062723  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3504 training Loss: 387.4762181793839 validation Loss: 92.22786127266525  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3505 training Loss: 387.3982549227454 validation Loss: 92.21030924061509  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3506 training Loss: 387.3203293708941 validation Loss: 92.19276550831876  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3507 training Loss: 387.2424414954417 validation Loss: 92.17523006962455  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3508 training Loss: 387.16459126802863 validation Loss: 92.15770291838679  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3509 training Loss: 387.08677866032394 validation Loss: 92.14018404846607  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3510 training Loss: 387.0090036440256 validation Loss: 92.12267345372908  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3511 training Loss: 386.93126619086024 validation Loss: 92.10517112804868  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3512 training Loss: 386.8535662725831 validation Loss: 92.08767706530384  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3513 training Loss: 386.77590386097813 validation Loss: 92.07019125937967  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3514 training Loss: 386.6982789278579 validation Loss: 92.05271370416737  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3515 training Loss: 386.6206914450635 validation Loss: 92.03524439356434  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3516 training Loss: 386.54314138446455 validation Loss: 92.01778332147398  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3517 training Loss: 386.4656287179592 validation Loss: 92.00033048180583  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3518 training Loss: 386.38815341747386 validation Loss: 91.98288586847553  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3519 training Loss: 386.31071545496354 validation Loss: 91.96544947540481  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3520 training Loss: 386.23331480241177 validation Loss: 91.9480212965214  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3521 training Loss: 386.15595143183003 validation Loss: 91.9306013257592  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3522 training Loss: 386.0786253152585 validation Loss: 91.91318955705808  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3523 training Loss: 386.00133642476544 validation Loss: 91.89578598436401  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3524 training Loss: 385.9240847324472 validation Loss: 91.87839060162901  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3525 training Loss: 385.84687021042896 validation Loss: 91.8610034028111  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3526 training Loss: 385.76969283086316 validation Loss: 91.84362438187433  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3527 training Loss: 385.692552565931 validation Loss: 91.82625353278883  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3528 training Loss: 385.61544938784164 validation Loss: 91.80889084953066  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3529 training Loss: 385.53838326883215 validation Loss: 91.79153632608194  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3530 training Loss: 385.46135418116785 validation Loss: 91.77418995643077  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3531 training Loss: 385.384362097142 validation Loss: 91.75685173457123  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3532 training Loss: 385.3074069890756 validation Loss: 91.73952165450342  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3533 training Loss: 385.2304888293177 validation Loss: 91.72219971023337  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3534 training Loss: 385.15360759024554 validation Loss: 91.70488589577309  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3535 training Loss: 385.0767632442634 validation Loss: 91.68758020514056  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3536 training Loss: 384.9999557638044 validation Loss: 91.67028263235976  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3537 training Loss: 384.92318512132863 validation Loss: 91.65299317146051  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3538 training Loss: 384.84645128932436 validation Loss: 91.63571181647865  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3539 training Loss: 384.7697542403074 validation Loss: 91.6184385614559  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3540 training Loss: 384.69309394682114 validation Loss: 91.60117340043996  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3541 training Loss: 384.6164703814369 validation Loss: 91.58391632748442  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3542 training Loss: 384.53988351675343 validation Loss: 91.56666733664875  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3543 training Loss: 384.4633333253969 validation Loss: 91.54942642199833  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3544 training Loss: 384.3868197800212 validation Loss: 91.53219357760447  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3545 training Loss: 384.3103428533077 validation Loss: 91.5149687975444  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3546 training Loss: 384.23390251796536 validation Loss: 91.4977520759011  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3547 training Loss: 384.1574987467302 validation Loss: 91.4805434067635  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3548 training Loss: 384.0811315123659 validation Loss: 91.46334278422643  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3549 training Loss: 384.0048007876636 validation Loss: 91.44615020239051  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3550 training Loss: 383.92850654544156 validation Loss: 91.42896565536225  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3551 training Loss: 383.8522487585452 validation Loss: 91.41178913725396  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3552 training Loss: 383.7760273998476 validation Loss: 91.39462064218387  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3553 training Loss: 383.6998424422488 validation Loss: 91.37746016427594  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3554 training Loss: 383.62369385867595 validation Loss: 91.36030769766  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3555 training Loss: 383.5475816220837 validation Loss: 91.3431632364717  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3556 training Loss: 383.4715057054534 validation Loss: 91.32602677485247  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3557 training Loss: 383.39546608179387 validation Loss: 91.30889830694959  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3558 training Loss: 383.31946272414064 validation Loss: 91.29177782691605  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3559 training Loss: 383.24349560555646 validation Loss: 91.27466532891069  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3560 training Loss: 383.1675646991311 validation Loss: 91.25756080709809  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3561 training Loss: 383.09166997798104 validation Loss: 91.24046425564865  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3562 training Loss: 383.01581141525 validation Loss: 91.22337566873847  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3563 training Loss: 382.93998898410825 validation Loss: 91.20629504054945  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3564 training Loss: 382.8642026577532 validation Loss: 91.18922236526923  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3565 training Loss: 382.7884524094089 validation Loss: 91.17215763709119  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3566 training Loss: 382.71273821232626 validation Loss: 91.15510085021442  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3567 training Loss: 382.6370600397829 validation Loss: 91.13805199884378  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3568 training Loss: 382.5614178650831 validation Loss: 91.12101107718985  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3569 training Loss: 382.48581166155805 validation Loss: 91.10397807946886  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3570 training Loss: 382.4102414025651 validation Loss: 91.08695299990279  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3571 training Loss: 382.33470706148887 validation Loss: 91.06993583271935  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3572 training Loss: 382.25920861173995 validation Loss: 91.0529265721519  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3573 training Loss: 382.18374602675595 validation Loss: 91.03592521243948  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3574 training Loss: 382.1083192800007 validation Loss: 91.01893174782685  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3575 training Loss: 382.03292834496466 validation Loss: 91.0019461725644  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3576 training Loss: 381.9575731951646 validation Loss: 90.98496848090818  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3577 training Loss: 381.88225380414383 validation Loss: 90.96799866711996  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3578 training Loss: 381.80697014547206 validation Loss: 90.9510367254671  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3579 training Loss: 381.7317221927452 validation Loss: 90.93408265022262  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3580 training Loss: 381.6565099195857 validation Loss: 90.91713643566513  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3581 training Loss: 381.5813332996419 validation Loss: 90.90019807607898  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3582 training Loss: 381.5061923065888 validation Loss: 90.88326756575402  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3583 training Loss: 381.43108691412743 validation Loss: 90.86634489898582  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3584 training Loss: 381.35601709598507 validation Loss: 90.84943007007548  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3585 training Loss: 381.2809828259152 validation Loss: 90.83252307332974  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3586 training Loss: 381.20598407769717 validation Loss: 90.81562390306092  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3587 training Loss: 381.1310208251366 validation Loss: 90.79873255358693  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3588 training Loss: 381.05609304206513 validation Loss: 90.78184901923126  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3589 training Loss: 380.9812007023405 validation Loss: 90.76497329432298  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3590 training Loss: 380.90634377984645 validation Loss: 90.74810537319675  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3591 training Loss: 380.83152224849243 validation Loss: 90.73124525019273  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3592 training Loss: 380.75673608221416 validation Loss: 90.71439291965667  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3593 training Loss: 380.6819852549729 validation Loss: 90.69754837593987  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3594 training Loss: 380.6072697407561 validation Loss: 90.6807116133992  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3595 training Loss: 380.53258951357685 validation Loss: 90.66388262639694  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3596 training Loss: 380.45794454747397 validation Loss: 90.64706140930105  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3597 training Loss: 380.3833348165122 validation Loss: 90.63024795648491  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3598 training Loss: 380.30876029478196 validation Loss: 90.61344226232742  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3599 training Loss: 380.23422095639944 validation Loss: 90.59664432121306  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3600 training Loss: 380.15971677550624 validation Loss: 90.57985412753173  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3601 training Loss: 380.08524772627 validation Loss: 90.56307167567886  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3602 training Loss: 380.0108137828835 validation Loss: 90.54629696005532  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3603 training Loss: 379.93641491956555 validation Loss: 90.52952997506752  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3604 training Loss: 379.86205111056006 validation Loss: 90.5127707151273  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3605 training Loss: 379.7877223301369 validation Loss: 90.49601917465199  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3606 training Loss: 379.71342855259104 validation Loss: 90.47927534806436  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3607 training Loss: 379.63916975224305 validation Loss: 90.46253922979261  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3608 training Loss: 379.5649459034389 validation Loss: 90.44581081427047  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3609 training Loss: 379.49075698055 validation Loss: 90.429090095937  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3610 training Loss: 379.416602957973 validation Loss: 90.41237706923678  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3611 training Loss: 379.34248381013003 validation Loss: 90.39567172861975  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3612 training Loss: 379.26839951146826 validation Loss: 90.37897406854131  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3613 training Loss: 379.19435003646026 validation Loss: 90.36228408346221  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3614 training Loss: 379.1203353596038 validation Loss: 90.34560176784872  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3615 training Loss: 379.04635545542203 validation Loss: 90.32892711617241  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3616 training Loss: 378.97241029846305 validation Loss: 90.31226012291026  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3617 training Loss: 378.8984998633 validation Loss: 90.29560078254467  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3618 training Loss: 378.82462412453134 validation Loss: 90.27894908956335  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3619 training Loss: 378.7507830567805 validation Loss: 90.26230503845949  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3620 training Loss: 378.676976634696 validation Loss: 90.24566862373153  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3621 training Loss: 378.60320483295135 validation Loss: 90.22903983988334  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3622 training Loss: 378.52946762624504 validation Loss: 90.21241868142413  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3623 training Loss: 378.4557649893002 validation Loss: 90.19580514286844  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3624 training Loss: 378.3820968968655 validation Loss: 90.17919921873614  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3625 training Loss: 378.308463323714 validation Loss: 90.16260090355249  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3626 training Loss: 378.23486424464386 validation Loss: 90.14601019184798  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3627 training Loss: 378.1612996344777 validation Loss: 90.12942707815853  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3628 training Loss: 378.0877694680635 validation Loss: 90.1128515570253  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3629 training Loss: 378.01427372027354 validation Loss: 90.09628362299475  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3630 training Loss: 377.94081236600505 validation Loss: 90.07972327061869  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3631 training Loss: 377.86738538017977 validation Loss: 90.0631704944542  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3632 training Loss: 377.79399273774436 validation Loss: 90.04662528906361  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3633 training Loss: 377.7206344136699 validation Loss: 90.03008764901462  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3634 training Loss: 377.64731038295236 validation Loss: 90.0135575688801  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3635 training Loss: 377.57402062061186 validation Loss: 89.99703504323828  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3636 training Loss: 377.5007651016934 validation Loss: 89.9805200666726  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3637 training Loss: 377.42754380126644 validation Loss: 89.96401263377174  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3638 training Loss: 377.35435669442484 validation Loss: 89.94751273912968  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3639 training Loss: 377.2812037562871 validation Loss: 89.93102037734559  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3640 training Loss: 377.20808496199584 validation Loss: 89.91453554302392  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3641 training Loss: 377.1350002867182 validation Loss: 89.89805823077432  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3642 training Loss: 377.06194970564593 validation Loss: 89.88158843521168  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3643 training Loss: 376.9889331939949 validation Loss: 89.86512615095612  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3644 training Loss: 376.9159507270052 validation Loss: 89.84867137263288  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3645 training Loss: 376.8430022799414 validation Loss: 89.83222409487257  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3646 training Loss: 376.7700878280922 validation Loss: 89.81578431231084  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3647 training Loss: 376.69720734677054 validation Loss: 89.7993520195886  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3648 training Loss: 376.6243608113135 validation Loss: 89.78292721135196  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3649 training Loss: 376.5515481970825 validation Loss: 89.76650988225215  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3650 training Loss: 376.47876947946287 validation Loss: 89.75010002694566  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3651 training Loss: 376.40602463386415 validation Loss: 89.73369764009405  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3652 training Loss: 376.3333136357199 validation Loss: 89.71730271636412  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3653 training Loss: 376.2606364604877 validation Loss: 89.70091525042774  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3654 training Loss: 376.1879930836492 validation Loss: 89.68453523696203  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3655 training Loss: 376.11538348071014 validation Loss: 89.66816267064917  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3656 training Loss: 376.0428076272 validation Loss: 89.65179754617651  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3657 training Loss: 375.97026549867223 validation Loss: 89.63543985823654  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3658 training Loss: 375.8977570707043 validation Loss: 89.6190896015268  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3659 training Loss: 375.8252823188973 validation Loss: 89.60274677075004  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3660 training Loss: 375.7528412188764 validation Loss: 89.58641136061408  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3661 training Loss: 375.68043374629053 validation Loss: 89.57008336583183  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3662 training Loss: 375.6080598768123 validation Loss: 89.5537627811213  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3663 training Loss: 375.5357195861382 validation Loss: 89.53744960120565  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3664 training Loss: 375.4634128499882 validation Loss: 89.52114382081304  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3665 training Loss: 375.39113964410615 validation Loss: 89.50484543467672  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3666 training Loss: 375.3188999442596 validation Loss: 89.48855443753513  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3667 training Loss: 375.2466937262395 validation Loss: 89.4722708241316  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3668 training Loss: 375.17452096586084 validation Loss: 89.45599458921464  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3669 training Loss: 375.10238163896156 validation Loss: 89.43972572753779  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3670 training Loss: 375.03027572140365 validation Loss: 89.42346423385962  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3671 training Loss: 374.95820318907244 validation Loss: 89.40721010294374  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3672 training Loss: 374.88616401787664 validation Loss: 89.39096332955882  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3673 training Loss: 374.8141581837486 validation Loss: 89.37472390847856  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3674 training Loss: 374.74218566264403 validation Loss: 89.35849183448165  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3675 training Loss: 374.670246430542 validation Loss: 89.34226710235185  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3676 training Loss: 374.5983404634451 validation Loss: 89.3260497068779  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3677 training Loss: 374.52646773737894 validation Loss: 89.30983964285352  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3678 training Loss: 374.45462822839283 validation Loss: 89.29363690507746  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3679 training Loss: 374.3828219125591 validation Loss: 89.2774414883535  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3680 training Loss: 374.3110487659734 validation Loss: 89.26125338749037  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3681 training Loss: 374.2393087647548 validation Loss: 89.24507259730176  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3682 training Loss: 374.16760188504537 validation Loss: 89.22889911260634  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3683 training Loss: 374.09592810301024 validation Loss: 89.21273292822784  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3684 training Loss: 374.024287394838 validation Loss: 89.19657403899481  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3685 training Loss: 373.95267973674015 validation Loss: 89.18042243974085  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3686 training Loss: 373.88110510495125 validation Loss: 89.16427812530452  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3687 training Loss: 373.80956347572896 validation Loss: 89.14814109052925  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3688 training Loss: 373.7380548253542 validation Loss: 89.13201133026352  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3689 training Loss: 373.6665791301306 validation Loss: 89.11588883936062  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3690 training Loss: 373.59513636638485 validation Loss: 89.09977361267887  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3691 training Loss: 373.52372651046653 validation Loss: 89.08366564508145  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3692 training Loss: 373.45234953874836 validation Loss: 89.06756493143651  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3693 training Loss: 373.3810054276257 validation Loss: 89.05147146661703  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3694 training Loss: 373.30969415351694 validation Loss: 89.03538524550099  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3695 training Loss: 373.2384156928632 validation Loss: 89.01930626297118  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3696 training Loss: 373.1671700221285 validation Loss: 89.00323451391537  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3697 training Loss: 373.09595711779957 validation Loss: 88.98716999322613  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3698 training Loss: 373.02477695638595 validation Loss: 88.971112695801  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3699 training Loss: 372.95362951441984 validation Loss: 88.95506261654228  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3700 training Loss: 372.88251476845625 validation Loss: 88.93901975035728  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3701 training Loss: 372.8114326950727 validation Loss: 88.92298409215805  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3702 training Loss: 372.74038327086953 validation Loss: 88.90695563686155  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3703 training Loss: 372.66936647246956 validation Loss: 88.8909343793896  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3704 training Loss: 372.59838227651824 validation Loss: 88.87492031466887  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3705 training Loss: 372.52743065968366 validation Loss: 88.85891343763083  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3706 training Loss: 372.45651159865616 validation Loss: 88.84291374321182  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3707 training Loss: 372.3856250701491 validation Loss: 88.826921226353  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3708 training Loss: 372.3147710508978 validation Loss: 88.81093588200034  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3709 training Loss: 372.24394951766044 validation Loss: 88.79495770510465  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3710 training Loss: 372.17316044721724 validation Loss: 88.77898669062154  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3711 training Loss: 372.1024038163711 validation Loss: 88.7630228335114  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3712 training Loss: 372.0316796019472 validation Loss: 88.74706612873946  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3713 training Loss: 371.96098778079306 validation Loss: 88.73111657127573  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3714 training Loss: 371.8903283297785 validation Loss: 88.715174156095  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3715 training Loss: 371.8197012257958 validation Loss: 88.69923887817686  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3716 training Loss: 371.74910644575925 validation Loss: 88.68331073250565  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3717 training Loss: 371.6785439666055 validation Loss: 88.66738971407048  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3718 training Loss: 371.60801376529326 validation Loss: 88.65147581786528  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3719 training Loss: 371.53751581880385 validation Loss: 88.63556903888866  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3720 training Loss: 371.4670501041401 validation Loss: 88.61966937214406  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3721 training Loss: 371.3966165983276 validation Loss: 88.60377681263961  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3722 training Loss: 371.3262152784137 validation Loss: 88.5878913553882  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3723 training Loss: 371.2558461214678 validation Loss: 88.57201299540749  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3724 training Loss: 371.1855091045815 validation Loss: 88.55614172771983  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3725 training Loss: 371.1152042048684 validation Loss: 88.5402775473523  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3726 training Loss: 371.04493139946396 validation Loss: 88.52442044933669  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3727 training Loss: 370.9746906655257 validation Loss: 88.50857042870959  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3728 training Loss: 370.9044819802333 validation Loss: 88.49272748051216  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3729 training Loss: 370.8343053207878 validation Loss: 88.47689159979035  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3730 training Loss: 370.7641606644127 validation Loss: 88.46106278159482  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3731 training Loss: 370.6940479883532 validation Loss: 88.44524102098089  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3732 training Loss: 370.623967269876 validation Loss: 88.42942631300855  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3733 training Loss: 370.5539184862701 validation Loss: 88.4136186527425  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3734 training Loss: 370.483901614846 validation Loss: 88.39781803525207  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3735 training Loss: 370.4139166329359 validation Loss: 88.38202445561137  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3736 training Loss: 370.3439635178941 validation Loss: 88.36623790889905  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3737 training Loss: 370.2740422470963 validation Loss: 88.3504583901985  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3738 training Loss: 370.2041527979398 validation Loss: 88.33468589459767  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3739 training Loss: 370.1342951478438 validation Loss: 88.31892041718928  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3740 training Loss: 370.06446927424895 validation Loss: 88.30316195307061  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3741 training Loss: 369.9946751546177 validation Loss: 88.2874104973436  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3742 training Loss: 369.9249127664339 validation Loss: 88.27166604511481  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3743 training Loss: 369.85518208720305 validation Loss: 88.25592859149543  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3744 training Loss: 369.785483094452 validation Loss: 88.24019813160129  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3745 training Loss: 369.7158157657293 validation Loss: 88.22447466055281  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3746 training Loss: 369.64618007860497 validation Loss: 88.20875817347502  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3747 training Loss: 369.5765760106703 validation Loss: 88.19304866549756  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3748 training Loss: 369.507003539538 validation Loss: 88.17734613175466  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3749 training Loss: 369.43746264284255 validation Loss: 88.16165056738518  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3750 training Loss: 369.36795329823934 validation Loss: 88.14596196753249  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3751 training Loss: 369.29847548340535 validation Loss: 88.13028032734462  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3752 training Loss: 369.22902917603875 validation Loss: 88.11460564197418  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3753 training Loss: 369.1596143538593 validation Loss: 88.09893790657821  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3754 training Loss: 369.0902309946075 validation Loss: 88.08327711631854  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3755 training Loss: 369.02087907604556 validation Loss: 88.06762326636137  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3756 training Loss: 368.9515585759567 validation Loss: 88.05197635187754  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3757 training Loss: 368.8822694721454 validation Loss: 88.03633636804243  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3758 training Loss: 368.8130117424373 validation Loss: 88.02070331003597  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3759 training Loss: 368.7437853646791 validation Loss: 88.0050771730426  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3760 training Loss: 368.6745903167388 validation Loss: 87.98945795225129  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3761 training Loss: 368.6054265765051 validation Loss: 87.9738456428556  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3762 training Loss: 368.5362941218882 validation Loss: 87.95824024005353  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3763 training Loss: 368.46719293081924 validation Loss: 87.94264173904767  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3764 training Loss: 368.39812298125014 validation Loss: 87.92705013504505  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3765 training Loss: 368.32908425115403 validation Loss: 87.91146542325728  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3766 training Loss: 368.26007671852494 validation Loss: 87.89588759890043  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3767 training Loss: 368.19110036137795 validation Loss: 87.88031665719507  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3768 training Loss: 368.12215515774875 validation Loss: 87.86475259336623  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3769 training Loss: 368.05324108569425 validation Loss: 87.8491954026435  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3770 training Loss: 367.98435812329205 validation Loss: 87.83364508026088  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3771 training Loss: 367.91550624864067 validation Loss: 87.81810162145689  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3772 training Loss: 367.84668543985947 validation Loss: 87.80256502147449  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3773 training Loss: 367.77789567508836 validation Loss: 87.7870352755611  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3774 training Loss: 367.70913693248826 validation Loss: 87.77151237896862  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3775 training Loss: 367.64040919024075 validation Loss: 87.75599632695341  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3776 training Loss: 367.5717124265482 validation Loss: 87.74048711477624  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3777 training Loss: 367.50304661963355 validation Loss: 87.72498473770236  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3778 training Loss: 367.43441174774046 validation Loss: 87.70948919100145  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3779 training Loss: 367.3658077891332 validation Loss: 87.6940004699476  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3780 training Loss: 367.2972347220968 validation Loss: 87.67851856981935  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3781 training Loss: 367.2286925249366 validation Loss: 87.66304348589966  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3782 training Loss: 367.1601811759789 validation Loss: 87.64757521347592  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3783 training Loss: 367.09170065357023 validation Loss: 87.63211374783988  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3784 training Loss: 367.02325093607766 validation Loss: 87.61665908428773  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3785 training Loss: 366.95483200188886 validation Loss: 87.60121121812011  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3786 training Loss: 366.8864438294121 validation Loss: 87.58577014464196  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3787 training Loss: 366.8180863970756 validation Loss: 87.57033585916265  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3788 training Loss: 366.74975968332853 validation Loss: 87.554908356996  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3789 training Loss: 366.6814636666404 validation Loss: 87.53948763346011  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3790 training Loss: 366.6131983255008 validation Loss: 87.52407368387749  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3791 training Loss: 366.54496363841974 validation Loss: 87.50866650357507  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3792 training Loss: 366.4767595839278 validation Loss: 87.49326608788408  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3793 training Loss: 366.40858614057555 validation Loss: 87.47787243214015  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3794 training Loss: 366.34044328693415 validation Loss: 87.46248553168326  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3795 training Loss: 366.27233100159475 validation Loss: 87.44710538185768  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3796 training Loss: 366.2042492631689 validation Loss: 87.43173197801214  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3797 training Loss: 366.13619805028816 validation Loss: 87.41636531549959  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3798 training Loss: 366.06817734160455 validation Loss: 87.40100538967741  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3799 training Loss: 366.00018711579 validation Loss: 87.38565219590726  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3800 training Loss: 365.9322273515367 validation Loss: 87.3703057295551  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3801 training Loss: 365.86429802755686 validation Loss: 87.35496598599127  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3802 training Loss: 365.79639912258284 validation Loss: 87.3396329605904  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3803 training Loss: 365.72853061536705 validation Loss: 87.32430664873141  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3804 training Loss: 365.66069248468193 validation Loss: 87.30898704579755  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3805 training Loss: 365.59288470931995 validation Loss: 87.29367414717635  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3806 training Loss: 365.52510726809356 validation Loss: 87.27836794825964  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3807 training Loss: 365.45736013983526 validation Loss: 87.26306844444353  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3808 training Loss: 365.38964330339707 validation Loss: 87.24777563112846  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3809 training Loss: 365.32195673765176 validation Loss: 87.23248950371911  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3810 training Loss: 365.25430042149117 validation Loss: 87.21721005762441  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3811 training Loss: 365.1866743338275 validation Loss: 87.20193728825761  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3812 training Loss: 365.11907845359246 validation Loss: 87.18667119103618  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3813 training Loss: 365.05151275973805 validation Loss: 87.1714117613819  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3814 training Loss: 364.9839772312356 validation Loss: 87.15615899472076  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3815 training Loss: 364.9164718470764 validation Loss: 87.14091288648301  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3816 training Loss: 364.8489965862717 validation Loss: 87.12567343210316  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3817 training Loss: 364.78155142785204 validation Loss: 87.11044062701991  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3818 training Loss: 364.7141363508681 validation Loss: 87.09521446667628  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3819 training Loss: 364.64675133438993 validation Loss: 87.07999494651945  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3820 training Loss: 364.5793963575075 validation Loss: 87.06478206200083  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3821 training Loss: 364.5120713993302 validation Loss: 87.04957580857608  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3822 training Loss: 364.44477643898716 validation Loss: 87.03437618170504  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3823 training Loss: 364.37751145562703 validation Loss: 87.0191831768518  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3824 training Loss: 364.3102764284181 validation Loss: 87.00399678948463  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3825 training Loss: 364.24307133654827 validation Loss: 86.98881701507601  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3826 training Loss: 364.17589615922486 validation Loss: 86.97364384910257  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3827 training Loss: 364.10875087567456 validation Loss: 86.95847728704521  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3828 training Loss: 364.0416354651438 validation Loss: 86.94331732438896  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3829 training Loss: 363.97454990689835 validation Loss: 86.92816395662304  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3830 training Loss: 363.9074941802234 validation Loss: 86.91301717924082  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3831 training Loss: 363.84046826442346 validation Loss: 86.89787698773992  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3832 training Loss: 363.77347213882274 validation Loss: 86.88274337762203  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3833 training Loss: 363.70650578276434 validation Loss: 86.86761634439308  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3834 training Loss: 363.6395691756111 validation Loss: 86.8524958835631  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3835 training Loss: 363.5726622967449 validation Loss: 86.83738199064628  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3836 training Loss: 363.5057851255673 validation Loss: 86.82227466116099  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3837 training Loss: 363.4389376414987 validation Loss: 86.80717389062971  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3838 training Loss: 363.372119823979 validation Loss: 86.7920796745791  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3839 training Loss: 363.30533165246726 validation Loss: 86.77699200853986  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3840 training Loss: 363.2385731064417 validation Loss: 86.76191088804691  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3841 training Loss: 363.1718441653998 validation Loss: 86.74683630863926  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3842 training Loss: 363.10514480885826 validation Loss: 86.73176826586004  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3843 training Loss: 363.0384750163526 validation Loss: 86.71670675525644  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3844 training Loss: 362.97183476743794 validation Loss: 86.70165177237988  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3845 training Loss: 362.9052240416881 validation Loss: 86.68660331278575  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3846 training Loss: 362.83864281869603 validation Loss: 86.67156137203364  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3847 training Loss: 362.772091078074 validation Loss: 86.65652594568712  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3848 training Loss: 362.70556879945286 validation Loss: 86.641497029314  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3849 training Loss: 362.6390759624829 validation Loss: 86.62647461848607  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3850 training Loss: 362.5726125468331 validation Loss: 86.61145870877922  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3851 training Loss: 362.5061785321916 validation Loss: 86.59644929577343  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3852 training Loss: 362.4397738982653 validation Loss: 86.5814463750527  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3853 training Loss: 362.3733986247801 validation Loss: 86.56644994220518  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3854 training Loss: 362.3070526914809 validation Loss: 86.551459992823  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3855 training Loss: 362.24073607813114 validation Loss: 86.53647652250243  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3856 training Loss: 362.1744487645135 validation Loss: 86.52149952684368  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3857 training Loss: 362.1081907304292 validation Loss: 86.50652900145111  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3858 training Loss: 362.0419619556984 validation Loss: 86.49156494193306  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3859 training Loss: 361.97576242016004 validation Loss: 86.47660734390195  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3860 training Loss: 361.9095921036718 validation Loss: 86.46165620297418  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3861 training Loss: 361.84345098611004 validation Loss: 86.44671151477023  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3862 training Loss: 361.7773390473699 validation Loss: 86.43177327491458  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3863 training Loss: 361.71125626736523 validation Loss: 86.41684147903571  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3864 training Loss: 361.64520262602855 validation Loss: 86.40191612276617  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3865 training Loss: 361.57917810331094 validation Loss: 86.38699720174245  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3866 training Loss: 361.51318267918225 validation Loss: 86.37208471160508  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3867 training Loss: 361.44721633363076 validation Loss: 86.35717864799861  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3868 training Loss: 361.3812790466636 validation Loss: 86.34227900657154  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3869 training Loss: 361.31537079830616 validation Loss: 86.3273857829764  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3870 training Loss: 361.2494915686026 validation Loss: 86.31249897286969  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3871 training Loss: 361.18364133761537 validation Loss: 86.29761857191188  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3872 training Loss: 361.1178200854258 validation Loss: 86.28274457576745  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3873 training Loss: 361.05202779213323 validation Loss: 86.2678769801048  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3874 training Loss: 360.9862644378559 validation Loss: 86.25301578059637  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3875 training Loss: 360.9205300027302 validation Loss: 86.2381609729185  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3876 training Loss: 360.8548244669108 validation Loss: 86.22331255275148  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3877 training Loss: 360.7891478105713 validation Loss: 86.20847051577964  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3878 training Loss: 360.72350001390316 validation Loss: 86.19363485769117  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3879 training Loss: 360.65788105711647 validation Loss: 86.17880557417824  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3880 training Loss: 360.5922909204394 validation Loss: 86.16398266093701  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3881 training Loss: 360.5267295841187 validation Loss: 86.14916611366746  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3882 training Loss: 360.4611970284192 validation Loss: 86.13435592807362  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3883 training Loss: 360.3956932336241 validation Loss: 86.11955209986337  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3884 training Loss: 360.3302181800349 validation Loss: 86.10475462474852  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3885 training Loss: 360.26477184797113 validation Loss: 86.08996349844486  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3886 training Loss: 360.1993542177706 validation Loss: 86.07517871667201  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3887 training Loss: 360.1339652697894 validation Loss: 86.06040027515357  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3888 training Loss: 360.0686049844017 validation Loss: 86.045628169617  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3889 training Loss: 360.00327334199983 validation Loss: 86.03086239579363  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3890 training Loss: 359.93797032299415 validation Loss: 86.01610294941881  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3891 training Loss: 359.8726959078132 validation Loss: 86.00134982623166  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3892 training Loss: 359.80745007690365 validation Loss: 85.98660302197521  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3893 training Loss: 359.7422328107301 validation Loss: 85.97186253239641  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3894 training Loss: 359.6770440897752 validation Loss: 85.95712835324608  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3895 training Loss: 359.61188389453974 validation Loss: 85.94240048027885  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3896 training Loss: 359.5467522055423 validation Loss: 85.9276789092533  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3897 training Loss: 359.4816490033196 validation Loss: 85.91296363593185  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3898 training Loss: 359.4165742684262 validation Loss: 85.89825465608072  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3899 training Loss: 359.35152798143463 validation Loss: 85.8835519654701  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3900 training Loss: 359.2865101229354 validation Loss: 85.86885555987399  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3901 training Loss: 359.2215206735368 validation Loss: 85.85416543507012  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3902 training Loss: 359.15655961386494 validation Loss: 85.83948158684022  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3903 training Loss: 359.091626924564 validation Loss: 85.82480401096981  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3904 training Loss: 359.02672258629576 validation Loss: 85.81013270324817  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3905 training Loss: 358.96184657973987 validation Loss: 85.79546765946853  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3906 training Loss: 358.8969988855938 validation Loss: 85.78080887542785  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3907 training Loss: 358.8321794845727 validation Loss: 85.76615634692696  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3908 training Loss: 358.7673883574097 validation Loss: 85.75151006977048  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3909 training Loss: 358.70262548485533 validation Loss: 85.73687003976684  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3910 training Loss: 358.637890847678 validation Loss: 85.7222362527283  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3911 training Loss: 358.5731844266637 validation Loss: 85.70760870447091  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3912 training Loss: 358.50850620261633 validation Loss: 85.69298739081452  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3913 training Loss: 358.4438561563571 validation Loss: 85.67837230758279  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3914 training Loss: 358.379234268725 validation Loss: 85.66376345060313  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3915 training Loss: 358.31464052057663 validation Loss: 85.64916081570676  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3916 training Loss: 358.2500748927862 validation Loss: 85.63456439872871  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3917 training Loss: 358.1855373662453 validation Loss: 85.61997419550772  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3918 training Loss: 358.1210279218633 validation Loss: 85.6053902018864  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3919 training Loss: 358.0565465405669 validation Loss: 85.59081241371098  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3920 training Loss: 357.99209320330044 validation Loss: 85.5762408268316  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3921 training Loss: 357.92766789102564 validation Loss: 85.56167543710211  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3922 training Loss: 357.8632705847217 validation Loss: 85.54711624038008  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3923 training Loss: 357.7989012653853 validation Loss: 85.53256323252688  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3924 training Loss: 357.73455991403046 validation Loss: 85.51801640940761  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3925 training Loss: 357.6702465116887 validation Loss: 85.50347576689111  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3926 training Loss: 357.60596103940884 validation Loss: 85.48894130084994  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3927 training Loss: 357.541703478257 validation Loss: 85.4744130071604  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3928 training Loss: 357.47747380931673 validation Loss: 85.45989088170253  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3929 training Loss: 357.4132720136889 validation Loss: 85.44537492036017  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3930 training Loss: 357.34909807249176 validation Loss: 85.43086511902072  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3931 training Loss: 357.28495196686066 validation Loss: 85.41636147357545  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3932 training Loss: 357.22083367794835 validation Loss: 85.40186397991926  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3933 training Loss: 357.1567431869247 validation Loss: 85.38737263395076  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3934 training Loss: 357.0926804749769 validation Loss: 85.3728874315723  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3935 training Loss: 357.02864552330914 validation Loss: 85.35840836868991  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3936 training Loss: 356.96463831314315 validation Loss: 85.3439354412133  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3937 training Loss: 356.90065882571747 validation Loss: 85.32946864505593  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3938 training Loss: 356.836707042288 validation Loss: 85.31500797613488  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3939 training Loss: 356.7727829441278 validation Loss: 85.30055343037094  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3940 training Loss: 356.7088865125266 validation Loss: 85.28610500368859  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3941 training Loss: 356.64501772879174 validation Loss: 85.27166269201598  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3942 training Loss: 356.58117657424737 validation Loss: 85.25722649128494  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3943 training Loss: 356.5173630302348 validation Loss: 85.24279639743095  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3944 training Loss: 356.4535770781121 validation Loss: 85.22837240639313  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3945 training Loss: 356.38981869925476 validation Loss: 85.21395451411433  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3946 training Loss: 356.32608787505484 validation Loss: 85.19954271654098  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3947 training Loss: 356.2623845869216 validation Loss: 85.18513700962322  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3948 training Loss: 356.19870881628117 validation Loss: 85.17073738931475  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3949 training Loss: 356.1350605445766 validation Loss: 85.15634385157304  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3950 training Loss: 356.0714397532679 validation Loss: 85.14195639235908  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3951 training Loss: 356.00784642383195 validation Loss: 85.12757500763755  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3952 training Loss: 355.94428053776244 validation Loss: 85.11319969337676  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3953 training Loss: 355.88074207656996 validation Loss: 85.09883044554863  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3954 training Loss: 355.81723102178194 validation Loss: 85.08446726012872  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3955 training Loss: 355.75374735494256 validation Loss: 85.07011013309618  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3956 training Loss: 355.69029105761285 validation Loss: 85.05575906043379  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3957 training Loss: 355.6268621113706 validation Loss: 85.04141403812797  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3958 training Loss: 355.5634604978103 validation Loss: 85.02707506216865  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3959 training Loss: 355.5000861985433 validation Loss: 85.01274212854948  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3960 training Loss: 355.43673919519733 validation Loss: 84.99841523326764  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3961 training Loss: 355.3734194694174 validation Loss: 84.98409437232388  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3962 training Loss: 355.31012700286465 validation Loss: 84.96977954172262  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3963 training Loss: 355.2468617772171 validation Loss: 84.95547073747179  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3964 training Loss: 355.1836237741694 validation Loss: 84.94116795558294  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3965 training Loss: 355.1204129754329 validation Loss: 84.92687119207118  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3966 training Loss: 355.0572293627353 validation Loss: 84.9125804429552  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3967 training Loss: 354.9940729178211 validation Loss: 84.89829570425725  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3968 training Loss: 354.9309436224514 validation Loss: 84.88401697200318  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3969 training Loss: 354.86784145840375 validation Loss: 84.86974424222234  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3970 training Loss: 354.804766407472 validation Loss: 84.85547751094768  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3971 training Loss: 354.74171845146685 validation Loss: 84.84121677421568  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3972 training Loss: 354.67869757221547 validation Loss: 84.8269620280664  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3973 training Loss: 354.6157037515611 validation Loss: 84.8127132685434  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3974 training Loss: 354.552736971364 validation Loss: 84.79847049169385  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3975 training Loss: 354.4897972135004 validation Loss: 84.78423369356835  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3976 training Loss: 354.42688445986323 validation Loss: 84.77000287022116  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3977 training Loss: 354.36399869236175 validation Loss: 84.75577801770996  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3978 training Loss: 354.30113989292136 validation Loss: 84.74155913209601  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3979 training Loss: 354.2383080434841 validation Loss: 84.72734620944408  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3980 training Loss: 354.17550312600827 validation Loss: 84.71313924582246  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3981 training Loss: 354.1127251224684 validation Loss: 84.69893823730295  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3982 training Loss: 354.0499740148554 validation Loss: 84.68474317996083  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3983 training Loss: 353.9872497851766 validation Loss: 84.67055406987498  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3984 training Loss: 353.9245524154552 validation Loss: 84.65637090312764  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3985 training Loss: 353.8618818877311 validation Loss: 84.64219367580465  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3986 training Loss: 353.79923818405996 validation Loss: 84.62802238399532  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3987 training Loss: 353.7366212865141 validation Loss: 84.61385702379243  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3988 training Loss: 353.6740311771818 validation Loss: 84.59969759129226  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3989 training Loss: 353.6114678381675 validation Loss: 84.5855440825946  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3990 training Loss: 353.54893125159185 validation Loss: 84.57139649380264  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3991 training Loss: 353.4864213995916 validation Loss: 84.55725482102314  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3992 training Loss: 353.4239382643197 validation Loss: 84.54311906036622  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3993 training Loss: 353.361481827945 validation Loss: 84.5289892079456  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3994 training Loss: 353.2990520726527 validation Loss: 84.51486525987835  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3995 training Loss: 353.23664898064385 validation Loss: 84.500747212285  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3996 training Loss: 353.1742725341355 validation Loss: 84.48663506128966  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3997 training Loss: 353.11192271536106 validation Loss: 84.47252880301971  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3998 training Loss: 353.0495995065696 validation Loss: 84.45842843360614  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 3999 training Loss: 352.9873028900264 validation Loss: 84.44433394918329  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4000 training Loss: 352.92503284801256 validation Loss: 84.43024534588892  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4001 training Loss: 352.8627893628251 validation Loss: 84.41616261986431  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4002 training Loss: 352.80057241677713 validation Loss: 84.4020857672541  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4003 training Loss: 352.7383819921977 validation Loss: 84.38801478420642  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4004 training Loss: 352.67621807143155 validation Loss: 84.37394966687275  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4005 training Loss: 352.6140806368394 validation Loss: 84.359890411408  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4006 training Loss: 352.55196967079803 validation Loss: 84.34583701397061  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4007 training Loss: 352.48988515569965 validation Loss: 84.33178947072224  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4008 training Loss: 352.4278270739527 validation Loss: 84.31774777782815  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4009 training Loss: 352.3657954079812 validation Loss: 84.30371193145685  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4010 training Loss: 352.30379014022503 validation Loss: 84.28968192778036  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4011 training Loss: 352.24181125313993 validation Loss: 84.27565776297405  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4012 training Loss: 352.1798587291971 validation Loss: 84.26163943321664  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4013 training Loss: 352.1179325508839 validation Loss: 84.24762693469035  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4014 training Loss: 352.0560327007031 validation Loss: 84.23362026358069  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4015 training Loss: 351.99415916117323 validation Loss: 84.21961941607657  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4016 training Loss: 351.9323119148286 validation Loss: 84.2056243883703  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4017 training Loss: 351.870490944219 validation Loss: 84.19163517665756  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4018 training Loss: 351.8086962319102 validation Loss: 84.17765177713738  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4019 training Loss: 351.7469277604832 validation Loss: 84.16367418601222  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4020 training Loss: 351.6851855125349 validation Loss: 84.14970239948778  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4021 training Loss: 351.6234694706777 validation Loss: 84.13573641377326  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4022 training Loss: 351.56177961753974 validation Loss: 84.1217762250811  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4023 training Loss: 351.5001159357643 validation Loss: 84.10782182962716  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4024 training Loss: 351.4384784080106 validation Loss: 84.09387322363062  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4025 training Loss: 351.3768670169532 validation Loss: 84.07993040331402  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4026 training Loss: 351.3152817452824 validation Loss: 84.06599336490324  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4027 training Loss: 351.25372257570353 validation Loss: 84.05206210462747  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4028 training Loss: 351.19218949093795 validation Loss: 84.03813661871925  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4029 training Loss: 351.1306824737221 validation Loss: 84.0242169034145  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4030 training Loss: 351.06920150680804 validation Loss: 84.01030295495235  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4031 training Loss: 351.00774657296313 validation Loss: 83.99639476957535  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4032 training Loss: 350.9463176549701 validation Loss: 83.98249234352934  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4033 training Loss: 350.88491473562726 validation Loss: 83.96859567306345  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4034 training Loss: 350.8235377977482 validation Loss: 83.9547047544302  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4035 training Loss: 350.7621868241618 validation Loss: 83.94081958388529  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4036 training Loss: 350.7008617977123 validation Loss: 83.92694015768782  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4037 training Loss: 350.6395627012594 validation Loss: 83.91306647210016  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4038 training Loss: 350.5782895176779 validation Loss: 83.89919852338798  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4039 training Loss: 350.5170422298579 validation Loss: 83.88533630782027  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4040 training Loss: 350.45582082070496 validation Loss: 83.87147982166923  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4041 training Loss: 350.39462527313964 validation Loss: 83.85762906121042  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4042 training Loss: 350.33345557009795 validation Loss: 83.84378402272266  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4043 training Loss: 350.27231169453114 validation Loss: 83.82994470248804  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4044 training Loss: 350.2111936294053 validation Loss: 83.81611109679194  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4045 training Loss: 350.15010135770206 validation Loss: 83.80228320192298  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4046 training Loss: 350.0890348624181 validation Loss: 83.78846101417308  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4047 training Loss: 350.0279941265652 validation Loss: 83.77464452983739  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4048 training Loss: 349.96697913317047 validation Loss: 83.76083374521438  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4049 training Loss: 349.9059898652758 validation Loss: 83.7470286566057  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4050 training Loss: 349.8450263059385 validation Loss: 83.73322926031632  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4051 training Loss: 349.7840884382307 validation Loss: 83.7194355526544  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4052 training Loss: 349.7231762452398 validation Loss: 83.70564752993137  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4053 training Loss: 349.6622897100681 validation Loss: 83.69186518846192  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4054 training Loss: 349.60142881583306 validation Loss: 83.67808852456398  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4055 training Loss: 349.5405935456671 validation Loss: 83.66431753455868  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4056 training Loss: 349.47978388271747 validation Loss: 83.65055221477039  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4057 training Loss: 349.4189998101467 validation Loss: 83.6367925615267  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4058 training Loss: 349.35824131113213 validation Loss: 83.6230385711585  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4059 training Loss: 349.2975083688659 validation Loss: 83.60929023999981  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4060 training Loss: 349.2368009665554 validation Loss: 83.59554756438787  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4061 training Loss: 349.17611908742265 validation Loss: 83.5818105406632  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4062 training Loss: 349.11546271470473 validation Loss: 83.56807916516945  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4063 training Loss: 349.05483183165353 validation Loss: 83.55435343425356  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4064 training Loss: 348.9942264215358 validation Loss: 83.54063334426563  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4065 training Loss: 348.93364646763314 validation Loss: 83.52691889155892  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4066 training Loss: 348.873091953242 validation Loss: 83.51321007248993  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4067 training Loss: 348.81256286167365 validation Loss: 83.49950688341838  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4068 training Loss: 348.75205917625425 validation Loss: 83.48580932070712  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4069 training Loss: 348.69158088032447 validation Loss: 83.47211738072222  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4070 training Loss: 348.63112795724004 validation Loss: 83.45843105983292  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4071 training Loss: 348.57070039037114 validation Loss: 83.44475035441164  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4072 training Loss: 348.510298163103 validation Loss: 83.43107526083395  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4073 training Loss: 348.44992125883545 validation Loss: 83.41740577547864  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4074 training Loss: 348.38956966098283 validation Loss: 83.40374189472766  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4075 training Loss: 348.3292433529744 validation Loss: 83.39008361496607  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4076 training Loss: 348.26894231825395 validation Loss: 83.37643093258211  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4077 training Loss: 348.2086665402799 validation Loss: 83.36278384396726  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4078 training Loss: 348.14841600252555 validation Loss: 83.34914234551606  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4079 training Loss: 348.08819068847856 validation Loss: 83.33550643362621  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4080 training Loss: 348.02799058164123 validation Loss: 83.32187610469856  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4081 training Loss: 347.96781566553045 validation Loss: 83.30825135513719  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4082 training Loss: 347.9076659236779 validation Loss: 83.29463218134917  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4083 training Loss: 347.8475413396294 validation Loss: 83.28101857974484  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4084 training Loss: 347.78744189694555 validation Loss: 83.26741054673757  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4085 training Loss: 347.72736757920154 validation Loss: 83.25380807874393  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4086 training Loss: 347.667318369987 validation Loss: 83.24021117218362  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4087 training Loss: 347.60729425290594 validation Loss: 83.22661982347937  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4088 training Loss: 347.54729521157685 validation Loss: 83.21303402905716  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4089 training Loss: 347.4873212296329 validation Loss: 83.19945378534595  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4090 training Loss: 347.42737229072145 validation Loss: 83.18587908877794  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4091 training Loss: 347.3674483785044 validation Loss: 83.17230993578838  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4092 training Loss: 347.3075494766581 validation Loss: 83.1587463228156  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4093 training Loss: 347.247675568873 validation Loss: 83.14518824630107  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4094 training Loss: 347.18782663885446 validation Loss: 83.13163570268934  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4095 training Loss: 347.1280026703216 validation Loss: 83.11808868842807  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4096 training Loss: 347.06820364700843 validation Loss: 83.10454719996801  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4097 training Loss: 347.00842955266285 validation Loss: 83.09101123376298  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4098 training Loss: 346.9486803710474 validation Loss: 83.0774807862699  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4099 training Loss: 346.88895608593856 validation Loss: 83.06395585394878  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4100 training Loss: 346.8292566811275 validation Loss: 83.05043643326272  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4101 training Loss: 346.76958214041946 validation Loss: 83.03692252067782  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4102 training Loss: 346.7099324476337 validation Loss: 83.02341411266333  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4103 training Loss: 346.6503075866042 validation Loss: 83.00991120569157  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4104 training Loss: 346.5907075411788 validation Loss: 82.99641379623785  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4105 training Loss: 346.53113229521955 validation Loss: 82.98292188078061  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4106 training Loss: 346.47158183260285 validation Loss: 82.96943545580135  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4107 training Loss: 346.41205613721917 validation Loss: 82.95595451778456  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4108 training Loss: 346.3525551929732 validation Loss: 82.94247906321786  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4109 training Loss: 346.29307898378363 validation Loss: 82.9290090885919  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4110 training Loss: 346.2336274935835 validation Loss: 82.91554459040032  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4111 training Loss: 346.17420070631977 validation Loss: 82.90208556513983  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4112 training Loss: 346.1147986059535 validation Loss: 82.88863200931023  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4113 training Loss: 346.05542117645996 validation Loss: 82.87518391941427  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4114 training Loss: 345.9960684018282 validation Loss: 82.86174129195783  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4115 training Loss: 345.9367402660618 validation Loss: 82.84830412344971  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4116 training Loss: 345.87743675317796 validation Loss: 82.8348724104018  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4117 training Loss: 345.818157847208 validation Loss: 82.821446149329  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4118 training Loss: 345.7589035321971 validation Loss: 82.80802533674925  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4119 training Loss: 345.69967379220486 validation Loss: 82.79460996918348  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4120 training Loss: 345.6404686113044 validation Loss: 82.7812000431556  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4121 training Loss: 345.5812879735831 validation Loss: 82.76779555519258  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4122 training Loss: 345.522131863142 validation Loss: 82.75439650182439  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4123 training Loss: 345.46300026409625 validation Loss: 82.74100287958399  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4124 training Loss: 345.4038931605748 validation Loss: 82.72761468500732  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4125 training Loss: 345.3448105367207 validation Loss: 82.71423191463336  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4126 training Loss: 345.28575237669054 validation Loss: 82.70085456500406  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4127 training Loss: 345.2267186646551 validation Loss: 82.68748263266434  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4128 training Loss: 345.16770938479885 validation Loss: 82.67411611416213  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4129 training Loss: 345.10872452132 validation Loss: 82.66075500604833  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4130 training Loss: 345.0497640584309 validation Loss: 82.64739930487684  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4131 training Loss: 344.9908279803573 validation Loss: 82.63404900720448  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4132 training Loss: 344.931916271339 validation Loss: 82.62070410959113  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4133 training Loss: 344.87302891562933 validation Loss: 82.60736460859955  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4134 training Loss: 344.8141658974957 validation Loss: 82.59403050079553  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4135 training Loss: 344.755327201219 validation Loss: 82.58070178274781  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4136 training Loss: 344.69651281109407 validation Loss: 82.56737845102808  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4137 training Loss: 344.63772271142915 validation Loss: 82.55406050221097  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4138 training Loss: 344.5789568865465 validation Loss: 82.5407479328741  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4139 training Loss: 344.5202153207818 validation Loss: 82.527440739598  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4140 training Loss: 344.4614979984846 validation Loss: 82.51413891896621  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4141 training Loss: 344.402804904018 validation Loss: 82.50084246756515  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4142 training Loss: 344.34413602175874 validation Loss: 82.48755138198419  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4143 training Loss: 344.2854913360971 validation Loss: 82.47426565881565  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4144 training Loss: 344.22687083143705 validation Loss: 82.46098529465482  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4145 training Loss: 344.1682744921963 validation Loss: 82.44771028609985  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4146 training Loss: 344.1097023028058 validation Loss: 82.43444062975189  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4147 training Loss: 344.0511542477104 validation Loss: 82.42117632221495  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4148 training Loss: 343.99263031136815 validation Loss: 82.40791736009601  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4149 training Loss: 343.934130478251 validation Loss: 82.39466374000492  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4150 training Loss: 343.87565473284417 validation Loss: 82.38141545855451  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4151 training Loss: 343.81720305964643 validation Loss: 82.36817251236047  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4152 training Loss: 343.75877544317 validation Loss: 82.35493489804142  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4153 training Loss: 343.7003718679408 validation Loss: 82.34170261221888  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4154 training Loss: 343.64199231849784 validation Loss: 82.32847565151727  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4155 training Loss: 343.5836367793938 validation Loss: 82.31525401256393  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4156 training Loss: 343.5253052351949 validation Loss: 82.30203769198907  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4157 training Loss: 343.46699767048045 validation Loss: 82.28882668642578  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4158 training Loss: 343.4087140698435 validation Loss: 82.27562099251011  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4159 training Loss: 343.35045441789015 validation Loss: 82.26242060688091  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4160 training Loss: 343.29221869924004 validation Loss: 82.24922552618  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4161 training Loss: 343.2340068985263 validation Loss: 82.23603574705197  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4162 training Loss: 343.17581900039517 validation Loss: 82.22285126614443  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4163 training Loss: 343.11765498950626 validation Loss: 82.20967208010775  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4164 training Loss: 343.05951485053265 validation Loss: 82.1964981855952  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4165 training Loss: 343.00139856816054 validation Loss: 82.18332957926293  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4166 training Loss: 342.9433061270895 validation Loss: 82.17016625776998  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4167 training Loss: 342.8852375120323 validation Loss: 82.15700821777821  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4168 training Loss: 342.8271927077151 validation Loss: 82.14385545595235  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4169 training Loss: 342.7691716988771 validation Loss: 82.13070796896  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4170 training Loss: 342.71117447027086 validation Loss: 82.11756575347158  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4171 training Loss: 342.6532010066621 validation Loss: 82.10442880616043  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4172 training Loss: 342.5952512928297 validation Loss: 82.09129712370263  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4173 training Loss: 342.53732531356593 validation Loss: 82.07817070277721  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4174 training Loss: 342.479423053676 validation Loss: 82.06504954006601  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4175 training Loss: 342.4215444979783 validation Loss: 82.05193363225362  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4176 training Loss: 342.3636896313044 validation Loss: 82.03882297602762  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4177 training Loss: 342.30585843849906 validation Loss: 82.02571756807829  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4178 training Loss: 342.24805090442004 validation Loss: 82.0126174050988  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4179 training Loss: 342.1902670139382 validation Loss: 81.99952248378514  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4180 training Loss: 342.1325067519376 validation Loss: 81.98643280083607  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4181 training Loss: 342.07477010331525 validation Loss: 81.9733483529533  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4182 training Loss: 342.01705705298116 validation Loss: 81.96026913684119  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4183 training Loss: 341.9593675858587 validation Loss: 81.94719514920703  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4184 training Loss: 341.9017016868837 validation Loss: 81.9341263867609  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4185 training Loss: 341.8440593410056 validation Loss: 81.92106284621565  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4186 training Loss: 341.7864405331866 validation Loss: 81.90800452428695  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4187 training Loss: 341.72884524840157 validation Loss: 81.89495141769329  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4188 training Loss: 341.67127347163887 validation Loss: 81.88190352315596  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4189 training Loss: 341.61372518789943 validation Loss: 81.86886083739898  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4190 training Loss: 341.55620038219735 validation Loss: 81.85582335714928  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4191 training Loss: 341.4986990395595 validation Loss: 81.84279107913648  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4192 training Loss: 341.4412211450258 validation Loss: 81.82976400009301  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4193 training Loss: 341.3837666836489 validation Loss: 81.81674211675409  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4194 training Loss: 341.32633564049456 validation Loss: 81.80372542585775  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4195 training Loss: 341.2689280006412 validation Loss: 81.79071392414475  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4196 training Loss: 341.2115437491801 validation Loss: 81.77770760835865  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4197 training Loss: 341.15418287121565 validation Loss: 81.76470647524577  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4198 training Loss: 341.09684535186466 validation Loss: 81.75171052155521  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4199 training Loss: 341.0395311762571 validation Loss: 81.73871974403883  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4200 training Loss: 340.98224032953567 validation Loss: 81.72573413945125  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4201 training Loss: 340.92497279685574 validation Loss: 81.71275370454984  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4202 training Loss: 340.8677285633854 validation Loss: 81.69977843609477  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4203 training Loss: 340.81050761430566 validation Loss: 81.68680833084892  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4204 training Loss: 340.7533099348103 validation Loss: 81.6738433855779  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4205 training Loss: 340.6961355101057 validation Loss: 81.66088359705013  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4206 training Loss: 340.638984325411 validation Loss: 81.64792896203674  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4207 training Loss: 340.58185636595806 validation Loss: 81.6349794773116  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4208 training Loss: 340.52475161699147 validation Loss: 81.62203513965133  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4209 training Loss: 340.4676700637684 validation Loss: 81.60909594583529  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4210 training Loss: 340.41061169155876 validation Loss: 81.59616189264557  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4211 training Loss: 340.3535764856451 validation Loss: 81.58323297686695  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4212 training Loss: 340.2965644313225 validation Loss: 81.570309195287  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4213 training Loss: 340.2395755138988 validation Loss: 81.55739054469598  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4214 training Loss: 340.18260971869444 validation Loss: 81.54447702188688  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4215 training Loss: 340.1256670310424 validation Loss: 81.5315686236554  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4216 training Loss: 340.0687474362882 validation Loss: 81.51866534679999  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4217 training Loss: 340.01185091979005 validation Loss: 81.50576718812174  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4218 training Loss: 339.95497746691854 validation Loss: 81.49287414442452  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4219 training Loss: 339.898127063057 validation Loss: 81.47998621251489  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4220 training Loss: 339.8412996936013 validation Loss: 81.46710338920207  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4221 training Loss: 339.7844953439594 validation Loss: 81.45422567129808  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4222 training Loss: 339.7277139995524 validation Loss: 81.4413530556175  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4223 training Loss: 339.6709556458133 validation Loss: 81.42848553897772  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4224 training Loss: 339.6142202681881 validation Loss: 81.41562311819881  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4225 training Loss: 339.5575078521348 validation Loss: 81.40276579010346  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4226 training Loss: 339.50081838312406 validation Loss: 81.38991355151711  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4227 training Loss: 339.444151846639 validation Loss: 81.37706639926785  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4228 training Loss: 339.38750822817514 validation Loss: 81.36422433018647  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4229 training Loss: 339.3308875132403 validation Loss: 81.35138734110643  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4230 training Loss: 339.2742896873548 validation Loss: 81.33855542886388  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4231 training Loss: 339.2177147360513 validation Loss: 81.32572859029763  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4232 training Loss: 339.1611626448749 validation Loss: 81.31290682224915  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4233 training Loss: 339.10463339938286 validation Loss: 81.30009012156258  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4234 training Loss: 339.048126985145 validation Loss: 81.28727848508476  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4235 training Loss: 338.99164338774347 validation Loss: 81.27447190966517  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4236 training Loss: 338.93518259277255 validation Loss: 81.26167039215589  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4237 training Loss: 338.87874458583883 validation Loss: 81.24887392941176  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4238 training Loss: 338.8223293525614 validation Loss: 81.23608251829019  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4239 training Loss: 338.7659368785713 validation Loss: 81.22329615565127  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4240 training Loss: 338.7095671495123 validation Loss: 81.21051483835775  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4241 training Loss: 338.6532201510399 validation Loss: 81.197738563275  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4242 training Loss: 338.59689586882206 validation Loss: 81.18496732727105  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4243 training Loss: 338.5405942885392 validation Loss: 81.17220112721655  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4244 training Loss: 338.48431539588347 validation Loss: 81.15943995998481  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4245 training Loss: 338.42805917655954 validation Loss: 81.14668382245176  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4246 training Loss: 338.3718256162843 validation Loss: 81.13393271149596  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4247 training Loss: 338.31561470078657 validation Loss: 81.12118662399863  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4248 training Loss: 338.2594264158075 validation Loss: 81.1084455568435  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4249 training Loss: 338.20326074710044 validation Loss: 81.09570950691707  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4250 training Loss: 338.1471176804305 validation Loss: 81.0829784711084  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4251 training Loss: 338.0909972015754 validation Loss: 81.07025244630913  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4252 training Loss: 338.0348992963246 validation Loss: 81.05753142941354  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4253 training Loss: 337.97882395047986 validation Loss: 81.04481541731856  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4254 training Loss: 337.92277114985495 validation Loss: 81.03210440692365  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4255 training Loss: 337.86674088027564 validation Loss: 81.01939839513096  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4256 training Loss: 337.8107331275798 validation Loss: 81.00669737884517  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4257 training Loss: 337.75474787761743 validation Loss: 80.99400135497362  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4258 training Loss: 337.6987851162503 validation Loss: 80.98131032042619  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4259 training Loss: 337.6428448293525 validation Loss: 80.96862427211539  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4260 training Loss: 337.58692700281006 validation Loss: 80.95594320695633  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4261 training Loss: 337.5310316225208 validation Loss: 80.94326712186668  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4262 training Loss: 337.4751586743947 validation Loss: 80.93059601376675  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4263 training Loss: 337.41930814435364 validation Loss: 80.91792987957932  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4264 training Loss: 337.3634800183315 validation Loss: 80.90526871622986  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4265 training Loss: 337.30767428227387 validation Loss: 80.8926125206464  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4266 training Loss: 337.2518909221386 validation Loss: 80.8799612897595  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4267 training Loss: 337.1961299238953 validation Loss: 80.86731502050233  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4268 training Loss: 337.14039127352544 validation Loss: 80.85467370981063  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4269 training Loss: 337.08467495702234 validation Loss: 80.84203735462268  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4270 training Loss: 337.0289809603913 validation Loss: 80.82940595187931  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4271 training Loss: 336.9733092696496 validation Loss: 80.81677949852401  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4272 training Loss: 336.9176598708259 validation Loss: 80.80415799150272  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4273 training Loss: 336.8620327499613 validation Loss: 80.79154142776399  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4274 training Loss: 336.80642789310826 validation Loss: 80.77892980425891  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4275 training Loss: 336.7508452863313 validation Loss: 80.76632311794111  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4276 training Loss: 336.6952849157066 validation Loss: 80.7537213657668  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4277 training Loss: 336.63974676732226 validation Loss: 80.7411245446947  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4278 training Loss: 336.584230827278 validation Loss: 80.72853265168612  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4279 training Loss: 336.52873708168545 validation Loss: 80.71594568370483  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4280 training Loss: 336.47326551666777 validation Loss: 80.70336363771722  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4281 training Loss: 336.41781611836007 validation Loss: 80.69078651069215  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4282 training Loss: 336.36238887290915 validation Loss: 80.67821429960108  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4283 training Loss: 336.30698376647354 validation Loss: 80.66564700141797  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4284 training Loss: 336.2516007852232 validation Loss: 80.65308461311923  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4285 training Loss: 336.19623991534024 validation Loss: 80.64052713168397  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4286 training Loss: 336.140901143018 validation Loss: 80.62797455409364  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4287 training Loss: 336.08558445446175 validation Loss: 80.61542687733231  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4288 training Loss: 336.0302898358882 validation Loss: 80.60288409838654  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4289 training Loss: 335.975017273526 validation Loss: 80.59034621424539  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4290 training Loss: 335.91976675361514 validation Loss: 80.57781322190047  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4291 training Loss: 335.86453826240745 validation Loss: 80.56528511834586  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4292 training Loss: 335.809331786166 validation Loss: 80.55276190057813  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4293 training Loss: 335.7541473111658 validation Loss: 80.54024356559644  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4294 training Loss: 335.69898482369337 validation Loss: 80.52773011040239  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4295 training Loss: 335.6438443100467 validation Loss: 80.51522153200003  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4296 training Loss: 335.5887257565354 validation Loss: 80.50271782739598  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4297 training Loss: 335.5336291494805 validation Loss: 80.49021899359936  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4298 training Loss: 335.47855447521465 validation Loss: 80.47772502762172  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4299 training Loss: 335.4235017200821 validation Loss: 80.4652359264771  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4300 training Loss: 335.3684708704383 validation Loss: 80.45275168718209  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4301 training Loss: 335.3134619126505 validation Loss: 80.44027230675573  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4302 training Loss: 335.25847483309747 validation Loss: 80.4277977822195  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4303 training Loss: 335.2035096181691 validation Loss: 80.41532811059741  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4304 training Loss: 335.148566254267 validation Loss: 80.40286328891591  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4305 training Loss: 335.0936447278041 validation Loss: 80.39040331420397  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4306 training Loss: 335.03874502520483 validation Loss: 80.37794818349298  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4307 training Loss: 334.98386713290495 validation Loss: 80.36549789381677  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4308 training Loss: 334.92901103735187 validation Loss: 80.35305244221172  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4309 training Loss: 334.87417672500396 validation Loss: 80.34061182571662  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4310 training Loss: 334.81936418233136 validation Loss: 80.32817604137274  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4311 training Loss: 334.76457339581543 validation Loss: 80.31574508622376  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4312 training Loss: 334.70980435194883 validation Loss: 80.30331895731587  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4313 training Loss: 334.65505703723557 validation Loss: 80.29089765169769  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4314 training Loss: 334.6003314381912 validation Loss: 80.27848116642025  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4315 training Loss: 334.5456275413423 validation Loss: 80.26606949853709  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4316 training Loss: 334.49094533322693 validation Loss: 80.25366264510419  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4317 training Loss: 334.43628480039445 validation Loss: 80.2412606031799  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4318 training Loss: 334.3816459294054 validation Loss: 80.22886336982506  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4319 training Loss: 334.32702870683164 validation Loss: 80.21647094210299  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4320 training Loss: 334.27243311925633 validation Loss: 80.20408331707934  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4321 training Loss: 334.21785915327393 validation Loss: 80.19170049182226  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4322 training Loss: 334.16330679548986 validation Loss: 80.17932246340233  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4323 training Loss: 334.10877603252106 validation Loss: 80.16694922889252  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4324 training Loss: 334.0542668509957 validation Loss: 80.15458078536827  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4325 training Loss: 333.99977923755296 validation Loss: 80.14221712990738  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4326 training Loss: 333.94531317884326 validation Loss: 80.12985825959011  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4327 training Loss: 333.8908686615282 validation Loss: 80.11750417149915  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4328 training Loss: 333.83644567228066 validation Loss: 80.10515486271956  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4329 training Loss: 333.78204419778444 validation Loss: 80.0928103303388  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4330 training Loss: 333.7276642247348 validation Loss: 80.08047057144684  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4331 training Loss: 333.67330573983776 validation Loss: 80.06813558313593  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4332 training Loss: 333.61896872981083 validation Loss: 80.05580536250079  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4333 training Loss: 333.5646531813823 validation Loss: 80.04347990663852  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4334 training Loss: 333.5103590812919 validation Loss: 80.03115921264866  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4335 training Loss: 333.45608641629 validation Loss: 80.01884327763307  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4336 training Loss: 333.40183517313847 validation Loss: 80.00653209869606  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4337 training Loss: 333.3476053386101 validation Loss: 79.99422567294434  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4338 training Loss: 333.2933968994886 validation Loss: 79.98192399748694  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4339 training Loss: 333.23920984256875 validation Loss: 79.96962706943535  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4340 training Loss: 333.18504415465657 validation Loss: 79.95733488590339  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4341 training Loss: 333.1308998225688 validation Loss: 79.94504744400732  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4342 training Loss: 333.0767768331334 validation Loss: 79.93276474086568  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4343 training Loss: 333.0226751731892 validation Loss: 79.92048677359949  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4344 training Loss: 332.9685948295861 validation Loss: 79.90821353933212  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4345 training Loss: 332.9145357891849 validation Loss: 79.89594503518926  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4346 training Loss: 332.8604980388575 validation Loss: 79.88368125829899  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4347 training Loss: 332.80648156548637 validation Loss: 79.87142220579177  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4348 training Loss: 332.7524863559653 validation Loss: 79.85916787480045  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4349 training Loss: 332.6985123971989 validation Loss: 79.84691826246018  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4350 training Loss: 332.64455967610263 validation Loss: 79.8346733659085  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4351 training Loss: 332.59062817960285 validation Loss: 79.8224331822853  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4352 training Loss: 332.53671789463687 validation Loss: 79.81019770873286  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4353 training Loss: 332.4828288081529 validation Loss: 79.79796694239575  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4354 training Loss: 332.42896090710985 validation Loss: 79.78574088042092  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4355 training Loss: 332.3751141784776 validation Loss: 79.77351951995769  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4356 training Loss: 332.32128860923694 validation Loss: 79.76130285815766  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4357 training Loss: 332.26748418637936 validation Loss: 79.74909089217482  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4358 training Loss: 332.2137008969073 validation Loss: 79.73688361916552  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4359 training Loss: 332.15993872783383 validation Loss: 79.72468103628839  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4360 training Loss: 332.106197666183 validation Loss: 79.71248314070444  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4361 training Loss: 332.0524776989895 validation Loss: 79.70028992957695  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4362 training Loss: 331.99877881329894 validation Loss: 79.68810140007159  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4363 training Loss: 331.9451009961675 validation Loss: 79.67591754935637  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4364 training Loss: 331.89144423466234 validation Loss: 79.66373837460154  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4365 training Loss: 331.83780851586124 validation Loss: 79.65156387297978  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4366 training Loss: 331.78419382685263 validation Loss: 79.63939404166598  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4367 training Loss: 331.7306001547357 validation Loss: 79.62722887783742  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4368 training Loss: 331.67702748662066 validation Loss: 79.61506837867373  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4369 training Loss: 331.62347580962796 validation Loss: 79.60291254135672  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4370 training Loss: 331.56994511088885 validation Loss: 79.59076136307065  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4371 training Loss: 331.51643537754546 validation Loss: 79.578614841002  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4372 training Loss: 331.46294659675044 validation Loss: 79.56647297233958  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4373 training Loss: 331.409478755667 validation Loss: 79.55433575427452  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4374 training Loss: 331.3560318414692 validation Loss: 79.54220318400023  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4375 training Loss: 331.3026058413416 validation Loss: 79.53007525871243  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4376 training Loss: 331.2492007424794 validation Loss: 79.51795197560912  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4377 training Loss: 331.1958165320883 validation Loss: 79.50583333189061  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4378 training Loss: 331.1424531973849 validation Loss: 79.49371932475951  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4379 training Loss: 331.08911072559596 validation Loss: 79.4816099514207  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4380 training Loss: 331.03578910395925 validation Loss: 79.46950520908133  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4381 training Loss: 330.9824883197228 validation Loss: 79.4574050949509  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4382 training Loss: 330.92920836014525 validation Loss: 79.4453096062411  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4383 training Loss: 330.8759492124959 validation Loss: 79.43321874016598  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4384 training Loss: 330.8227108640545 validation Loss: 79.42113249394183  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4385 training Loss: 330.76949330211124 validation Loss: 79.40905086478719  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4386 training Loss: 330.71629651396694 validation Loss: 79.39697384992294  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4387 training Loss: 330.6631204869329 validation Loss: 79.38490144657217  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4388 training Loss: 330.60996520833083 validation Loss: 79.37283365196025  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4389 training Loss: 330.556830665493 validation Loss: 79.36077046331485  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4390 training Loss: 330.50371684576214 validation Loss: 79.34871187786587  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4391 training Loss: 330.45062373649125 validation Loss: 79.33665789284548  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4392 training Loss: 330.3975513250442 validation Loss: 79.3246085054881  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4393 training Loss: 330.34449959879476 validation Loss: 79.31256371303043  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4394 training Loss: 330.2914685451275 validation Loss: 79.30052351271138  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4395 training Loss: 330.23845815143727 validation Loss: 79.28848790177216  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4396 training Loss: 330.1854684051293 validation Loss: 79.27645687745623  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4397 training Loss: 330.1324992936194 validation Loss: 79.26443043700922  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4398 training Loss: 330.0795508043334 validation Loss: 79.25240857767909  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4399 training Loss: 330.0266229247077 validation Loss: 79.240391296716  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4400 training Loss: 329.9737156421892 validation Loss: 79.22837859137238  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4401 training Loss: 329.9208289442349 validation Loss: 79.21637045890287  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4402 training Loss: 329.8679628183122 validation Loss: 79.20436689656434  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4403 training Loss: 329.815117251899 validation Loss: 79.19236790161594  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4404 training Loss: 329.7622922324832 validation Loss: 79.18037347131897  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4405 training Loss: 329.70948774756334 validation Loss: 79.16838360293707  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4406 training Loss: 329.6567037846479 validation Loss: 79.156398293736  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4407 training Loss: 329.603940331256 validation Loss: 79.1444175409838  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4408 training Loss: 329.5511973749168 validation Loss: 79.13244134195071  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4409 training Loss: 329.4984749031698 validation Loss: 79.1204696939092  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4410 training Loss: 329.4457729035646 validation Loss: 79.10850259413397  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4411 training Loss: 329.39309136366137 validation Loss: 79.09654003990192  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4412 training Loss: 329.3404302710302 validation Loss: 79.08458202849215  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4413 training Loss: 329.28778961325156 validation Loss: 79.072628557186  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4414 training Loss: 329.23516937791595 validation Loss: 79.06067962326696  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4415 training Loss: 329.18256955262433 validation Loss: 79.04873522402082  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4416 training Loss: 329.12999012498767 validation Loss: 79.0367953567355  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4417 training Loss: 329.07743108262713 validation Loss: 79.02486001870112  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4418 training Loss: 329.0248924131741 validation Loss: 79.01292920721005  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4419 training Loss: 328.9723741042701 validation Loss: 79.00100291955681  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4420 training Loss: 328.9198761435667 validation Loss: 78.98908115303817  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4421 training Loss: 328.86739851872574 validation Loss: 78.97716390495302  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4422 training Loss: 328.8149412174191 validation Loss: 78.96525117260246  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4423 training Loss: 328.76250422732886 validation Loss: 78.95334295328982  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4424 training Loss: 328.7100875361471 validation Loss: 78.94143924432058  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4425 training Loss: 328.65769113157603 validation Loss: 78.92954004300243  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4426 training Loss: 328.60531500132794 validation Loss: 78.9176453466452  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4427 training Loss: 328.5529591331252 validation Loss: 78.90575515256091  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4428 training Loss: 328.50062351470035 validation Loss: 78.89386945806383  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4429 training Loss: 328.4483081337957 validation Loss: 78.88198826047027  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4430 training Loss: 328.39601297816387 validation Loss: 78.87011155709881  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4431 training Loss: 328.3437380355674 validation Loss: 78.85823934527019  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4432 training Loss: 328.29148329377887 validation Loss: 78.84637162230729  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4433 training Loss: 328.2392487405808 validation Loss: 78.83450838553517  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4434 training Loss: 328.18703436376575 validation Loss: 78.82264963228106  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4435 training Loss: 328.1348401511365 validation Loss: 78.81079535987433  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4436 training Loss: 328.08266609050537 validation Loss: 78.79894556564656  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4437 training Loss: 328.03051216969493 validation Loss: 78.7871002469314  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4438 training Loss: 327.97837837653776 validation Loss: 78.77525940106472  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4439 training Loss: 327.92626469887625 validation Loss: 78.76342302538454  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4440 training Loss: 327.8741711245626 validation Loss: 78.751591117231  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4441 training Loss: 327.8220976414594 validation Loss: 78.73976367394643  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4442 training Loss: 327.7700442374384 validation Loss: 78.72794069287525  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4443 training Loss: 327.71801090038207 validation Loss: 78.71612217136405  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4444 training Loss: 327.6659976181824 validation Loss: 78.7043081067616  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4445 training Loss: 327.61400437874124 validation Loss: 78.69249849641876  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4446 training Loss: 327.5620311699703 validation Loss: 78.68069333768855  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4447 training Loss: 327.5100779797913 validation Loss: 78.66889262792608  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4448 training Loss: 327.45814479613557 validation Loss: 78.65709636448867  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4449 training Loss: 327.4062316069447 validation Loss: 78.64530454473574  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4450 training Loss: 327.3543384001697 validation Loss: 78.63351716602878  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4451 training Loss: 327.30246516377156 validation Loss: 78.6217342257315  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4452 training Loss: 327.25061188572124 validation Loss: 78.60995572120967  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4453 training Loss: 327.1987785539993 validation Loss: 78.59818164983122  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4454 training Loss: 327.1469651565961 validation Loss: 78.58641200896616  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4455 training Loss: 327.0951716815119 validation Loss: 78.57464679598662  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4456 training Loss: 327.04339811675663 validation Loss: 78.56288600826693  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4457 training Loss: 326.99164445035 validation Loss: 78.5511296431834  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4458 training Loss: 326.93991067032164 validation Loss: 78.53937769811455  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4459 training Loss: 326.88819676471087 validation Loss: 78.527630170441  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4460 training Loss: 326.83650272156643 validation Loss: 78.51588705754543  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4461 training Loss: 326.7848285289472 validation Loss: 78.50414835681265  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4462 training Loss: 326.7331741749216 validation Loss: 78.49241406562959  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4463 training Loss: 326.68153964756755 validation Loss: 78.48068418138524  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4464 training Loss: 326.62992493497313 validation Loss: 78.46895870147071  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4465 training Loss: 326.5783300252359 validation Loss: 78.45723762327927  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4466 training Loss: 326.5267549064628 validation Loss: 78.44552094420614  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4467 training Loss: 326.47519956677087 validation Loss: 78.43380866164875  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4468 training Loss: 326.4236639942865 validation Loss: 78.42210077300663  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4469 training Loss: 326.372148177146 validation Loss: 78.41039727568128  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4470 training Loss: 326.3206521034951 validation Loss: 78.39869816707639  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4471 training Loss: 326.26917576148924 validation Loss: 78.38700344459772  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4472 training Loss: 326.21771913929337 validation Loss: 78.37531310565308  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4473 training Loss: 326.1662822250821 validation Loss: 78.36362714765235  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4474 training Loss: 326.1148650070399 validation Loss: 78.35194556800754  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4475 training Loss: 326.06346747336045 validation Loss: 78.3402683641327  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4476 training Loss: 326.0120896122471 validation Loss: 78.32859553344397  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4477 training Loss: 325.96073141191306 validation Loss: 78.31692707335952  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4478 training Loss: 325.90939286058074 validation Loss: 78.30526298129966  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4479 training Loss: 325.8580739464821 validation Loss: 78.2936032546867  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4480 training Loss: 325.8067746578589 validation Loss: 78.28194789094505  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4481 training Loss: 325.7554949829623 validation Loss: 78.27029688750116  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4482 training Loss: 325.70423491005295 validation Loss: 78.25865024178358  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4483 training Loss: 325.652994427401 validation Loss: 78.24700795122286  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4484 training Loss: 325.60177352328617 validation Loss: 78.23537001325167  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4485 training Loss: 325.55057218599757 validation Loss: 78.22373642530471  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4486 training Loss: 325.4993904038339 validation Loss: 78.21210718481868  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4487 training Loss: 325.4482281651034 validation Loss: 78.20048228923241  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4488 training Loss: 325.3970854581235 validation Loss: 78.18886173598676  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4489 training Loss: 325.3459622712213 validation Loss: 78.1772455225246  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4490 training Loss: 325.29485859273325 validation Loss: 78.16563364629086  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4491 training Loss: 325.24377441100523 validation Loss: 78.15402610473254  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4492 training Loss: 325.1927097143927 validation Loss: 78.14242289529864  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4493 training Loss: 325.1416644912603 validation Loss: 78.1308240154402  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4494 training Loss: 325.09063872998223 validation Loss: 78.11922946261035  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4495 training Loss: 325.039632418942 validation Loss: 78.10763923426417  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4496 training Loss: 324.9886455465326 validation Loss: 78.09605332785887  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4497 training Loss: 324.9376781011563 validation Loss: 78.08447174085359  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4498 training Loss: 324.88673007122486 validation Loss: 78.07289447070957  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4499 training Loss: 324.8358014451592 validation Loss: 78.06132151489005  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4500 training Loss: 324.7848922113896 validation Loss: 78.04975287086029  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4501 training Loss: 324.73400235835607 validation Loss: 78.03818853608756  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4502 training Loss: 324.68313187450735 validation Loss: 78.02662850804117  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4503 training Loss: 324.632280748302 validation Loss: 78.01507278419248  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4504 training Loss: 324.5814489682076 validation Loss: 78.00352136201477  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4505 training Loss: 324.53063652270106 validation Loss: 77.99197423898342  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4506 training Loss: 324.47984340026875 validation Loss: 77.98043141257577  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4507 training Loss: 324.42906958940614 validation Loss: 77.96889288027126  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4508 training Loss: 324.37831507861813 validation Loss: 77.95735863955119  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4509 training Loss: 324.3275798564187 validation Loss: 77.94582868789898  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4510 training Loss: 324.27686391133113 validation Loss: 77.93430302280002  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4511 training Loss: 324.2261672318882 validation Loss: 77.9227816417417  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4512 training Loss: 324.1754898066315 validation Loss: 77.9112645422134  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4513 training Loss: 324.124831624112 validation Loss: 77.89975172170651  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4514 training Loss: 324.07419267289015 validation Loss: 77.8882431777144  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4515 training Loss: 324.0235729415353 validation Loss: 77.87673890773249  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4516 training Loss: 323.9729724186261 validation Loss: 77.8652389092581  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4517 training Loss: 323.92239109275033 validation Loss: 77.8537431797906  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4518 training Loss: 323.87182895250504 validation Loss: 77.84225171683131  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4519 training Loss: 323.8212859864963 validation Loss: 77.83076451788361  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4520 training Loss: 323.7707621833396 validation Loss: 77.81928158045278  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4521 training Loss: 323.7202575316594 validation Loss: 77.80780290204612  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4522 training Loss: 323.6697720200892 validation Loss: 77.79632848017292  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4523 training Loss: 323.61930563727185 validation Loss: 77.78485831234441  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4524 training Loss: 323.5688583718592 validation Loss: 77.7733923960738  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4525 training Loss: 323.51843021251227 validation Loss: 77.76193072887634  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4526 training Loss: 323.46802114790114 validation Loss: 77.75047330826916  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4527 training Loss: 323.4176311667049 validation Loss: 77.73902013177141  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4528 training Loss: 323.367260257612 validation Loss: 77.72757119690422  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4529 training Loss: 323.31690840931964 validation Loss: 77.71612650119064  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4530 training Loss: 323.26657561053435 validation Loss: 77.7046860421557  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4531 training Loss: 323.2162618499715 validation Loss: 77.6932498173264  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4532 training Loss: 323.16596711635566 validation Loss: 77.68181782423176  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4533 training Loss: 323.1156913984204 validation Loss: 77.67039006040261  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4534 training Loss: 323.0654346849082 validation Loss: 77.6589665233719  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4535 training Loss: 323.0151969645709 validation Loss: 77.64754721067442  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4536 training Loss: 322.964978226169 validation Loss: 77.63613211984692  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4537 training Loss: 322.91477845847214 validation Loss: 77.62472124842819  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4538 training Loss: 322.86459765025893 validation Loss: 77.61331459395888  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4539 training Loss: 322.8144357903169 validation Loss: 77.60191215398159  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4540 training Loss: 322.7642928674428 validation Loss: 77.59051392604093  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4541 training Loss: 322.7141688704422 validation Loss: 77.57911990768338  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4542 training Loss: 322.6640637881294 validation Loss: 77.56773009645741  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4543 training Loss: 322.613977609328 validation Loss: 77.55634448991336  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4544 training Loss: 322.5639103228703 validation Loss: 77.54496308560363  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4545 training Loss: 322.5138619175978 validation Loss: 77.53358588108244  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4546 training Loss: 322.4638323823606 validation Loss: 77.52221287390597  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4547 training Loss: 322.4138217060179 validation Loss: 77.51084406163235  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4548 training Loss: 322.36382987743787 validation Loss: 77.49947944182165  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4549 training Loss: 322.31385688549733 validation Loss: 77.48811901203584  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4550 training Loss: 322.26390271908224 validation Loss: 77.47676276983881  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4551 training Loss: 322.2139673670872 validation Loss: 77.4654107127964  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4552 training Loss: 322.1640508184159 validation Loss: 77.45406283847635  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4553 training Loss: 322.1141530619808 validation Loss: 77.44271914444832  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4554 training Loss: 322.06427408670316 validation Loss: 77.43137962828392  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4555 training Loss: 322.01441388151324 validation Loss: 77.42004428755664  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4556 training Loss: 321.9645724353499 validation Loss: 77.40871311984188  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4557 training Loss: 321.914749737161 validation Loss: 77.39738612271697  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4558 training Loss: 321.8649457759031 validation Loss: 77.38606329376111  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4559 training Loss: 321.8151605405417 validation Loss: 77.37474463055551  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4560 training Loss: 321.76539402005096 validation Loss: 77.36343013068318  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4561 training Loss: 321.7156462034138 validation Loss: 77.35211979172905  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4562 training Loss: 321.6659170796223 validation Loss: 77.34081361128003  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4563 training Loss: 321.61620663767684 validation Loss: 77.32951158692481  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4564 training Loss: 321.5665148665867 validation Loss: 77.31821371625409  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4565 training Loss: 321.5168417553701 validation Loss: 77.30691999686042  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4566 training Loss: 321.4671872930537 validation Loss: 77.29563042633822  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4567 training Loss: 321.4175514686732 validation Loss: 77.2843450022838  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4568 training Loss: 321.3679342712728 validation Loss: 77.27306372229546  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4569 training Loss: 321.31833568990567 validation Loss: 77.26178658397325  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4570 training Loss: 321.26875571363325 validation Loss: 77.25051358491922  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4571 training Loss: 321.2191943315261 validation Loss: 77.23924472273721  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4572 training Loss: 321.16965153266335 validation Loss: 77.22797999503302  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4573 training Loss: 321.1201273061328 validation Loss: 77.21671939941432  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4574 training Loss: 321.0706216410308 validation Loss: 77.2054629334906  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4575 training Loss: 321.0211345264627 validation Loss: 77.19421059487331  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4576 training Loss: 320.971665951542 validation Loss: 77.1829623811757  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4577 training Loss: 320.92221590539134 validation Loss: 77.17171829001296  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4578 training Loss: 320.87278437714167 validation Loss: 77.16047831900208  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4579 training Loss: 320.82337135593286 validation Loss: 77.14924246576203  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4580 training Loss: 320.77397683091317 validation Loss: 77.13801072791352  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4581 training Loss: 320.7246007912396 validation Loss: 77.12678310307922  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4582 training Loss: 320.67524322607755 validation Loss: 77.11555958888363  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4583 training Loss: 320.62590412460133 validation Loss: 77.10434018295308  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4584 training Loss: 320.5765834759935 validation Loss: 77.09312488291586  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4585 training Loss: 320.52728126944567 validation Loss: 77.081913686402  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4586 training Loss: 320.4779974941574 validation Loss: 77.07070659104346  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4587 training Loss: 320.42873213933746 validation Loss: 77.05950359447407  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4588 training Loss: 320.37948519420263 validation Loss: 77.04830469432943  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4589 training Loss: 320.33025664797856 validation Loss: 77.03710988824707  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4590 training Loss: 320.28104648989927 validation Loss: 77.02591917386636  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4591 training Loss: 320.2318547092075 validation Loss: 77.01473254882846  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4592 training Loss: 320.1826812951542 validation Loss: 77.00355001077645  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4593 training Loss: 320.1335262369992 validation Loss: 76.99237155735524  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4594 training Loss: 320.0843895240106 validation Loss: 76.98119718621153  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4595 training Loss: 320.0352711454651 validation Loss: 76.97002689499391  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4596 training Loss: 319.98617109064776 validation Loss: 76.95886068135282  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4597 training Loss: 319.9370893488523 validation Loss: 76.94769854294047  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4598 training Loss: 319.88802590938064 validation Loss: 76.936540477411  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4599 training Loss: 319.8389807615433 validation Loss: 76.9253864824203  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4600 training Loss: 319.7899538946596 validation Loss: 76.9142365556261  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4601 training Loss: 319.74094529805666 validation Loss: 76.90309069468807  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4602 training Loss: 319.6919549610706 validation Loss: 76.89194889726755  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4603 training Loss: 319.6429828730456 validation Loss: 76.88081116102781  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4604 training Loss: 319.5940290233343 validation Loss: 76.86967748363391  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4605 training Loss: 319.54509340129806 validation Loss: 76.85854786275272  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4606 training Loss: 319.4961759963062 validation Loss: 76.84742229605297  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4607 training Loss: 319.4472767977368 validation Loss: 76.8363007812052  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4608 training Loss: 319.39839579497607 validation Loss: 76.82518331588173  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4609 training Loss: 319.3495329774188 validation Loss: 76.81406989775675  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4610 training Loss: 319.3006883344681 validation Loss: 76.8029605245062  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4611 training Loss: 319.25186185553525 validation Loss: 76.7918551938079  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4612 training Loss: 319.2030535300401 validation Loss: 76.78075390334145  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4613 training Loss: 319.15426334741085 validation Loss: 76.76965665078822  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4614 training Loss: 319.1054912970838 validation Loss: 76.75856343383147  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4615 training Loss: 319.05673736850395 validation Loss: 76.74747425015619  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4616 training Loss: 319.00800155112415 validation Loss: 76.7363890974492  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4617 training Loss: 318.959283834406 validation Loss: 76.72530797339914  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4618 training Loss: 318.91058420781917 validation Loss: 76.71423087569644  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4619 training Loss: 318.8619026608417 validation Loss: 76.7031578020333  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4620 training Loss: 318.8132391829598 validation Loss: 76.69208875010378  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4621 training Loss: 318.76459376366824 validation Loss: 76.68102371760364  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4622 training Loss: 318.7159663924698 validation Loss: 76.66996270223052  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4623 training Loss: 318.6673570588754 validation Loss: 76.6589057016838  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4624 training Loss: 318.61876575240467 validation Loss: 76.64785271366469  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4625 training Loss: 318.57019246258506 validation Loss: 76.63680373587613  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4626 training Loss: 318.5216371789526 validation Loss: 76.62575876602293  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4627 training Loss: 318.4730998910511 validation Loss: 76.61471780181157  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4628 training Loss: 318.42458058843306 validation Loss: 76.6036808409504  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4629 training Loss: 318.3760792606589 validation Loss: 76.59264788114953  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4630 training Loss: 318.32759589729744 validation Loss: 76.58161892012086  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4631 training Loss: 318.2791304879255 validation Loss: 76.57059395557805  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4632 training Loss: 318.2306830221281 validation Loss: 76.5595729852365  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4633 training Loss: 318.18225348949886 validation Loss: 76.54855600681347  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4634 training Loss: 318.1338418796389 validation Loss: 76.5375430180279  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4635 training Loss: 318.085448182158 validation Loss: 76.52653401660058  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4636 training Loss: 318.0370723866739 validation Loss: 76.51552900025399  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4637 training Loss: 317.98871448281255 validation Loss: 76.50452796671246  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4638 training Loss: 317.940374460208 validation Loss: 76.49353091370202  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4639 training Loss: 317.89205230850234 validation Loss: 76.48253783895046  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4640 training Loss: 317.84374801734606 validation Loss: 76.47154874018743  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4641 training Loss: 317.79546157639754 validation Loss: 76.4605636151442  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4642 training Loss: 317.74719297532334 validation Loss: 76.44958246155389  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4643 training Loss: 317.6989422037981 validation Loss: 76.43860527715134  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4644 training Loss: 317.6507092515045 validation Loss: 76.42763205967316  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4645 training Loss: 317.60249410813344 validation Loss: 76.41666280685773  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4646 training Loss: 317.55429676338383 validation Loss: 76.40569751644513  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4647 training Loss: 317.5061172069626 validation Loss: 76.39473618617725  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4648 training Loss: 317.4579554285848 validation Loss: 76.38377881379763  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4649 training Loss: 317.4098114179734 validation Loss: 76.37282539705173  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4650 training Loss: 317.3616851648597 validation Loss: 76.36187593368655  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4651 training Loss: 317.31357665898275 validation Loss: 76.35093042145098  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4652 training Loss: 317.26548589008985 validation Loss: 76.3399888580956  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4653 training Loss: 317.2174128479361 validation Loss: 76.3290512413727  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4654 training Loss: 317.1693575222847 validation Loss: 76.31811756903636  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4655 training Loss: 317.1213199029071 validation Loss: 76.3071878388424  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4656 training Loss: 317.07329997958226 validation Loss: 76.29626204854827  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4657 training Loss: 317.02529774209756 validation Loss: 76.28534019591332  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4658 training Loss: 316.9773131802482 validation Loss: 76.27442227869848  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4659 training Loss: 316.9293462838373 validation Loss: 76.26350829466651  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4660 training Loss: 316.88139704267587 validation Loss: 76.25259824158181  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4661 training Loss: 316.8334654465832 validation Loss: 76.24169211721059  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4662 training Loss: 316.7855514853863 validation Loss: 76.23078991932073  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4663 training Loss: 316.73765514892017 validation Loss: 76.21989164568187  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4664 training Loss: 316.6897764270276 validation Loss: 76.20899729406531  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4665 training Loss: 316.6419153095596 validation Loss: 76.19810686224417  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4666 training Loss: 316.59407178637485 validation Loss: 76.18722034799316  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4667 training Loss: 316.54624584734 validation Loss: 76.17633774908883  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4668 training Loss: 316.4984374823298 validation Loss: 76.16545906330936  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4669 training Loss: 316.45064668122643 validation Loss: 76.15458428843468  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4670 training Loss: 316.4028734339206 validation Loss: 76.14371342224638  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4671 training Loss: 316.3551177303103 validation Loss: 76.13284646252785  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4672 training Loss: 316.3073795603018 validation Loss: 76.1219834070641  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4673 training Loss: 316.25965891380895 validation Loss: 76.11112425364192  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4674 training Loss: 316.2119557807537 validation Loss: 76.10026900004974  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4675 training Loss: 316.1642701510657 validation Loss: 76.0894176440777  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4676 training Loss: 316.1166020146825 validation Loss: 76.0785701835177  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4677 training Loss: 316.06895136154947 validation Loss: 76.06772661616324  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4678 training Loss: 316.02131818161973 validation Loss: 76.05688693980966  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4679 training Loss: 315.97370246485434 validation Loss: 76.04605115225384  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4680 training Loss: 315.92610420122213 validation Loss: 76.03521925129446  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4681 training Loss: 315.8785233806998 validation Loss: 76.02439123473184  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4682 training Loss: 315.83095999327156 validation Loss: 76.01356710036802  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4683 training Loss: 315.78341402892977 validation Loss: 76.00274684600672  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4684 training Loss: 315.7358854776744 validation Loss: 75.99193046945334  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4685 training Loss: 315.6883743295132 validation Loss: 75.98111796851497  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4686 training Loss: 315.6408805744617 validation Loss: 75.97030934100039  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4687 training Loss: 315.5934042025432 validation Loss: 75.95950458472007  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4688 training Loss: 315.54594520378873 validation Loss: 75.94870369748611  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4689 training Loss: 315.49850356823697 validation Loss: 75.93790667711238  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4690 training Loss: 315.45107928593455 validation Loss: 75.92711352141436  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4691 training Loss: 315.4036723469357 validation Loss: 75.91632422820922  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4692 training Loss: 315.3562827413024 validation Loss: 75.9055387953158  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4693 training Loss: 315.30891045910414 validation Loss: 75.89475722055465  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4694 training Loss: 315.2615554904185 validation Loss: 75.88397950174792  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4695 training Loss: 315.2142178253305 validation Loss: 75.87320563671949  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4696 training Loss: 315.1668974539328 validation Loss: 75.8624356232949  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4697 training Loss: 315.11959436632594 validation Loss: 75.85166945930133  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4698 training Loss: 315.0723085526181 validation Loss: 75.84090714256769  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4699 training Loss: 315.0250400029249 validation Loss: 75.83014867092444  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4700 training Loss: 314.97778870736994 validation Loss: 75.81939404220381  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4701 training Loss: 314.93055465608427 validation Loss: 75.80864325423963  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4702 training Loss: 314.88333783920655 validation Loss: 75.79789630486741  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4703 training Loss: 314.8361382468832 validation Loss: 75.78715319192429  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4704 training Loss: 314.78895586926825 validation Loss: 75.77641391324912  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4705 training Loss: 314.74179069652337 validation Loss: 75.76567846668237  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4706 training Loss: 314.69464271881776 validation Loss: 75.75494685006616  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4707 training Loss: 314.6475119263283 validation Loss: 75.74421906124425  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4708 training Loss: 314.6003983092395 validation Loss: 75.73349509806206  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4709 training Loss: 314.55330185774324 validation Loss: 75.7227749583667  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4710 training Loss: 314.50622256203934 validation Loss: 75.71205864000686  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4711 training Loss: 314.4591604123349 validation Loss: 75.70134614083287  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4712 training Loss: 314.4121153988447 validation Loss: 75.69063745869678  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4713 training Loss: 314.36508751179116 validation Loss: 75.67993259145221  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4714 training Loss: 314.3180767414042 validation Loss: 75.66923153695447  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4715 training Loss: 314.27108307792116 validation Loss: 75.65853429306044  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4716 training Loss: 314.22410651158714 validation Loss: 75.64784085762872  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4717 training Loss: 314.17714703265455 validation Loss: 75.63715122851946  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4718 training Loss: 314.1302046313835 validation Loss: 75.62646540359452  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4719 training Loss: 314.0832792980416 validation Loss: 75.61578338071735  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4720 training Loss: 314.0363710229039 validation Loss: 75.60510515775302  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4721 training Loss: 313.989479796253 validation Loss: 75.59443073256826  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4722 training Loss: 313.94260560837887 validation Loss: 75.5837601030314  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4723 training Loss: 313.8957484495793 validation Loss: 75.57309326701241  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4724 training Loss: 313.84890831015923 validation Loss: 75.56243022238291  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4725 training Loss: 313.8020851804313 validation Loss: 75.55177096701607  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4726 training Loss: 313.7552790507152 validation Loss: 75.54111549878675  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4727 training Loss: 313.70848991133875 validation Loss: 75.53046381557141  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4728 training Loss: 313.6617177526367 validation Loss: 75.5198159152481  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4729 training Loss: 313.61496256495144 validation Loss: 75.50917179569652  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4730 training Loss: 313.56822433863283 validation Loss: 75.49853145479797  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4731 training Loss: 313.5215030640381 validation Loss: 75.48789489043536  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4732 training Loss: 313.47479873153173 validation Loss: 75.47726210049319  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4733 training Loss: 313.4281113314861 validation Loss: 75.46663308285767  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4734 training Loss: 313.38144085428047 validation Loss: 75.45600783541646  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4735 training Loss: 313.33478729030185 validation Loss: 75.44538635605895  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4736 training Loss: 313.2881506299444 validation Loss: 75.43476864267608  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4737 training Loss: 313.24153086360997 validation Loss: 75.42415469316043  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4738 training Loss: 313.19492798170756 validation Loss: 75.41354450540614  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4739 training Loss: 313.14834197465353 validation Loss: 75.40293807730899  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4740 training Loss: 313.1017728328718 validation Loss: 75.39233540676632  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4741 training Loss: 313.0552205467934 validation Loss: 75.38173649167712  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4742 training Loss: 313.00868510685706 validation Loss: 75.37114132994189  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4743 training Loss: 312.96216650350846 validation Loss: 75.3605499194628  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4744 training Loss: 312.9156647272008 validation Loss: 75.34996225814362  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4745 training Loss: 312.8691797683947 validation Loss: 75.33937834388966  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4746 training Loss: 312.82271161755807 validation Loss: 75.32879817460784  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4747 training Loss: 312.776260265166 validation Loss: 75.31822174820667  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4748 training Loss: 312.729825701701 validation Loss: 75.30764906259628  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4749 training Loss: 312.68340791765286 validation Loss: 75.29708011568829  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4750 training Loss: 312.6370069035186 validation Loss: 75.28651490539602  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4751 training Loss: 312.59062264980275 validation Loss: 75.27595342963428  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4752 training Loss: 312.54425514701694 validation Loss: 75.26539568631951  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4753 training Loss: 312.49790438568004 validation Loss: 75.25484167336975  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4754 training Loss: 312.45157035631826 validation Loss: 75.24429138870455  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4755 training Loss: 312.40525304946516 validation Loss: 75.23374483024509  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4756 training Loss: 312.3589524556613 validation Loss: 75.2232019959141  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4757 training Loss: 312.3126685654548 validation Loss: 75.21266288363589  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4758 training Loss: 312.2664013694009 validation Loss: 75.20212749133633  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4759 training Loss: 312.220150858062 validation Loss: 75.19159581694288  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4760 training Loss: 312.17391702200774 validation Loss: 75.18106785838458  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4761 training Loss: 312.127699851815 validation Loss: 75.170543613592  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4762 training Loss: 312.08149933806794 validation Loss: 75.1600230804973  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4763 training Loss: 312.0353154713578 validation Loss: 75.1495062570342  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4764 training Loss: 311.98914824228325 validation Loss: 75.13899314113797  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4765 training Loss: 311.94299764144984 validation Loss: 75.12848373074544  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4766 training Loss: 311.8968636594705 validation Loss: 75.11797802379508  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4767 training Loss: 311.8507462869653 validation Loss: 75.10747601822675  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4768 training Loss: 311.80464551456157 validation Loss: 75.09697771198202  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4769 training Loss: 311.7585613328936 validation Loss: 75.08648310300399  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4770 training Loss: 311.712493732603 validation Loss: 75.07599218923724  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4771 training Loss: 311.66644270433835 validation Loss: 75.06550496862795  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4772 training Loss: 311.6204082387558 validation Loss: 75.05502143912386  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4773 training Loss: 311.574390326518 validation Loss: 75.04454159867427  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4774 training Loss: 311.52838895829535 validation Loss: 75.03406544522998  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4775 training Loss: 311.48240412476497 validation Loss: 75.02359297674336  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4776 training Loss: 311.43643581661127 validation Loss: 75.01312419116833  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4777 training Loss: 311.3904840245256 validation Loss: 75.00265908646037  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4778 training Loss: 311.34454873920674 validation Loss: 74.99219766057647  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4779 training Loss: 311.29862995136017 validation Loss: 74.98173991147519  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4780 training Loss: 311.2527276516987 validation Loss: 74.97128583711657  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4781 training Loss: 311.20684183094227 validation Loss: 74.96083543546226  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4782 training Loss: 311.16097247981764 validation Loss: 74.95038870447543  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4783 training Loss: 311.11511958905896 validation Loss: 74.93994564212075  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4784 training Loss: 311.06928314940717 validation Loss: 74.92950624636445  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4785 training Loss: 311.02346315161043 validation Loss: 74.91907051517428  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4786 training Loss: 310.9776595864238 validation Loss: 74.90863844651955  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4787 training Loss: 310.93187244460955 validation Loss: 74.89821003837105  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4788 training Loss: 310.8861017169369 validation Loss: 74.88778528870114  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4789 training Loss: 310.8403473941822 validation Loss: 74.87736419548366  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4790 training Loss: 310.79460946712857 validation Loss: 74.86694675669406  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4791 training Loss: 310.74888792656634 validation Loss: 74.85653297030922  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4792 training Loss: 310.70318276329283 validation Loss: 74.84612283430756  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4793 training Loss: 310.65749396811236 validation Loss: 74.83571634666907  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4794 training Loss: 310.6118215318362 validation Loss: 74.82531350537522  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4795 training Loss: 310.56616544528276 validation Loss: 74.81491430840902  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4796 training Loss: 310.5205256992771 validation Loss: 74.80451875375495  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4797 training Loss: 310.47490228465153 validation Loss: 74.79412683939907  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4798 training Loss: 310.4292951922454 validation Loss: 74.78373856332887  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4799 training Loss: 310.3837044129047 validation Loss: 74.77335392353343  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4800 training Loss: 310.33812993748256 validation Loss: 74.76297291800331  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4801 training Loss: 310.29257175683927 validation Loss: 74.7525955447306  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4802 training Loss: 310.2470298618415 validation Loss: 74.74222180170884  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4803 training Loss: 310.2015042433634 validation Loss: 74.7318516869331  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4804 training Loss: 310.1559948922859 validation Loss: 74.72148519840002  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4805 training Loss: 310.11050179949666 validation Loss: 74.71112233410764  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4806 training Loss: 310.0650249558905 validation Loss: 74.70076309205558  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4807 training Loss: 310.01956435236895 validation Loss: 74.69040747024493  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4808 training Loss: 309.97411997984057 validation Loss: 74.68005546667828  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4809 training Loss: 309.92869182922084 validation Loss: 74.66970707935971  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4810 training Loss: 309.883279891432 validation Loss: 74.6593623062948  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4811 training Loss: 309.8378841574032 validation Loss: 74.64902114549062  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4812 training Loss: 309.7925046180705 validation Loss: 74.63868359495575  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4813 training Loss: 309.74714126437686 validation Loss: 74.6283496527003  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4814 training Loss: 309.7017940872722 validation Loss: 74.61801931673574  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4815 training Loss: 309.65646307771294 validation Loss: 74.60769258507514  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4816 training Loss: 309.61114822666275 validation Loss: 74.59736945573307  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4817 training Loss: 309.5658495250919 validation Loss: 74.58704992672551  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4818 training Loss: 309.52056696397756 validation Loss: 74.57673399606996  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4819 training Loss: 309.47530053430376 validation Loss: 74.56642166178541  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4820 training Loss: 309.4300502270613 validation Loss: 74.55611292189234  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4821 training Loss: 309.38481603324783 validation Loss: 74.54580777441267  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4822 training Loss: 309.33959794386783 validation Loss: 74.53550621736983  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4823 training Loss: 309.29439594993255 validation Loss: 74.52520824878874  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4824 training Loss: 309.2492100424601 validation Loss: 74.51491386669578  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4825 training Loss: 309.2040402124752 validation Loss: 74.50462306911878  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4826 training Loss: 309.1588864510095 validation Loss: 74.4943358540871  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4827 training Loss: 309.11374874910155 validation Loss: 74.48405221963148  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4828 training Loss: 309.06862709779637 validation Loss: 74.47377216378426  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4829 training Loss: 309.023521488146 validation Loss: 74.46349568457916  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4830 training Loss: 308.9784319112091 validation Loss: 74.45322278005136  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4831 training Loss: 308.9333583580511 validation Loss: 74.44295344823757  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4832 training Loss: 308.88830081974425 validation Loss: 74.43268768717587  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4833 training Loss: 308.84325928736746 validation Loss: 74.42242549490595  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4834 training Loss: 308.79823375200647 validation Loss: 74.41216686946882  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4835 training Loss: 308.75322420475356 validation Loss: 74.40191180890703  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4836 training Loss: 308.7082306367079 validation Loss: 74.39166031126456  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4837 training Loss: 308.6632530389754 validation Loss: 74.38141237458683  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4838 training Loss: 308.61829140266855 validation Loss: 74.37116799692076  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4839 training Loss: 308.57334571890635 validation Loss: 74.3609271763147  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4840 training Loss: 308.52841597881513 validation Loss: 74.35068991081852  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4841 training Loss: 308.48350217352726 validation Loss: 74.34045619848341  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4842 training Loss: 308.4386042941822 validation Loss: 74.33022603736211  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4843 training Loss: 308.3937223319258 validation Loss: 74.3199994255088  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4844 training Loss: 308.3488562779107 validation Loss: 74.30977636097909  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4845 training Loss: 308.30400612329646 validation Loss: 74.29955684183003  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4846 training Loss: 308.2591718592487 validation Loss: 74.28934086612014  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4847 training Loss: 308.2143534769402 validation Loss: 74.27912843190937  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4848 training Loss: 308.1695509675502 validation Loss: 74.26891953725914  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4849 training Loss: 308.12476432226464 validation Loss: 74.25871418023226  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4850 training Loss: 308.07999353227603 validation Loss: 74.24851235889301  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4851 training Loss: 308.03523858878356 validation Loss: 74.23831407130712  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4852 training Loss: 307.99049948299285 validation Loss: 74.22811931554175  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4853 training Loss: 307.94577620611653 validation Loss: 74.21792808966546  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4854 training Loss: 307.9010687493733 validation Loss: 74.20774039174832  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4855 training Loss: 307.85637710398896 validation Loss: 74.19755621986178  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4856 training Loss: 307.8117012611957 validation Loss: 74.18737557207874  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4857 training Loss: 307.76704121223224 validation Loss: 74.17719844647353  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4858 training Loss: 307.72239694834394 validation Loss: 74.16702484112187  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4859 training Loss: 307.67776846078266 validation Loss: 74.15685475410099  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4860 training Loss: 307.63315574080707 validation Loss: 74.14668818348949  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4861 training Loss: 307.5885587796821 validation Loss: 74.13652512736738  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4862 training Loss: 307.54397756867945 validation Loss: 74.12636558381615  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4863 training Loss: 307.4994120990772 validation Loss: 74.1162095509187  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4864 training Loss: 307.45486236216004 validation Loss: 74.1060570267593  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4865 training Loss: 307.4103283492193 validation Loss: 74.09590800942371  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4866 training Loss: 307.3658100515528 validation Loss: 74.08576249699907  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4867 training Loss: 307.32130746046477 validation Loss: 74.07562048757391  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4868 training Loss: 307.27682056726604 validation Loss: 74.06548197923827  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4869 training Loss: 307.232349363274 validation Loss: 74.05534697008352  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4870 training Loss: 307.1878938398125 validation Loss: 74.04521545820245  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4871 training Loss: 307.14345398821183 validation Loss: 74.03508744168933  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4872 training Loss: 307.0990297998089 validation Loss: 74.02496291863974  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4873 training Loss: 307.05462126594693 validation Loss: 74.01484188715077  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4874 training Loss: 307.01022837797575 validation Loss: 74.00472434532085  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4875 training Loss: 306.96585112725177 validation Loss: 73.99461029124987  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4876 training Loss: 306.92148950513774 validation Loss: 73.98449972303906  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4877 training Loss: 306.8771435030028 validation Loss: 73.97439263879113  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4878 training Loss: 306.83281311222265 validation Loss: 73.96428903661011  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4879 training Loss: 306.7884983241795 validation Loss: 73.95418891460152  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4880 training Loss: 306.74419913026185 validation Loss: 73.9440922708722  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4881 training Loss: 306.6999155218648 validation Loss: 73.93399910353048  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4882 training Loss: 306.6556474903897 validation Loss: 73.923909410686  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4883 training Loss: 306.6113950272445 validation Loss: 73.9138231904498  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4884 training Loss: 306.56715812384346 validation Loss: 73.90374044093441  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4885 training Loss: 306.52293677160736 validation Loss: 73.89366116025369  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4886 training Loss: 306.4787309619634 validation Loss: 73.88358534652284  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4887 training Loss: 306.43454068634486 validation Loss: 73.87351299785854  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4888 training Loss: 306.390365936192 validation Loss: 73.86344411237886  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4889 training Loss: 306.3462067029508 validation Loss: 73.85337868820315  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4890 training Loss: 306.3020629780743 validation Loss: 73.84331672345232  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4891 training Loss: 306.25793475302135 validation Loss: 73.83325821624851  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4892 training Loss: 306.2138220192575 validation Loss: 73.8232031647153  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4893 training Loss: 306.1697247682547 validation Loss: 73.8131515669777  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4894 training Loss: 306.12564299149096 validation Loss: 73.80310342116202  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4895 training Loss: 306.08157668045084 validation Loss: 73.79305872539601  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4896 training Loss: 306.03752582662537 validation Loss: 73.78301747780881  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4897 training Loss: 305.99349042151164 validation Loss: 73.77297967653092  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4898 training Loss: 305.9494704566134 validation Loss: 73.76294531969415  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4899 training Loss: 305.9054659234404 validation Loss: 73.7529144054318  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4900 training Loss: 305.86147681350894 validation Loss: 73.74288693187845  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4901 training Loss: 305.8175031183416 validation Loss: 73.73286289717012  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4902 training Loss: 305.77354482946726 validation Loss: 73.72284229944421  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4903 training Loss: 305.72960193842107 validation Loss: 73.71282513683943  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4904 training Loss: 305.6856744367444 validation Loss: 73.70281140749584  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4905 training Loss: 305.64176231598526 validation Loss: 73.692801109555  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4906 training Loss: 305.59786556769757 validation Loss: 73.68279424115971  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4907 training Loss: 305.5539841834418 validation Loss: 73.67279080045421  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4908 training Loss: 305.51011815478444 validation Loss: 73.66279078558404  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4909 training Loss: 305.46626747329833 validation Loss: 73.65279419469616  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4910 training Loss: 305.4224321305629 validation Loss: 73.64280102593887  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4911 training Loss: 305.37861211816346 validation Loss: 73.63281127746183  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4912 training Loss: 305.33480742769177 validation Loss: 73.62282494741606  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4913 training Loss: 305.29101805074555 validation Loss: 73.61284203395394  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4914 training Loss: 305.24724397892913 validation Loss: 73.60286253522924  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4915 training Loss: 305.20348520385295 validation Loss: 73.59288644939699  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4916 training Loss: 305.1597417171336 validation Loss: 73.5829137746137  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4917 training Loss: 305.11601351039405 validation Loss: 73.57294450903714  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4918 training Loss: 305.0723005752633 validation Loss: 73.56297865082647  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4919 training Loss: 305.0286029033767 validation Loss: 73.55301619814219  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4920 training Loss: 304.9849204863757 validation Loss: 73.5430571491462  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4921 training Loss: 304.9412533159082 validation Loss: 73.53310150200164  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4922 training Loss: 304.89760138362794 validation Loss: 73.52314925487309  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4923 training Loss: 304.8539646811952 validation Loss: 73.51320040592645  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4924 training Loss: 304.81034320027607 validation Loss: 73.50325495332896  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4925 training Loss: 304.7667369325432 validation Loss: 73.4933128952492  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4926 training Loss: 304.72314586967525 validation Loss: 73.4833742298571  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4927 training Loss: 304.6795700033569 validation Loss: 73.47343895532393  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4928 training Loss: 304.63600932527913 validation Loss: 73.46350706982228  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4929 training Loss: 304.5924638271393 validation Loss: 73.45357857152612  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4930 training Loss: 304.5489335006404 validation Loss: 73.44365345861075  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4931 training Loss: 304.5054183374921 validation Loss: 73.43373172925274  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4932 training Loss: 304.46191832940985 validation Loss: 73.42381338163008  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4933 training Loss: 304.4184334681154 validation Loss: 73.41389841392203  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4934 training Loss: 304.3749637453366 validation Loss: 73.40398682430924  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4935 training Loss: 304.33150915280726 validation Loss: 73.39407861097366  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4936 training Loss: 304.28806968226763 validation Loss: 73.38417377209856  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4937 training Loss: 304.2446453254639 validation Loss: 73.37427230586856  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4938 training Loss: 304.20123607414826 validation Loss: 73.36437421046958  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4939 training Loss: 304.15784192007914 validation Loss: 73.3544794840889  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4940 training Loss: 304.1144628550211 validation Loss: 73.34458812491509  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4941 training Loss: 304.07109887074455 validation Loss: 73.33470013113809  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4942 training Loss: 304.02774995902627 validation Loss: 73.32481550094911  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4943 training Loss: 303.9844161116489 validation Loss: 73.31493423254072  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4944 training Loss: 303.9410973204014 validation Loss: 73.30505632410681  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4945 training Loss: 303.89779357707846 validation Loss: 73.29518177384257  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4946 training Loss: 303.85450487348106 validation Loss: 73.2853105799445  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4947 training Loss: 303.81123120141626 validation Loss: 73.27544274061043  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4948 training Loss: 303.767972552697 validation Loss: 73.26557825403948  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4949 training Loss: 303.72472891914236 validation Loss: 73.25571711843216  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4950 training Loss: 303.68150029257754 validation Loss: 73.24585933199022  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4951 training Loss: 303.63828666483346 validation Loss: 73.23600489291675  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4952 training Loss: 303.5950880277476 validation Loss: 73.22615379941612  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4953 training Loss: 303.55190437316304 validation Loss: 73.21630604969408  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4954 training Loss: 303.5087356929289 validation Loss: 73.20646164195762  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4955 training Loss: 303.4655819789004 validation Loss: 73.19662057441502  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4956 training Loss: 303.42244322293885 validation Loss: 73.18678284527596  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4957 training Loss: 303.37931941691136 validation Loss: 73.17694845275132  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4958 training Loss: 303.3362105526912 validation Loss: 73.16711739505338  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4959 training Loss: 303.2931166221576 validation Loss: 73.15728967039564  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4960 training Loss: 303.25003761719563 validation Loss: 73.14746527699293  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4961 training Loss: 303.2069735296966 validation Loss: 73.1376442130614  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4962 training Loss: 303.1639243515575 validation Loss: 73.12782647681848  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4963 training Loss: 303.12089007468137 validation Loss: 73.1180120664829  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4964 training Loss: 303.0778706909775 validation Loss: 73.1082009802747  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4965 training Loss: 303.03486619236065 validation Loss: 73.09839321641518  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4966 training Loss: 302.9918765707519 validation Loss: 73.08858877312694  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4967 training Loss: 302.9489018180782 validation Loss: 73.07878764863392  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4968 training Loss: 302.90594192627213 validation Loss: 73.0689898411613  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4969 training Loss: 302.86299688727274 validation Loss: 73.05919534893557  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4970 training Loss: 302.8200666930245 validation Loss: 73.04940417018453  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4971 training Loss: 302.77715133547815 validation Loss: 73.03961630313721  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4972 training Loss: 302.73425080659024 validation Loss: 73.02983174602397  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4973 training Loss: 302.6913650983231 validation Loss: 73.02005049707645  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4974 training Loss: 302.6484942026451 validation Loss: 73.01027255452759  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4975 training Loss: 302.60563811153054 validation Loss: 73.00049791661156  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4976 training Loss: 302.5627968169595 validation Loss: 72.99072658156389  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4977 training Loss: 302.5199703109181 validation Loss: 72.9809585476213  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4978 training Loss: 302.477158585398 validation Loss: 72.97119381302187  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4979 training Loss: 302.43436163239727 validation Loss: 72.96143237600491  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4980 training Loss: 302.39157944391934 validation Loss: 72.95167423481102  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4981 training Loss: 302.34881201197396 validation Loss: 72.94191938768208  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4982 training Loss: 302.3060593285763 validation Loss: 72.93216783286125  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4983 training Loss: 302.26332138574764 validation Loss: 72.92241956859294  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4984 training Loss: 302.2205981755152 validation Loss: 72.9126745931229  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4985 training Loss: 302.1778896899118 validation Loss: 72.90293290469803  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4986 training Loss: 302.13519592097623 validation Loss: 72.89319450156661  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4987 training Loss: 302.09251686075305 validation Loss: 72.88345938197816  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4988 training Loss: 302.0498525012928 validation Loss: 72.87372754418342  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4989 training Loss: 302.0072028346517 validation Loss: 72.86399898643447  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4990 training Loss: 301.96456785289183 validation Loss: 72.8542737069846  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4991 training Loss: 301.92194754808094 validation Loss: 72.84455170408837  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4992 training Loss: 301.8793419122929 validation Loss: 72.83483297600168  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4993 training Loss: 301.8367509376071 validation Loss: 72.82511752098155  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4994 training Loss: 301.79417461610893 validation Loss: 72.81540533728639  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4995 training Loss: 301.7516129398894 validation Loss: 72.80569642317579  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4996 training Loss: 301.7090659010454 validation Loss: 72.79599077691067  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4997 training Loss: 301.6665334916796 validation Loss: 72.78628839675315  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4998 training Loss: 301.6240157039005 validation Loss: 72.77658928096659  valid acc: 1.0  train Acc: 1.0\n",
      "epoch: 4999 training Loss: 301.5815125298221 validation Loss: 72.76689342781569  valid acc: 1.0  train Acc: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Neural_Network(2,1)\n",
    "model.train(trainX, trainY, epochs = 5000, learningRate = 0.00001, validationX = validX, validationY = validY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.saveModel('bestmodelTask1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: This below section will be used for the evaluation of this task, we need your model and we will run script below to evaluated your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create class object\n",
    "mm = Neural_Network()\n",
    "# load model which will be provided by you\n",
    "mm.loadModel('bestmodelTask1.npy')\n",
    "# check accuracy of that model\n",
    "mm.accuracy(testX,testY)\n",
    "\n",
    "\n",
    "# error = 0.1\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
